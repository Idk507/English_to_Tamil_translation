{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dhanu\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 1124012 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Eng_tam_data.csv')  \n",
    "print(f\"Dataset loaded with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['english'] = df['english'].apply(clean_text)\n",
    "df['tamil'] = df['tamil'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>tamil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e - Gov Initiative in Uttar Pradesh</td>\n",
       "      <td>e - à®‰à®¤à¯à®¤à®°à®ªà®¿à®°à®¤à¯‡à®šà®¤à¯à®¤à®¿à®²à¯ à®…à®°à®šà¯ à®®à¯à®¯à®±à¯à®šà®¿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The court is under an obligation to undo a wro...</td>\n",
       "      <td>à®¨à¯€à®¤à®¿à®®à®©à¯à®±à®¤à¯à®¤à®¿à®©à¯ à®šà¯†à®¯à®²à®¾à®²à¯ à®’à®°à¯ à®¤à®°à®ªà¯à®ªà®¿à®©à®°à¯à®•à¯à®•à¯ à®šà¯†à®¯à¯à®¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You should have got the expressway 8-9 years ago.</td>\n",
       "      <td>8 - 9 à®†à®£à¯à®Ÿà¯à®•à®³à¯à®•à¯à®•à¯ à®®à¯à®©à¯à®ªà¯‡ à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®µà®¿à®°à¯ˆà®µà¯à®šà¯ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are no shortcuts.</td>\n",
       "      <td>à®…à®¤à®±à¯à®•à¯ à®•à¯à®±à¯à®•à¯à®•à¯à®µà®´à®¿à®•à®³à¯ à®‡à®²à¯à®²à¯ˆ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Chatterjee complains that there is no indic...</td>\n",
       "      <td>à®¤à®¿à®°à¯ à®šà®¾à®Ÿà¯à®Ÿà®°à¯à®œà®¿ à®ªà¯à®¤à®¿à®¯ à®¨à®Ÿà¯ˆà®®à¯à®±à¯ˆ à®®à®±à¯à®ªà®°à®¿à®šà¯€à®²à®©à¯ˆ à®šà¯†à®¯à¯à®¯...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0                e - Gov Initiative in Uttar Pradesh   \n",
       "1  The court is under an obligation to undo a wro...   \n",
       "2  You should have got the expressway 8-9 years ago.   \n",
       "3                            There are no shortcuts.   \n",
       "4  Mr Chatterjee complains that there is no indic...   \n",
       "\n",
       "                                               tamil  \n",
       "0                 e - à®‰à®¤à¯à®¤à®°à®ªà®¿à®°à®¤à¯‡à®šà®¤à¯à®¤à®¿à®²à¯ à®…à®°à®šà¯ à®®à¯à®¯à®±à¯à®šà®¿  \n",
       "1  à®¨à¯€à®¤à®¿à®®à®©à¯à®±à®¤à¯à®¤à®¿à®©à¯ à®šà¯†à®¯à®²à®¾à®²à¯ à®’à®°à¯ à®¤à®°à®ªà¯à®ªà®¿à®©à®°à¯à®•à¯à®•à¯ à®šà¯†à®¯à¯à®¯...  \n",
       "2  8 - 9 à®†à®£à¯à®Ÿà¯à®•à®³à¯à®•à¯à®•à¯ à®®à¯à®©à¯à®ªà¯‡ à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®µà®¿à®°à¯ˆà®µà¯à®šà¯ ...  \n",
       "3                       à®…à®¤à®±à¯à®•à¯ à®•à¯à®±à¯à®•à¯à®•à¯à®µà®´à®¿à®•à®³à¯ à®‡à®²à¯à®²à¯ˆ.  \n",
       "4  à®¤à®¿à®°à¯ à®šà®¾à®Ÿà¯à®Ÿà®°à¯à®œà®¿ à®ªà¯à®¤à®¿à®¯ à®¨à®Ÿà¯ˆà®®à¯à®±à¯ˆ à®®à®±à¯à®ªà®°à®¿à®šà¯€à®²à®©à¯ˆ à®šà¯†à®¯à¯à®¯...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after cleaning: 1124007 rows\n"
     ]
    }
   ],
   "source": [
    "df = df[df['english'].str.len() > 0]\n",
    "df = df[df['tamil'].str.len() > 0]\n",
    "print(f\"Dataset after cleaning: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 899205 rows\n",
      "Validation set: 112401 rows\n",
      "Test set: 112401 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set: {len(train_df)} rows\")\n",
    "print(f\"Validation set: {len(val_df)} rows\")\n",
    "print(f\"Test set: {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac9d08bec9944448a8da170f6c46b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhanu\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-en-mul. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d77ddb28ce4bba8905fc2d8cb413b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936ed2137018453c85b7ea13c798921a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309f1c2c32c74ab5bdef405ee64d4dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205555f980f64cbeb056344d92c5a76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41dcec957754f8d98955704a949a9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0737b47606d6425d86caff290fd8322c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.target_language = \"ta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [str(ex) for ex in examples[\"english\"]]\n",
    "    targets = [str(ex) for ex in examples[\"tamil\"]]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=128, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"])\n",
    "    model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"])\n",
    "    model_inputs[\"labels\"] = torch.tensor(model_inputs[\"labels\"])\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493e0fc06147429da57c7d42e50a0eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/899205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3962: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8a8969ad814975bacac06588b86385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1f964538c24beabce3d8b54fb3e855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_dict.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mjpost/sacrebleu \n",
    "\n",
    "https://towardsdatascience.com/machine-translation-evaluation-with-sacrebleu-and-bertscore-d7fdb0c47eb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\training_args.py:1483: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./english-tamil-translation\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"none\"],  # Disable wandb logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\accelerate\\accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59468a39369b4ca4b3586425effc3bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/337203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2875, 'grad_norm': 3.7842488288879395, 'learning_rate': 1.999436541193287e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0902, 'grad_norm': 4.6688714027404785, 'learning_rate': 1.9988434266599053e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9817, 'grad_norm': 2.7973673343658447, 'learning_rate': 1.9982503121265233e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9415, 'grad_norm': 3.703372001647949, 'learning_rate': 1.9976571975931414e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9, 'grad_norm': 3.3869900703430176, 'learning_rate': 1.9970640830597595e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8595, 'grad_norm': 3.9578616619110107, 'learning_rate': 1.9964709685263776e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8414, 'grad_norm': 5.775361061096191, 'learning_rate': 1.9958778539929954e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8643, 'grad_norm': 3.141183614730835, 'learning_rate': 1.9952847394596135e-05, 'epoch': 0.01}\n",
      "{'loss': 0.866, 'grad_norm': 4.083657264709473, 'learning_rate': 1.9946916249262315e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8111, 'grad_norm': 2.9297542572021484, 'learning_rate': 1.9940985103928496e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7895, 'grad_norm': 3.253199338912964, 'learning_rate': 1.9935053958594677e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7746, 'grad_norm': 2.924509048461914, 'learning_rate': 1.9929122813260858e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7787, 'grad_norm': 3.0118038654327393, 'learning_rate': 1.992319166792704e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7508, 'grad_norm': 1.8728883266448975, 'learning_rate': 1.9917260522593217e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7959, 'grad_norm': 3.4230308532714844, 'learning_rate': 1.9911329377259397e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7698, 'grad_norm': 3.00016450881958, 'learning_rate': 1.990539823192558e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7324, 'grad_norm': 3.109696388244629, 'learning_rate': 1.9899467086591756e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7245, 'grad_norm': 2.4220995903015137, 'learning_rate': 1.9893535941257937e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7411, 'grad_norm': 2.5265707969665527, 'learning_rate': 1.9887604795924118e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7077, 'grad_norm': 3.25948429107666, 'learning_rate': 1.98816736505903e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7212, 'grad_norm': 2.7606797218322754, 'learning_rate': 1.987574250525648e-05, 'epoch': 0.02}\n",
      "{'loss': 0.707, 'grad_norm': 2.677105188369751, 'learning_rate': 1.986981135992266e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6904, 'grad_norm': 3.1102168560028076, 'learning_rate': 1.986388021458884e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6924, 'grad_norm': 2.230022430419922, 'learning_rate': 1.985794906925502e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6799, 'grad_norm': 3.388925552368164, 'learning_rate': 1.98520179239212e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6717, 'grad_norm': 3.0085134506225586, 'learning_rate': 1.984608677858738e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6605, 'grad_norm': 2.2859504222869873, 'learning_rate': 1.984015563325356e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6384, 'grad_norm': 2.831610679626465, 'learning_rate': 1.9834224487919742e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6548, 'grad_norm': 2.83980655670166, 'learning_rate': 1.9828293342585923e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6348, 'grad_norm': 2.010327100753784, 'learning_rate': 1.9822362197252104e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6401, 'grad_norm': 3.3882243633270264, 'learning_rate': 1.9816431051918282e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6279, 'grad_norm': 2.8333024978637695, 'learning_rate': 1.9810499906584463e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6427, 'grad_norm': 2.6151928901672363, 'learning_rate': 1.9804568761250644e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6335, 'grad_norm': 3.8452699184417725, 'learning_rate': 1.979863761591682e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6145, 'grad_norm': 2.967728853225708, 'learning_rate': 1.9792706470583002e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6737, 'grad_norm': 2.8505706787109375, 'learning_rate': 1.9786775325249183e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6322, 'grad_norm': 2.916309356689453, 'learning_rate': 1.9780844179915364e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6154, 'grad_norm': 3.2298154830932617, 'learning_rate': 1.9774913034581545e-05, 'epoch': 0.03}\n",
      "{'loss': 0.6237, 'grad_norm': 2.9979679584503174, 'learning_rate': 1.9768981889247726e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6297, 'grad_norm': 2.718062162399292, 'learning_rate': 1.9763050743913906e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6317, 'grad_norm': 3.0462310314178467, 'learning_rate': 1.9757119598580084e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6341, 'grad_norm': 2.853344202041626, 'learning_rate': 1.9751188453246265e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6097, 'grad_norm': 2.8692917823791504, 'learning_rate': 1.9745257307912446e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6169, 'grad_norm': 2.50392746925354, 'learning_rate': 1.9739326162578627e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6348, 'grad_norm': 3.2911319732666016, 'learning_rate': 1.9733395017244808e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6124, 'grad_norm': 2.207138776779175, 'learning_rate': 1.972746387191099e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6099, 'grad_norm': 3.119050979614258, 'learning_rate': 1.972153272657717e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6189, 'grad_norm': 3.3122360706329346, 'learning_rate': 1.9715601581243347e-05, 'epoch': 0.04}\n",
      "{'loss': 0.6287, 'grad_norm': 2.4241461753845215, 'learning_rate': 1.9709670435909528e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5846, 'grad_norm': 2.900542736053467, 'learning_rate': 1.970373929057571e-05, 'epoch': 0.04}\n",
      "{'loss': 0.5958, 'grad_norm': 2.549448251724243, 'learning_rate': 1.9697808145241886e-05, 'epoch': 0.05}\n",
      "{'loss': 0.581, 'grad_norm': 2.713135004043579, 'learning_rate': 1.9691876999908067e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5922, 'grad_norm': 2.670902967453003, 'learning_rate': 1.9685945854574248e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5599, 'grad_norm': 3.8177177906036377, 'learning_rate': 1.968001470924043e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6062, 'grad_norm': 2.503767490386963, 'learning_rate': 1.967408356390661e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6009, 'grad_norm': 3.083780527114868, 'learning_rate': 1.966815241857279e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5797, 'grad_norm': 2.4977619647979736, 'learning_rate': 1.9662280584692307e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5952, 'grad_norm': 2.4057114124298096, 'learning_rate': 1.9656349439358488e-05, 'epoch': 0.05}\n",
      "{'loss': 0.6005, 'grad_norm': 2.0295557975769043, 'learning_rate': 1.965041829402467e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5593, 'grad_norm': 2.844735860824585, 'learning_rate': 1.964448714869085e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5598, 'grad_norm': 2.3606600761413574, 'learning_rate': 1.963855600335703e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5656, 'grad_norm': 4.205491542816162, 'learning_rate': 1.963262485802321e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5827, 'grad_norm': 3.0425052642822266, 'learning_rate': 1.9626693712689392e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5876, 'grad_norm': 3.608137607574463, 'learning_rate': 1.962076256735557e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.578, 'grad_norm': 3.243757963180542, 'learning_rate': 1.961483142202175e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5747, 'grad_norm': 2.8364551067352295, 'learning_rate': 1.960890027668793e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5315, 'grad_norm': 2.314417600631714, 'learning_rate': 1.9602969131354112e-05, 'epoch': 0.06}\n",
      "{'loss': 0.591, 'grad_norm': 2.5494232177734375, 'learning_rate': 1.9597037986020293e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5435, 'grad_norm': 2.9632787704467773, 'learning_rate': 1.959110684068647e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5598, 'grad_norm': 3.134343385696411, 'learning_rate': 1.9585175695352652e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5435, 'grad_norm': 2.633174180984497, 'learning_rate': 1.9579244550018833e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5566, 'grad_norm': 7.469795227050781, 'learning_rate': 1.9573313404685014e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5584, 'grad_norm': 2.4311211109161377, 'learning_rate': 1.9567382259351194e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5523, 'grad_norm': 2.0153939723968506, 'learning_rate': 1.9561451114017372e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5559, 'grad_norm': 2.876981019973755, 'learning_rate': 1.9555519968683553e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5567, 'grad_norm': 5.506684303283691, 'learning_rate': 1.9549588823349734e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5413, 'grad_norm': 3.261019229888916, 'learning_rate': 1.9543657678015915e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5825, 'grad_norm': 3.0937113761901855, 'learning_rate': 1.9537726532682096e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5421, 'grad_norm': 3.848369598388672, 'learning_rate': 1.9531795387348276e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5548, 'grad_norm': 2.9907026290893555, 'learning_rate': 1.9525864242014457e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5272, 'grad_norm': 2.1920197010040283, 'learning_rate': 1.9519933096680638e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5805, 'grad_norm': 2.370675563812256, 'learning_rate': 1.9514001951346816e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5443, 'grad_norm': 2.769005298614502, 'learning_rate': 1.9508070806012997e-05, 'epoch': 0.07}\n",
      "{'loss': 0.5219, 'grad_norm': 2.9496538639068604, 'learning_rate': 1.9502139660679178e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5354, 'grad_norm': 2.5053188800811768, 'learning_rate': 1.949620851534536e-05, 'epoch': 0.08}\n",
      "{'loss': 0.547, 'grad_norm': 2.2436490058898926, 'learning_rate': 1.9490277370011536e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5259, 'grad_norm': 2.2319350242614746, 'learning_rate': 1.9484346224677717e-05, 'epoch': 0.08}\n",
      "{'loss': 0.558, 'grad_norm': 2.51754093170166, 'learning_rate': 1.9478415079343898e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5601, 'grad_norm': 2.1842355728149414, 'learning_rate': 1.947248393401008e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5184, 'grad_norm': 2.416459798812866, 'learning_rate': 1.946655278867626e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5108, 'grad_norm': 1.7957103252410889, 'learning_rate': 1.946062164334244e-05, 'epoch': 0.08}\n",
      "{'loss': 0.535, 'grad_norm': 2.8810203075408936, 'learning_rate': 1.9454690498008618e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5663, 'grad_norm': 3.1071972846984863, 'learning_rate': 1.94487593526748e-05, 'epoch': 0.08}\n",
      "{'loss': 0.52, 'grad_norm': 3.6492738723754883, 'learning_rate': 1.944282820734098e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5158, 'grad_norm': 3.2264957427978516, 'learning_rate': 1.943689706200716e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5065, 'grad_norm': 2.410290479660034, 'learning_rate': 1.943096591667334e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4979, 'grad_norm': 2.1438276767730713, 'learning_rate': 1.9425034771339523e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5439, 'grad_norm': 3.294696569442749, 'learning_rate': 1.9419103626005703e-05, 'epoch': 0.09}\n",
      "{'loss': 0.512, 'grad_norm': 1.863102912902832, 'learning_rate': 1.941317248067188e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5186, 'grad_norm': 2.042496681213379, 'learning_rate': 1.9407241335338062e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5176, 'grad_norm': 3.00091552734375, 'learning_rate': 1.9401310190004243e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5263, 'grad_norm': 2.469146490097046, 'learning_rate': 1.9395379044670424e-05, 'epoch': 0.09}\n",
      "{'loss': 0.51, 'grad_norm': 2.2785303592681885, 'learning_rate': 1.93894478993366e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5064, 'grad_norm': 2.404634714126587, 'learning_rate': 1.9383516754002782e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5238, 'grad_norm': 2.4649603366851807, 'learning_rate': 1.9377585608668963e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5228, 'grad_norm': 3.197065591812134, 'learning_rate': 1.9371654463335144e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5237, 'grad_norm': 2.56575870513916, 'learning_rate': 1.9365723318001325e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5261, 'grad_norm': 2.566969156265259, 'learning_rate': 1.9359792172667506e-05, 'epoch': 0.1}\n",
      "{'loss': 0.52, 'grad_norm': 2.2852046489715576, 'learning_rate': 1.9353861027333683e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5169, 'grad_norm': 1.8827929496765137, 'learning_rate': 1.9347929881999864e-05, 'epoch': 0.1}\n",
      "{'loss': 0.512, 'grad_norm': 3.3066794872283936, 'learning_rate': 1.9341998736666045e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5164, 'grad_norm': 2.6800248622894287, 'learning_rate': 1.9336067591332226e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5148, 'grad_norm': 2.5579283237457275, 'learning_rate': 1.9330136445998407e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5061, 'grad_norm': 1.7916135787963867, 'learning_rate': 1.9324205300664588e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5069, 'grad_norm': 2.8646299839019775, 'learning_rate': 1.931827415533077e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5241, 'grad_norm': 1.8332319259643555, 'learning_rate': 1.9312343009996946e-05, 'epoch': 0.1}\n",
      "{'loss': 0.49, 'grad_norm': 3.01641845703125, 'learning_rate': 1.9306471176116466e-05, 'epoch': 0.1}\n",
      "{'loss': 0.4948, 'grad_norm': 3.5710396766662598, 'learning_rate': 1.9300540030782646e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5137, 'grad_norm': 2.4583775997161865, 'learning_rate': 1.9294608885448827e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5116, 'grad_norm': 2.045036792755127, 'learning_rate': 1.9288677740115008e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4989, 'grad_norm': 2.8851544857025146, 'learning_rate': 1.928274659478119e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5106, 'grad_norm': 2.490368127822876, 'learning_rate': 1.9276815449447367e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4804, 'grad_norm': 2.843473434448242, 'learning_rate': 1.9270884304113548e-05, 'epoch': 0.11}\n",
      "{'loss': 0.4959, 'grad_norm': 2.0380985736846924, 'learning_rate': 1.9264953158779725e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4893, 'grad_norm': 2.550859212875366, 'learning_rate': 1.9259022013445906e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5028, 'grad_norm': 11.629587173461914, 'learning_rate': 1.9253090868112087e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5066, 'grad_norm': 2.207742214202881, 'learning_rate': 1.9247159722778268e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5181, 'grad_norm': 2.173912286758423, 'learning_rate': 1.924122857744445e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5204, 'grad_norm': 2.359933853149414, 'learning_rate': 1.923529743211063e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5038, 'grad_norm': 1.9710197448730469, 'learning_rate': 1.922936628677681e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4774, 'grad_norm': 1.5115958452224731, 'learning_rate': 1.922343514144299e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4849, 'grad_norm': 2.2605981826782227, 'learning_rate': 1.921750399610917e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4884, 'grad_norm': 2.558304786682129, 'learning_rate': 1.921157285077535e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4749, 'grad_norm': 2.127054214477539, 'learning_rate': 1.920564170544153e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4996, 'grad_norm': 1.799004316329956, 'learning_rate': 1.919971056010771e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5073, 'grad_norm': 2.8519113063812256, 'learning_rate': 1.9193779414773893e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4991, 'grad_norm': 2.545652389526367, 'learning_rate': 1.9187848269440073e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4885, 'grad_norm': 2.451232671737671, 'learning_rate': 1.9181917124106254e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4734, 'grad_norm': 2.177121639251709, 'learning_rate': 1.9175985978772432e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5004, 'grad_norm': 2.73677396774292, 'learning_rate': 1.9170054833438613e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4611, 'grad_norm': 2.032421350479126, 'learning_rate': 1.9164123688104794e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5042, 'grad_norm': 1.6511331796646118, 'learning_rate': 1.915819254277097e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5028, 'grad_norm': 2.651021957397461, 'learning_rate': 1.9152261397437152e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4547, 'grad_norm': 2.9002552032470703, 'learning_rate': 1.9146330252103333e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4874, 'grad_norm': 2.2922167778015137, 'learning_rate': 1.9140399106769514e-05, 'epoch': 0.13}\n",
      "{'loss': 0.464, 'grad_norm': 2.550293207168579, 'learning_rate': 1.9134467961435695e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4875, 'grad_norm': 1.7576665878295898, 'learning_rate': 1.9128536816101876e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4661, 'grad_norm': 2.466125726699829, 'learning_rate': 1.9122605670768057e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4732, 'grad_norm': 2.2568576335906982, 'learning_rate': 1.9116674525434234e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4862, 'grad_norm': 2.2607386112213135, 'learning_rate': 1.9110743380100415e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4862, 'grad_norm': 1.7705650329589844, 'learning_rate': 1.9104812234766596e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4971, 'grad_norm': 2.054584503173828, 'learning_rate': 1.9098940400886115e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4849, 'grad_norm': 2.3351004123687744, 'learning_rate': 1.909306856700563e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4526, 'grad_norm': 2.2308056354522705, 'learning_rate': 1.9087137421671812e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4619, 'grad_norm': 2.5128700733184814, 'learning_rate': 1.9081206276337993e-05, 'epoch': 0.14}\n",
      "{'loss': 0.458, 'grad_norm': 1.8914377689361572, 'learning_rate': 1.9075275131004174e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4874, 'grad_norm': 3.0955214500427246, 'learning_rate': 1.9069343985670355e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4683, 'grad_norm': 1.9478356838226318, 'learning_rate': 1.9063412840336536e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4807, 'grad_norm': 2.2018964290618896, 'learning_rate': 1.9057481695002717e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4892, 'grad_norm': 1.986523985862732, 'learning_rate': 1.9051550549668894e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4718, 'grad_norm': 1.4707536697387695, 'learning_rate': 1.9045619404335075e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4716, 'grad_norm': 1.9485070705413818, 'learning_rate': 1.9039688259001256e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4821, 'grad_norm': 2.393315315246582, 'learning_rate': 1.9033757113667437e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4872, 'grad_norm': 1.674933910369873, 'learning_rate': 1.9027825968333618e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4787, 'grad_norm': 2.673353672027588, 'learning_rate': 1.90218948229998e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4753, 'grad_norm': 2.127371072769165, 'learning_rate': 1.9015963677665976e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4812, 'grad_norm': 2.9179303646087646, 'learning_rate': 1.9010032532332157e-05, 'epoch': 0.15}\n",
      "{'loss': 0.472, 'grad_norm': 2.148667335510254, 'learning_rate': 1.9004101386998338e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4921, 'grad_norm': 2.2805919647216797, 'learning_rate': 1.899817024166452e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4806, 'grad_norm': 2.2790019512176514, 'learning_rate': 1.8992239096330696e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4489, 'grad_norm': 2.8209097385406494, 'learning_rate': 1.8986307950996877e-05, 'epoch': 0.15}\n",
      "{'loss': 0.44, 'grad_norm': 1.8506195545196533, 'learning_rate': 1.8980376805663058e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4666, 'grad_norm': 2.5276408195495605, 'learning_rate': 1.897444566032924e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4792, 'grad_norm': 3.027158498764038, 'learning_rate': 1.896851451499542e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.473, 'grad_norm': 2.1614978313446045, 'learning_rate': 1.89625833696616e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4491, 'grad_norm': 1.8077006340026855, 'learning_rate': 1.8956652224327782e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4851, 'grad_norm': 2.5054006576538086, 'learning_rate': 1.895072107899396e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4369, 'grad_norm': 2.571197986602783, 'learning_rate': 1.894478993366014e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4475, 'grad_norm': 2.7415575981140137, 'learning_rate': 1.893885878832632e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4639, 'grad_norm': 2.885089635848999, 'learning_rate': 1.8932927642992502e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4575, 'grad_norm': 2.5217597484588623, 'learning_rate': 1.8926996497658683e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4376, 'grad_norm': 2.495924234390259, 'learning_rate': 1.8921065352324864e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4568, 'grad_norm': 2.501075029373169, 'learning_rate': 1.891513420699104e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4616, 'grad_norm': 2.4541025161743164, 'learning_rate': 1.8909203061657222e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4588, 'grad_norm': 1.4776859283447266, 'learning_rate': 1.8903271916323403e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4404, 'grad_norm': 2.928995132446289, 'learning_rate': 1.8897340770989584e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4614, 'grad_norm': 2.0480825901031494, 'learning_rate': 1.889140962565576e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4432, 'grad_norm': 2.180323362350464, 'learning_rate': 1.8885478480321943e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4307, 'grad_norm': 2.6104307174682617, 'learning_rate': 1.8879547334988123e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.448, 'grad_norm': 2.372131586074829, 'learning_rate': 1.8873675501107643e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4661, 'grad_norm': 2.0359222888946533, 'learning_rate': 1.8867744355773824e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4588, 'grad_norm': 6.476291179656982, 'learning_rate': 1.8861813210440005e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4639, 'grad_norm': 2.843092918395996, 'learning_rate': 1.8855882065106182e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4495, 'grad_norm': 2.417572021484375, 'learning_rate': 1.8849950919772363e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.454, 'grad_norm': 2.443451166152954, 'learning_rate': 1.8844019774438544e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4267, 'grad_norm': 1.92769455909729, 'learning_rate': 1.8838088629104725e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4603, 'grad_norm': 2.3044357299804688, 'learning_rate': 1.8832157483770906e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4373, 'grad_norm': 2.2657361030578613, 'learning_rate': 1.8826226338437087e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4534, 'grad_norm': 2.820500373840332, 'learning_rate': 1.8820295193103268e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4508, 'grad_norm': 3.1729238033294678, 'learning_rate': 1.8814364047769445e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4739, 'grad_norm': 1.609587550163269, 'learning_rate': 1.8808432902435626e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4775, 'grad_norm': 2.2818799018859863, 'learning_rate': 1.8802501757101807e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4556, 'grad_norm': 2.672917127609253, 'learning_rate': 1.8796570611767988e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4287, 'grad_norm': 2.635133981704712, 'learning_rate': 1.8790639466434165e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.445, 'grad_norm': 1.5855945348739624, 'learning_rate': 1.8784708321100346e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4387, 'grad_norm': 2.3798868656158447, 'learning_rate': 1.8778777175766527e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4779, 'grad_norm': 3.1932201385498047, 'learning_rate': 1.8772846030432708e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4134, 'grad_norm': 2.3688764572143555, 'learning_rate': 1.876691488509889e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4295, 'grad_norm': 1.1698817014694214, 'learning_rate': 1.876098373976507e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4492, 'grad_norm': 2.660353422164917, 'learning_rate': 1.8755052594431247e-05, 'epoch': 0.19}\n",
      "{'loss': 0.465, 'grad_norm': 2.3369507789611816, 'learning_rate': 1.8749121449097428e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4359, 'grad_norm': 2.1307497024536133, 'learning_rate': 1.874319030376361e-05, 'epoch': 0.19}\n",
      "{'loss': 0.473, 'grad_norm': 1.7931989431381226, 'learning_rate': 1.873725915842979e-05, 'epoch': 0.19}\n",
      "{'loss': 0.453, 'grad_norm': 2.770599126815796, 'learning_rate': 1.873132801309597e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4304, 'grad_norm': 2.2866108417510986, 'learning_rate': 1.8725396867762152e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4564, 'grad_norm': 2.209906578063965, 'learning_rate': 1.8719465722428333e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4339, 'grad_norm': 1.8790031671524048, 'learning_rate': 1.8713534577094514e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4334, 'grad_norm': 2.216200828552246, 'learning_rate': 1.870760343176069e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4586, 'grad_norm': 2.618943214416504, 'learning_rate': 1.8701672286426872e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4474, 'grad_norm': 1.6886522769927979, 'learning_rate': 1.8695741141093053e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4305, 'grad_norm': 1.9885073900222778, 'learning_rate': 1.868980999575923e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4511, 'grad_norm': 2.2222254276275635, 'learning_rate': 1.868387885042541e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4334, 'grad_norm': 2.381234884262085, 'learning_rate': 1.8677947705091592e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4354, 'grad_norm': 2.260566473007202, 'learning_rate': 1.8672016559757773e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4505, 'grad_norm': 2.493352174758911, 'learning_rate': 1.8666085414423954e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4474, 'grad_norm': 2.585531711578369, 'learning_rate': 1.8660154269090135e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4505, 'grad_norm': 2.406802177429199, 'learning_rate': 1.8654223123756316e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4451, 'grad_norm': 2.5846633911132812, 'learning_rate': 1.8648291978422493e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4222, 'grad_norm': 2.0608441829681396, 'learning_rate': 1.8642360833088674e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4166, 'grad_norm': 2.046593189239502, 'learning_rate': 1.8636429687754855e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4581, 'grad_norm': 7.881505489349365, 'learning_rate': 1.8630498542421036e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4383, 'grad_norm': 2.6234328746795654, 'learning_rate': 1.8624567397087217e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4148, 'grad_norm': 1.8497395515441895, 'learning_rate': 1.8618636251753398e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4394, 'grad_norm': 2.3942089080810547, 'learning_rate': 1.861270510641958e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4083, 'grad_norm': 2.9217090606689453, 'learning_rate': 1.8606773961085756e-05, 'epoch': 0.21}\n",
      "{'loss': 0.405, 'grad_norm': 2.7269034385681152, 'learning_rate': 1.8600842815751937e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4638, 'grad_norm': 2.0930752754211426, 'learning_rate': 1.8594911670418118e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4494, 'grad_norm': 1.5727914571762085, 'learning_rate': 1.8588980525084296e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4305, 'grad_norm': 2.2070631980895996, 'learning_rate': 1.858310869120382e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4387, 'grad_norm': 2.1650331020355225, 'learning_rate': 1.857717754587e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4126, 'grad_norm': 2.4148995876312256, 'learning_rate': 1.8571246400536177e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4282, 'grad_norm': 2.737359046936035, 'learning_rate': 1.8565315255202358e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4448, 'grad_norm': 2.2513444423675537, 'learning_rate': 1.8559384109868535e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4237, 'grad_norm': 3.0297060012817383, 'learning_rate': 1.8553452964534716e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.458, 'grad_norm': 2.031323194503784, 'learning_rate': 1.8547521819200897e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4092, 'grad_norm': 1.8705799579620361, 'learning_rate': 1.8541590673867078e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4222, 'grad_norm': 2.3361926078796387, 'learning_rate': 1.853565952853326e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4382, 'grad_norm': 2.36301326751709, 'learning_rate': 1.852972838319944e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4253, 'grad_norm': 2.975339412689209, 'learning_rate': 1.852379723786562e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4508, 'grad_norm': 1.5819735527038574, 'learning_rate': 1.8517866092531798e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4439, 'grad_norm': 1.5944560766220093, 'learning_rate': 1.851193494719798e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4596, 'grad_norm': 2.140120506286621, 'learning_rate': 1.850600380186416e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4445, 'grad_norm': 1.9413673877716064, 'learning_rate': 1.850007265653034e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4293, 'grad_norm': 3.0696239471435547, 'learning_rate': 1.8494141511196522e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4357, 'grad_norm': 2.031864643096924, 'learning_rate': 1.8488210365862703e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4217, 'grad_norm': 2.4777274131774902, 'learning_rate': 1.8482279220528884e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4455, 'grad_norm': 1.7000889778137207, 'learning_rate': 1.8476348075195065e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4373, 'grad_norm': 2.0279250144958496, 'learning_rate': 1.8470416929861242e-05, 'epoch': 0.23}\n",
      "{'loss': 0.417, 'grad_norm': 2.018516778945923, 'learning_rate': 1.8464485784527423e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.437, 'grad_norm': 2.9107425212860107, 'learning_rate': 1.84585546391936e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4408, 'grad_norm': 2.4243898391723633, 'learning_rate': 1.845262349385978e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4148, 'grad_norm': 3.1288352012634277, 'learning_rate': 1.8446692348525962e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4357, 'grad_norm': 1.5807075500488281, 'learning_rate': 1.8440761203192143e-05, 'epoch': 0.23}\n",
      "{'loss': 0.4612, 'grad_norm': 1.4305576086044312, 'learning_rate': 1.8434830057858324e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4062, 'grad_norm': 2.192260265350342, 'learning_rate': 1.8428898912524505e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4118, 'grad_norm': 2.160493850708008, 'learning_rate': 1.8422967767190686e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4362, 'grad_norm': 2.269434928894043, 'learning_rate': 1.8417036621856867e-05, 'epoch': 0.24}\n",
      "{'loss': 0.424, 'grad_norm': 2.4426522254943848, 'learning_rate': 1.8411105476523044e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4179, 'grad_norm': 2.8144097328186035, 'learning_rate': 1.8405174331189225e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4594, 'grad_norm': 1.8273793458938599, 'learning_rate': 1.8399243185855406e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4112, 'grad_norm': 1.8728978633880615, 'learning_rate': 1.8393312040521587e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4253, 'grad_norm': 2.061439037322998, 'learning_rate': 1.8387380895187768e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4039, 'grad_norm': 2.191592216491699, 'learning_rate': 1.8381509061307284e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4211, 'grad_norm': 2.416334390640259, 'learning_rate': 1.8375577915973465e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4104, 'grad_norm': 1.85015869140625, 'learning_rate': 1.8369646770639646e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4479, 'grad_norm': 2.2800981998443604, 'learning_rate': 1.8363715625305827e-05, 'epoch': 0.25}\n",
      "{'loss': 0.448, 'grad_norm': 2.9768927097320557, 'learning_rate': 1.8357784479972008e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4232, 'grad_norm': 2.259608030319214, 'learning_rate': 1.835185333463819e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4386, 'grad_norm': 2.344651699066162, 'learning_rate': 1.834592218930437e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4307, 'grad_norm': 1.746439814567566, 'learning_rate': 1.8339991043970547e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4099, 'grad_norm': 2.0940394401550293, 'learning_rate': 1.8334059898636728e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4124, 'grad_norm': 2.1678998470306396, 'learning_rate': 1.832812875330291e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4283, 'grad_norm': 1.6376458406448364, 'learning_rate': 1.8322197607969086e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4512, 'grad_norm': 2.7860817909240723, 'learning_rate': 1.8316266462635267e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4127, 'grad_norm': 2.0126938819885254, 'learning_rate': 1.8310335317301448e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4183, 'grad_norm': 2.3563778400421143, 'learning_rate': 1.830440417196763e-05, 'epoch': 0.25}\n",
      "{'loss': 0.4166, 'grad_norm': 2.456834316253662, 'learning_rate': 1.829847302663381e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4371, 'grad_norm': 2.090733766555786, 'learning_rate': 1.829254188129999e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4005, 'grad_norm': 2.2060651779174805, 'learning_rate': 1.828661073596617e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4066, 'grad_norm': 1.8480498790740967, 'learning_rate': 1.8280679590632352e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4181, 'grad_norm': 3.3398780822753906, 'learning_rate': 1.827474844529853e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4, 'grad_norm': 1.7411147356033325, 'learning_rate': 1.826881729996471e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4282, 'grad_norm': 2.7912511825561523, 'learning_rate': 1.8262886154630892e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4147, 'grad_norm': 2.0186681747436523, 'learning_rate': 1.8256955009297073e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4407, 'grad_norm': 2.366541862487793, 'learning_rate': 1.8251023863963254e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4394, 'grad_norm': 2.425318956375122, 'learning_rate': 1.824509271862943e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4195, 'grad_norm': 2.2024104595184326, 'learning_rate': 1.8239161573295612e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4284, 'grad_norm': 2.5050721168518066, 'learning_rate': 1.8233230427961793e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4516, 'grad_norm': 2.5967743396759033, 'learning_rate': 1.8227299282627974e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4231, 'grad_norm': 2.5090491771698, 'learning_rate': 1.8221368137294155e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4265, 'grad_norm': 2.361438274383545, 'learning_rate': 1.8215436991960332e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4237, 'grad_norm': 3.001289129257202, 'learning_rate': 1.8209505846626513e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4061, 'grad_norm': 2.8402700424194336, 'learning_rate': 1.8203574701292694e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4088, 'grad_norm': 2.4224703311920166, 'learning_rate': 1.8197643555958875e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4309, 'grad_norm': 1.5025479793548584, 'learning_rate': 1.8191712410625056e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4137, 'grad_norm': 2.299410820007324, 'learning_rate': 1.8185781265291237e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4035, 'grad_norm': 1.7582205533981323, 'learning_rate': 1.8179850119957418e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4195, 'grad_norm': 1.8942452669143677, 'learning_rate': 1.8173918974623595e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4438, 'grad_norm': 1.9216772317886353, 'learning_rate': 1.8167987829289776e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4073, 'grad_norm': 2.014928102493286, 'learning_rate': 1.8162056683955957e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4217, 'grad_norm': 1.9836680889129639, 'learning_rate': 1.8156125538622138e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4093, 'grad_norm': 1.742180347442627, 'learning_rate': 1.815019439328832e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4091, 'grad_norm': 1.6730554103851318, 'learning_rate': 1.8144263247954496e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4135, 'grad_norm': 2.0642950534820557, 'learning_rate': 1.8138332102620677e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4194, 'grad_norm': 2.1718697547912598, 'learning_rate': 1.8132400957286858e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4052, 'grad_norm': 2.3516392707824707, 'learning_rate': 1.812646981195304e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4173, 'grad_norm': 2.97162127494812, 'learning_rate': 1.812059797807256e-05, 'epoch': 0.28}\n",
      "{'loss': 0.438, 'grad_norm': 2.2326433658599854, 'learning_rate': 1.8114666832738736e-05, 'epoch': 0.28}\n",
      "{'loss': 0.404, 'grad_norm': 2.0474624633789062, 'learning_rate': 1.8108735687404917e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3889, 'grad_norm': 1.5088648796081543, 'learning_rate': 1.8102804542071098e-05, 'epoch': 0.28}\n",
      "{'loss': 0.4019, 'grad_norm': 2.238093376159668, 'learning_rate': 1.809687339673728e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3976, 'grad_norm': 2.4459638595581055, 'learning_rate': 1.809094225140346e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3925, 'grad_norm': 1.813773512840271, 'learning_rate': 1.808501110606964e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4221, 'grad_norm': 2.360584259033203, 'learning_rate': 1.8079079960735818e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4002, 'grad_norm': 1.8864833116531372, 'learning_rate': 1.8073148815402e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4078, 'grad_norm': 2.1642632484436035, 'learning_rate': 1.806721767006818e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4293, 'grad_norm': 2.0593016147613525, 'learning_rate': 1.806128652473436e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4171, 'grad_norm': 1.9720715284347534, 'learning_rate': 1.805535537940054e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3958, 'grad_norm': 1.826672077178955, 'learning_rate': 1.8049424234066722e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4063, 'grad_norm': 2.2025396823883057, 'learning_rate': 1.8043493088732903e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4212, 'grad_norm': 2.0050010681152344, 'learning_rate': 1.803756194339908e-05, 'epoch': 0.29}\n",
      "{'loss': 0.4132, 'grad_norm': 1.9596327543258667, 'learning_rate': 1.8031630798065262e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4258, 'grad_norm': 1.9984140396118164, 'learning_rate': 1.8025699652731443e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4134, 'grad_norm': 2.1642730236053467, 'learning_rate': 1.801976850739762e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4088, 'grad_norm': 2.017657518386841, 'learning_rate': 1.80138373620638e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3864, 'grad_norm': 2.6229310035705566, 'learning_rate': 1.800796552818332e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3675, 'grad_norm': 2.5330801010131836, 'learning_rate': 1.80020343828495e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4248, 'grad_norm': 2.660526752471924, 'learning_rate': 1.7996103237515682e-05, 'epoch': 0.3}\n",
      "{'loss': 0.431, 'grad_norm': 2.708709716796875, 'learning_rate': 1.799017209218186e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4213, 'grad_norm': 2.47245192527771, 'learning_rate': 1.798424094684804e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4176, 'grad_norm': 2.236337661743164, 'learning_rate': 1.797830980151422e-05, 'epoch': 0.3}\n",
      "{'loss': 0.4125, 'grad_norm': 2.191725492477417, 'learning_rate': 1.7972378656180403e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3827, 'grad_norm': 2.4832911491394043, 'learning_rate': 1.7966447510846583e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3922, 'grad_norm': 1.1020256280899048, 'learning_rate': 1.7960516365512764e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3825, 'grad_norm': 2.201922655105591, 'learning_rate': 1.7954585220178945e-05, 'epoch': 0.31}\n",
      "{'loss': 0.415, 'grad_norm': 2.2452356815338135, 'learning_rate': 1.7948654074845123e-05, 'epoch': 0.31}\n",
      "{'loss': 0.408, 'grad_norm': 1.7541605234146118, 'learning_rate': 1.7942722929511304e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4121, 'grad_norm': 2.4077024459838867, 'learning_rate': 1.7936791784177485e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4036, 'grad_norm': 2.1660563945770264, 'learning_rate': 1.7930860638843665e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3677, 'grad_norm': 1.6925383806228638, 'learning_rate': 1.7924929493509846e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3851, 'grad_norm': 2.2108991146087646, 'learning_rate': 1.7918998348176027e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4423, 'grad_norm': 2.1899149417877197, 'learning_rate': 1.7913067202842208e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3882, 'grad_norm': 2.1027705669403076, 'learning_rate': 1.790713605750839e-05, 'epoch': 0.31}\n",
      "{'loss': 0.4028, 'grad_norm': 2.6089675426483154, 'learning_rate': 1.7901204912174567e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4038, 'grad_norm': 2.1980371475219727, 'learning_rate': 1.7895273766840747e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4043, 'grad_norm': 2.5122923851013184, 'learning_rate': 1.7889342621506925e-05, 'epoch': 0.32}\n",
      "{'loss': 0.403, 'grad_norm': 1.9977171421051025, 'learning_rate': 1.7883411476173106e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4053, 'grad_norm': 2.5754082202911377, 'learning_rate': 1.7877480330839287e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3989, 'grad_norm': 1.7009704113006592, 'learning_rate': 1.7871549185505468e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4053, 'grad_norm': 2.3674426078796387, 'learning_rate': 1.786561804017165e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3782, 'grad_norm': 1.978393793106079, 'learning_rate': 1.785968689483783e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3966, 'grad_norm': 2.9112536907196045, 'learning_rate': 1.785375574950401e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4185, 'grad_norm': 3.284280300140381, 'learning_rate': 1.784782460417019e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4234, 'grad_norm': 1.6961950063705444, 'learning_rate': 1.784189345883637e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3931, 'grad_norm': 1.8242915868759155, 'learning_rate': 1.783596231350255e-05, 'epoch': 0.32}\n",
      "{'loss': 0.409, 'grad_norm': 1.427451252937317, 'learning_rate': 1.783003116816873e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3874, 'grad_norm': 2.2950804233551025, 'learning_rate': 1.782410002283491e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3955, 'grad_norm': 2.1041364669799805, 'learning_rate': 1.7818168877501092e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4032, 'grad_norm': 2.4712438583374023, 'learning_rate': 1.7812237732167273e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3938, 'grad_norm': 2.6106460094451904, 'learning_rate': 1.7806306586833454e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3867, 'grad_norm': 1.9046459197998047, 'learning_rate': 1.7800375441499632e-05, 'epoch': 0.33}\n",
      "{'loss': 0.394, 'grad_norm': 2.571293830871582, 'learning_rate': 1.7794444296165813e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3922, 'grad_norm': 2.1078832149505615, 'learning_rate': 1.7788513150831994e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4071, 'grad_norm': 2.3096797466278076, 'learning_rate': 1.778258200549817e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3868, 'grad_norm': 2.0894851684570312, 'learning_rate': 1.7776650860164352e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3889, 'grad_norm': 1.5693081617355347, 'learning_rate': 1.7770719714830533e-05, 'epoch': 0.33}\n",
      "{'loss': 0.4165, 'grad_norm': 3.3045151233673096, 'learning_rate': 1.7764788569496714e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3981, 'grad_norm': 2.248222827911377, 'learning_rate': 1.7758857424162895e-05, 'epoch': 0.34}\n",
      "{'loss': 0.387, 'grad_norm': 2.366384267807007, 'learning_rate': 1.7752926278829076e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3751, 'grad_norm': 3.0253121852874756, 'learning_rate': 1.7746995133495256e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4153, 'grad_norm': 1.5444072484970093, 'learning_rate': 1.7741063988161434e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3874, 'grad_norm': 2.2889556884765625, 'learning_rate': 1.7735132842827615e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3929, 'grad_norm': 2.4445366859436035, 'learning_rate': 1.7729201697493796e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4081, 'grad_norm': 2.5818593502044678, 'learning_rate': 1.7723270552159977e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.416, 'grad_norm': 1.7126210927963257, 'learning_rate': 1.7717339406826158e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3807, 'grad_norm': 2.2267842292785645, 'learning_rate': 1.7711467572945677e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4283, 'grad_norm': 1.9303209781646729, 'learning_rate': 1.7705536427611854e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4026, 'grad_norm': 2.094841718673706, 'learning_rate': 1.7699605282278035e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3634, 'grad_norm': 1.5620763301849365, 'learning_rate': 1.7693674136944216e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4183, 'grad_norm': 2.301804304122925, 'learning_rate': 1.7687742991610397e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3895, 'grad_norm': 1.910590410232544, 'learning_rate': 1.7681811846276578e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4065, 'grad_norm': 1.7748937606811523, 'learning_rate': 1.767588070094276e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4117, 'grad_norm': 1.5968831777572632, 'learning_rate': 1.7669949555608937e-05, 'epoch': 0.35}\n",
      "{'loss': 0.4425, 'grad_norm': 2.7399775981903076, 'learning_rate': 1.7664018410275117e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3968, 'grad_norm': 2.230098009109497, 'learning_rate': 1.76580872649413e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3752, 'grad_norm': 1.7377938032150269, 'learning_rate': 1.765215611960748e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3805, 'grad_norm': 2.589167594909668, 'learning_rate': 1.7646224974273657e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3712, 'grad_norm': 2.1430132389068604, 'learning_rate': 1.7640293828939838e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3814, 'grad_norm': 2.277146816253662, 'learning_rate': 1.763436268360602e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3812, 'grad_norm': 1.689924955368042, 'learning_rate': 1.76284315382722e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4042, 'grad_norm': 2.1231963634490967, 'learning_rate': 1.762250039293838e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4093, 'grad_norm': 2.1551597118377686, 'learning_rate': 1.761656924760456e-05, 'epoch': 0.36}\n",
      "{'loss': 0.396, 'grad_norm': 2.046947717666626, 'learning_rate': 1.7610638102270742e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4116, 'grad_norm': 2.0536434650421143, 'learning_rate': 1.760470695693692e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3785, 'grad_norm': 1.621741533279419, 'learning_rate': 1.75987758116031e-05, 'epoch': 0.36}\n",
      "{'loss': 0.397, 'grad_norm': 2.4575552940368652, 'learning_rate': 1.759284466626928e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3849, 'grad_norm': 2.285808801651001, 'learning_rate': 1.7586913520935462e-05, 'epoch': 0.36}\n",
      "{'loss': 0.4054, 'grad_norm': 1.79331374168396, 'learning_rate': 1.7580982375601643e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3694, 'grad_norm': 1.7613742351531982, 'learning_rate': 1.7575051230267824e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3904, 'grad_norm': 1.8715225458145142, 'learning_rate': 1.7569120084934002e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3872, 'grad_norm': 1.5941497087478638, 'learning_rate': 1.7563188939600183e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3818, 'grad_norm': 2.245236873626709, 'learning_rate': 1.7557317105719702e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3892, 'grad_norm': 1.7811107635498047, 'learning_rate': 1.7551385960385883e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3814, 'grad_norm': 2.287870168685913, 'learning_rate': 1.7545454815052064e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3883, 'grad_norm': 1.2207605838775635, 'learning_rate': 1.753952366971824e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3958, 'grad_norm': 2.1215696334838867, 'learning_rate': 1.7533592524384422e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3892, 'grad_norm': 2.3534440994262695, 'learning_rate': 1.7527661379050603e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3968, 'grad_norm': 1.6979155540466309, 'learning_rate': 1.7521730233716784e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3866, 'grad_norm': 2.1567957401275635, 'learning_rate': 1.751579908838296e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3624, 'grad_norm': 2.0730254650115967, 'learning_rate': 1.7509867943049142e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3998, 'grad_norm': 1.8502496480941772, 'learning_rate': 1.7503936797715323e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3901, 'grad_norm': 1.9735090732574463, 'learning_rate': 1.7498064963834843e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3845, 'grad_norm': 2.492445468902588, 'learning_rate': 1.7492133818501024e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3845, 'grad_norm': 1.7561558485031128, 'learning_rate': 1.7486202673167205e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3889, 'grad_norm': 2.1284735202789307, 'learning_rate': 1.7480271527833382e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4011, 'grad_norm': 2.631531000137329, 'learning_rate': 1.7474340382499563e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3818, 'grad_norm': 3.096311092376709, 'learning_rate': 1.7468409237165744e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4087, 'grad_norm': 2.9543776512145996, 'learning_rate': 1.7462478091831925e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3738, 'grad_norm': 1.8619526624679565, 'learning_rate': 1.7456546946498106e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3955, 'grad_norm': 2.168224811553955, 'learning_rate': 1.7450615801164287e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3763, 'grad_norm': 2.5702784061431885, 'learning_rate': 1.7444684655830467e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3923, 'grad_norm': 2.2541825771331787, 'learning_rate': 1.7438753510496645e-05, 'epoch': 0.38}\n",
      "{'loss': 0.41, 'grad_norm': 2.8149783611297607, 'learning_rate': 1.7432822365162826e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3888, 'grad_norm': 2.0659255981445312, 'learning_rate': 1.7426891219829007e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4035, 'grad_norm': 2.211329460144043, 'learning_rate': 1.7420960074495188e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3903, 'grad_norm': 2.3575825691223145, 'learning_rate': 1.7415028929161365e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3877, 'grad_norm': 2.0974838733673096, 'learning_rate': 1.7409097783827546e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3799, 'grad_norm': 2.1558103561401367, 'learning_rate': 1.7403166638493727e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3651, 'grad_norm': 2.3591253757476807, 'learning_rate': 1.7397235493159908e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3879, 'grad_norm': 1.738528847694397, 'learning_rate': 1.739130434782609e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3664, 'grad_norm': 1.7879289388656616, 'learning_rate': 1.738537320249227e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3883, 'grad_norm': 2.2452075481414795, 'learning_rate': 1.7379442057158447e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3829, 'grad_norm': 2.198707103729248, 'learning_rate': 1.7373510911824628e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3951, 'grad_norm': 2.2518179416656494, 'learning_rate': 1.736757976649081e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3687, 'grad_norm': 2.3483078479766846, 'learning_rate': 1.736164862115699e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3926, 'grad_norm': 2.190195083618164, 'learning_rate': 1.735571747582317e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3921, 'grad_norm': 3.716184377670288, 'learning_rate': 1.7349786330489352e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3843, 'grad_norm': 2.0584404468536377, 'learning_rate': 1.7343855185155533e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3968, 'grad_norm': 2.123908042907715, 'learning_rate': 1.7337924039821714e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.363, 'grad_norm': 2.1498939990997314, 'learning_rate': 1.733205220594123e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3824, 'grad_norm': 1.949447751045227, 'learning_rate': 1.732612106060741e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3952, 'grad_norm': 1.0938574075698853, 'learning_rate': 1.732018991527359e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3935, 'grad_norm': 1.5943536758422852, 'learning_rate': 1.7314258769939772e-05, 'epoch': 0.4}\n",
      "{'loss': 0.385, 'grad_norm': 1.7036802768707275, 'learning_rate': 1.7308327624605953e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3695, 'grad_norm': 2.4016077518463135, 'learning_rate': 1.730245579072547e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4035, 'grad_norm': 1.8263965845108032, 'learning_rate': 1.729652464539165e-05, 'epoch': 0.41}\n",
      "{'loss': 0.391, 'grad_norm': 2.0535173416137695, 'learning_rate': 1.729059350005783e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3873, 'grad_norm': 1.7903119325637817, 'learning_rate': 1.7284662354724012e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3955, 'grad_norm': 2.3445053100585938, 'learning_rate': 1.7278731209390193e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.388, 'grad_norm': 1.7774269580841064, 'learning_rate': 1.727280006405637e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3615, 'grad_norm': 2.2251269817352295, 'learning_rate': 1.726686891872255e-05, 'epoch': 0.41}\n",
      "{'loss': 0.382, 'grad_norm': 2.0505988597869873, 'learning_rate': 1.7260937773388732e-05, 'epoch': 0.41}\n",
      "{'loss': 0.4087, 'grad_norm': 2.086158514022827, 'learning_rate': 1.725500662805491e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3622, 'grad_norm': 2.3463563919067383, 'learning_rate': 1.724907548272109e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4176, 'grad_norm': 2.438401937484741, 'learning_rate': 1.724314433738727e-05, 'epoch': 0.41}\n",
      "{'loss': 0.4082, 'grad_norm': 2.4127883911132812, 'learning_rate': 1.7237213192053452e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3905, 'grad_norm': 2.089580535888672, 'learning_rate': 1.7231282046719633e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3879, 'grad_norm': 2.2025539875030518, 'learning_rate': 1.7225350901385814e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3762, 'grad_norm': 1.8605223894119263, 'learning_rate': 1.7219419756051995e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4028, 'grad_norm': 1.9252928495407104, 'learning_rate': 1.7213488610718173e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4005, 'grad_norm': 2.698503017425537, 'learning_rate': 1.7207557465384353e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3832, 'grad_norm': 2.391957998275757, 'learning_rate': 1.7201626320050534e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3733, 'grad_norm': 1.7564260959625244, 'learning_rate': 1.7195695174716715e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3461, 'grad_norm': 1.6546379327774048, 'learning_rate': 1.7189764029382896e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.384, 'grad_norm': 1.917283058166504, 'learning_rate': 1.7183832884049077e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4015, 'grad_norm': 2.0130302906036377, 'learning_rate': 1.7177901738715258e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3684, 'grad_norm': 2.4179229736328125, 'learning_rate': 1.717197059338144e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3705, 'grad_norm': 2.2102982997894287, 'learning_rate': 1.7166039448047616e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3816, 'grad_norm': 2.259826421737671, 'learning_rate': 1.7160108302713797e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3875, 'grad_norm': 1.9634350538253784, 'learning_rate': 1.7154177157379975e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3738, 'grad_norm': 3.4483489990234375, 'learning_rate': 1.7148246012046156e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3929, 'grad_norm': 2.3622846603393555, 'learning_rate': 1.7142314866712337e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3829, 'grad_norm': 2.4962761402130127, 'learning_rate': 1.7136383721378518e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3787, 'grad_norm': 2.4972569942474365, 'learning_rate': 1.71304525760447e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4071, 'grad_norm': 1.8606352806091309, 'learning_rate': 1.712452143071088e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3766, 'grad_norm': 1.9447057247161865, 'learning_rate': 1.711859028537706e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3758, 'grad_norm': 2.4275689125061035, 'learning_rate': 1.711265914004324e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4159, 'grad_norm': 2.216923952102661, 'learning_rate': 1.710672799470942e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3972, 'grad_norm': 2.396986722946167, 'learning_rate': 1.71007968493756e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3864, 'grad_norm': 2.828195095062256, 'learning_rate': 1.709486570404178e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3986, 'grad_norm': 1.7842249870300293, 'learning_rate': 1.708893455870796e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3692, 'grad_norm': 1.6044058799743652, 'learning_rate': 1.7083003413374142e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3966, 'grad_norm': 1.7149156332015991, 'learning_rate': 1.7077072268040323e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3725, 'grad_norm': 2.0429673194885254, 'learning_rate': 1.7071141122706504e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3958, 'grad_norm': 2.0045154094696045, 'learning_rate': 1.706520997737268e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3806, 'grad_norm': 1.7334614992141724, 'learning_rate': 1.7059278832038862e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3808, 'grad_norm': 2.2880420684814453, 'learning_rate': 1.7053347686705043e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3627, 'grad_norm': 2.2751002311706543, 'learning_rate': 1.704741654137122e-05, 'epoch': 0.44}\n",
      "{'loss': 0.376, 'grad_norm': 1.8022277355194092, 'learning_rate': 1.7041485396037402e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3749, 'grad_norm': 2.1524863243103027, 'learning_rate': 1.7035554250703583e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3781, 'grad_norm': 1.806857943534851, 'learning_rate': 1.7029623105369764e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3596, 'grad_norm': 1.3484896421432495, 'learning_rate': 1.7023691960035944e-05, 'epoch': 0.45}\n",
      "{'loss': 0.364, 'grad_norm': 1.826339840888977, 'learning_rate': 1.7017760814702125e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3717, 'grad_norm': 1.8730576038360596, 'learning_rate': 1.7011829669368306e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3884, 'grad_norm': 1.4529907703399658, 'learning_rate': 1.7005898524034484e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3718, 'grad_norm': 2.017803192138672, 'learning_rate': 1.6999967378700665e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3733, 'grad_norm': 1.9553399085998535, 'learning_rate': 1.6994095544820184e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3853, 'grad_norm': 1.58831787109375, 'learning_rate': 1.6988164399486365e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3957, 'grad_norm': 2.5211730003356934, 'learning_rate': 1.6982233254152546e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3857, 'grad_norm': 2.1955528259277344, 'learning_rate': 1.6976302108818727e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3847, 'grad_norm': 2.15006947517395, 'learning_rate': 1.6970370963484904e-05, 'epoch': 0.45}\n",
      "{'loss': 0.373, 'grad_norm': 1.5062031745910645, 'learning_rate': 1.6964439818151085e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3758, 'grad_norm': 2.000366687774658, 'learning_rate': 1.6958508672817266e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3736, 'grad_norm': 1.7717697620391846, 'learning_rate': 1.6952577527483447e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3802, 'grad_norm': 2.3540427684783936, 'learning_rate': 1.6946646382149628e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3771, 'grad_norm': 1.9272557497024536, 'learning_rate': 1.6940715236815805e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3811, 'grad_norm': 1.895914077758789, 'learning_rate': 1.6934784091481986e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3765, 'grad_norm': 1.9348554611206055, 'learning_rate': 1.6928852946148167e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3824, 'grad_norm': 1.7748079299926758, 'learning_rate': 1.6922921800814348e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3777, 'grad_norm': 2.28332781791687, 'learning_rate': 1.6917049966933868e-05, 'epoch': 0.46}\n",
      "{'loss': 0.374, 'grad_norm': 2.319164991378784, 'learning_rate': 1.6911118821600045e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3869, 'grad_norm': 3.549605131149292, 'learning_rate': 1.6905187676266226e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3733, 'grad_norm': 1.639965534210205, 'learning_rate': 1.6899256530932407e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3711, 'grad_norm': 1.5445955991744995, 'learning_rate': 1.6893325385598588e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3682, 'grad_norm': 1.9187471866607666, 'learning_rate': 1.688739424026477e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3962, 'grad_norm': 2.739193916320801, 'learning_rate': 1.6881463094930946e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3901, 'grad_norm': 2.769866466522217, 'learning_rate': 1.6875531949597127e-05, 'epoch': 0.47}\n",
      "{'loss': 0.37, 'grad_norm': 1.4473750591278076, 'learning_rate': 1.6869600804263308e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3673, 'grad_norm': 1.7326303720474243, 'learning_rate': 1.686366965892949e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3656, 'grad_norm': 2.072547674179077, 'learning_rate': 1.685773851359567e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3584, 'grad_norm': 1.5553467273712158, 'learning_rate': 1.685180736826185e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3879, 'grad_norm': 2.2201809883117676, 'learning_rate': 1.684587622292803e-05, 'epoch': 0.47}\n",
      "{'loss': 0.359, 'grad_norm': 2.456005811691284, 'learning_rate': 1.6839945077594213e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3836, 'grad_norm': 1.632621169090271, 'learning_rate': 1.683401393226039e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3961, 'grad_norm': 1.4705780744552612, 'learning_rate': 1.682808278692657e-05, 'epoch': 0.48}\n",
      "{'loss': 0.377, 'grad_norm': 2.0145070552825928, 'learning_rate': 1.6822151641592752e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3671, 'grad_norm': 1.5356566905975342, 'learning_rate': 1.681622049625893e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3776, 'grad_norm': 1.5767179727554321, 'learning_rate': 1.681028935092511e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3829, 'grad_norm': 1.3034721612930298, 'learning_rate': 1.680435820559129e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3864, 'grad_norm': 1.7830621004104614, 'learning_rate': 1.6798427060257472e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3766, 'grad_norm': 2.2714242935180664, 'learning_rate': 1.6792495914923653e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3707, 'grad_norm': 1.4563727378845215, 'learning_rate': 1.6786564769589834e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3714, 'grad_norm': 1.888696312904358, 'learning_rate': 1.678063362425601e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3767, 'grad_norm': 5.710397243499756, 'learning_rate': 1.6774702478922192e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.388, 'grad_norm': 1.9913753271102905, 'learning_rate': 1.6768771333588373e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3577, 'grad_norm': 3.0458080768585205, 'learning_rate': 1.6762840188254554e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3593, 'grad_norm': 1.8071430921554565, 'learning_rate': 1.6756909042920735e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3902, 'grad_norm': 1.7849681377410889, 'learning_rate': 1.6750977897586916e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3803, 'grad_norm': 1.9514342546463013, 'learning_rate': 1.6745046752253097e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.388, 'grad_norm': 1.4457378387451172, 'learning_rate': 1.6739115606919278e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3842, 'grad_norm': 1.5946555137634277, 'learning_rate': 1.6733184461585455e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3545, 'grad_norm': 1.356688141822815, 'learning_rate': 1.6727253316251636e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3556, 'grad_norm': 1.690506935119629, 'learning_rate': 1.6721322170917817e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3591, 'grad_norm': 1.1397610902786255, 'learning_rate': 1.6715391025583995e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3699, 'grad_norm': 1.7810015678405762, 'learning_rate': 1.6709459880250175e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3754, 'grad_norm': 2.4933791160583496, 'learning_rate': 1.6703528734916356e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3729, 'grad_norm': 1.6812647581100464, 'learning_rate': 1.6697597589582537e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3695, 'grad_norm': 1.5606590509414673, 'learning_rate': 1.6691666444248718e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3721, 'grad_norm': 1.7003353834152222, 'learning_rate': 1.66857352989149e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3486, 'grad_norm': 1.633056879043579, 'learning_rate': 1.667980415358108e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3723, 'grad_norm': 1.8702030181884766, 'learning_rate': 1.6673873008247257e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3739, 'grad_norm': 2.1542675495147705, 'learning_rate': 1.666794186291344e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3493, 'grad_norm': 1.4152032136917114, 'learning_rate': 1.666201071757962e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3878, 'grad_norm': 2.7761502265930176, 'learning_rate': 1.66560795722458e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3854, 'grad_norm': 3.119235038757324, 'learning_rate': 1.665014842691198e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3648, 'grad_norm': 1.642493486404419, 'learning_rate': 1.6644217281578162e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3931, 'grad_norm': 2.17671799659729, 'learning_rate': 1.6638286136244343e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3664, 'grad_norm': 1.8955167531967163, 'learning_rate': 1.663235499091052e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3856, 'grad_norm': 2.406870126724243, 'learning_rate': 1.66264238455767e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3642, 'grad_norm': 1.3709667921066284, 'learning_rate': 1.6620492700242882e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3763, 'grad_norm': 2.2576653957366943, 'learning_rate': 1.661456155490906e-05, 'epoch': 0.51}\n",
      "{'loss': 0.379, 'grad_norm': 2.0514283180236816, 'learning_rate': 1.660863040957524e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3782, 'grad_norm': 1.8560243844985962, 'learning_rate': 1.660269926424142e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3904, 'grad_norm': 1.6794215440750122, 'learning_rate': 1.6596768118907602e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3407, 'grad_norm': 1.5855438709259033, 'learning_rate': 1.6590836973573783e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3495, 'grad_norm': 2.2436044216156006, 'learning_rate': 1.6584905828239964e-05, 'epoch': 0.51}\n",
      "{'loss': 0.347, 'grad_norm': 2.5236213207244873, 'learning_rate': 1.6578974682906145e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3615, 'grad_norm': 1.9097228050231934, 'learning_rate': 1.6573043537572323e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3926, 'grad_norm': 2.0076744556427, 'learning_rate': 1.6567112392238504e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3601, 'grad_norm': 2.5063114166259766, 'learning_rate': 1.6561181246904684e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3533, 'grad_norm': 1.9828932285308838, 'learning_rate': 1.6555250101570865e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3408, 'grad_norm': 1.5606422424316406, 'learning_rate': 1.6549378267690385e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3722, 'grad_norm': 2.417959213256836, 'learning_rate': 1.6543447122356566e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3875, 'grad_norm': 2.1237945556640625, 'learning_rate': 1.6537515977022743e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.368, 'grad_norm': 1.6524370908737183, 'learning_rate': 1.6531584831688924e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3682, 'grad_norm': 2.3807878494262695, 'learning_rate': 1.6525653686355105e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3692, 'grad_norm': 1.8484162092208862, 'learning_rate': 1.6519722541021286e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3735, 'grad_norm': 1.9026492834091187, 'learning_rate': 1.6513791395687467e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3592, 'grad_norm': 1.7516918182373047, 'learning_rate': 1.6507860250353648e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3869, 'grad_norm': 2.146785259246826, 'learning_rate': 1.650192910501983e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3698, 'grad_norm': 2.272550582885742, 'learning_rate': 1.6495997959686006e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3668, 'grad_norm': 1.9728564023971558, 'learning_rate': 1.6490066814352187e-05, 'epoch': 0.53}\n",
      "{'loss': 0.372, 'grad_norm': 1.538106918334961, 'learning_rate': 1.6484135669018368e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3514, 'grad_norm': 2.352160930633545, 'learning_rate': 1.6478204523684545e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3566, 'grad_norm': 1.8977437019348145, 'learning_rate': 1.6472273378350726e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3655, 'grad_norm': 1.9560949802398682, 'learning_rate': 1.6466342233016907e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3657, 'grad_norm': 2.8843324184417725, 'learning_rate': 1.6460411087683088e-05, 'epoch': 0.53}\n",
      "{'loss': 0.377, 'grad_norm': 1.4097734689712524, 'learning_rate': 1.645447994234927e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3571, 'grad_norm': 1.2142678499221802, 'learning_rate': 1.644854879701545e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3561, 'grad_norm': 1.4117240905761719, 'learning_rate': 1.644261765168163e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3607, 'grad_norm': 2.6296935081481934, 'learning_rate': 1.643668650634781e-05, 'epoch': 0.53}\n",
      "{'loss': 0.3629, 'grad_norm': 2.926280975341797, 'learning_rate': 1.643075536101399e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3446, 'grad_norm': 2.0771877765655518, 'learning_rate': 1.642482421568017e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3617, 'grad_norm': 2.4353504180908203, 'learning_rate': 1.641889307034635e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3743, 'grad_norm': 3.444533109664917, 'learning_rate': 1.6412961925012532e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3484, 'grad_norm': 1.76764976978302, 'learning_rate': 1.6407030779678713e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3666, 'grad_norm': 2.0725579261779785, 'learning_rate': 1.6401099634344894e-05, 'epoch': 0.54}\n",
      "{'loss': 0.359, 'grad_norm': 1.957106351852417, 'learning_rate': 1.639516848901107e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3338, 'grad_norm': 2.2075955867767334, 'learning_rate': 1.6389237343677252e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3541, 'grad_norm': 1.9702919721603394, 'learning_rate': 1.6383306198343433e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3656, 'grad_norm': 1.761839747428894, 'learning_rate': 1.637737505300961e-05, 'epoch': 0.54}\n",
      "{'loss': 0.351, 'grad_norm': 1.8874253034591675, 'learning_rate': 1.6371503219129133e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3859, 'grad_norm': 1.868610143661499, 'learning_rate': 1.636557207379531e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3776, 'grad_norm': 2.208437204360962, 'learning_rate': 1.6359640928461492e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3407, 'grad_norm': 2.1956236362457275, 'learning_rate': 1.6353709783127673e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3801, 'grad_norm': 1.7287001609802246, 'learning_rate': 1.634777863779385e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3873, 'grad_norm': 1.8794645071029663, 'learning_rate': 1.634184749246003e-05, 'epoch': 0.55}\n",
      "{'loss': 0.381, 'grad_norm': 2.7684168815612793, 'learning_rate': 1.6335916347126212e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3739, 'grad_norm': 2.0164201259613037, 'learning_rate': 1.6329985201792393e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3415, 'grad_norm': 2.0569233894348145, 'learning_rate': 1.6324054056458574e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3327, 'grad_norm': 2.073199510574341, 'learning_rate': 1.6318122911124755e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3802, 'grad_norm': 1.9803494215011597, 'learning_rate': 1.6312191765790936e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3557, 'grad_norm': 1.6013141870498657, 'learning_rate': 1.6306260620457117e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3941, 'grad_norm': 1.6345012187957764, 'learning_rate': 1.6300329475123294e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3649, 'grad_norm': 3.200117349624634, 'learning_rate': 1.6294398329789475e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3682, 'grad_norm': 1.7237180471420288, 'learning_rate': 1.6288467184455656e-05, 'epoch': 0.56}\n",
      "{'loss': 0.353, 'grad_norm': 2.220247507095337, 'learning_rate': 1.6282536039121837e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3669, 'grad_norm': 2.3344295024871826, 'learning_rate': 1.6276604893788018e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3735, 'grad_norm': 1.8313804864883423, 'learning_rate': 1.62706737484542e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3637, 'grad_norm': 1.7380326986312866, 'learning_rate': 1.6264742603120376e-05, 'epoch': 0.56}\n",
      "{'loss': 0.366, 'grad_norm': 2.0236949920654297, 'learning_rate': 1.6258811457786557e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3519, 'grad_norm': 2.192857503890991, 'learning_rate': 1.6252880312452738e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3564, 'grad_norm': 1.6869547367095947, 'learning_rate': 1.624694916711892e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3467, 'grad_norm': 2.4656128883361816, 'learning_rate': 1.6241018021785096e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3433, 'grad_norm': 1.728581190109253, 'learning_rate': 1.6235086876451277e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3538, 'grad_norm': 1.885148525238037, 'learning_rate': 1.6229155731117458e-05, 'epoch': 0.57}\n",
      "{'loss': 0.383, 'grad_norm': 1.5654834508895874, 'learning_rate': 1.622322458578364e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3495, 'grad_norm': 1.3229765892028809, 'learning_rate': 1.621729344044982e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3507, 'grad_norm': 1.4417263269424438, 'learning_rate': 1.6211362295116e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3647, 'grad_norm': 2.5946717262268066, 'learning_rate': 1.6205490461235517e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3774, 'grad_norm': 2.2923531532287598, 'learning_rate': 1.6199559315901698e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3564, 'grad_norm': 2.0324132442474365, 'learning_rate': 1.619362817056788e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3702, 'grad_norm': 1.9928278923034668, 'learning_rate': 1.618769702523406e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3764, 'grad_norm': 2.426102876663208, 'learning_rate': 1.618176587990024e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3528, 'grad_norm': 2.257411241531372, 'learning_rate': 1.617583473456642e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3891, 'grad_norm': 2.2871615886688232, 'learning_rate': 1.6169903589232602e-05, 'epoch': 0.57}\n",
      "{'loss': 0.3721, 'grad_norm': 2.0278384685516357, 'learning_rate': 1.616397244389878e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3696, 'grad_norm': 1.7833428382873535, 'learning_rate': 1.615804129856496e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3503, 'grad_norm': 2.080840587615967, 'learning_rate': 1.615211015323114e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3376, 'grad_norm': 2.1318185329437256, 'learning_rate': 1.6146179007897322e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3492, 'grad_norm': 2.0537896156311035, 'learning_rate': 1.61402478625635e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3566, 'grad_norm': 1.8310790061950684, 'learning_rate': 1.613431671722968e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3444, 'grad_norm': 2.141641139984131, 'learning_rate': 1.6128385571895862e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3739, 'grad_norm': 2.1629908084869385, 'learning_rate': 1.6122454426562043e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3559, 'grad_norm': 2.012523651123047, 'learning_rate': 1.6116523281228224e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3507, 'grad_norm': 1.8908947706222534, 'learning_rate': 1.6110592135894404e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3563, 'grad_norm': 1.610229253768921, 'learning_rate': 1.6104660990560582e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3524, 'grad_norm': 1.5104707479476929, 'learning_rate': 1.6098729845226763e-05, 'epoch': 0.59}\n",
      "{'loss': 0.371, 'grad_norm': 1.7661802768707275, 'learning_rate': 1.6092798699892944e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3353, 'grad_norm': 1.7962887287139893, 'learning_rate': 1.6086867554559125e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3461, 'grad_norm': 1.8086053133010864, 'learning_rate': 1.6080936409225306e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3632, 'grad_norm': 2.26593017578125, 'learning_rate': 1.607506457534482e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3693, 'grad_norm': 2.264059543609619, 'learning_rate': 1.6069133430011002e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3588, 'grad_norm': 1.7612611055374146, 'learning_rate': 1.6063202284677183e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3676, 'grad_norm': 2.2885115146636963, 'learning_rate': 1.6057271139343364e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3505, 'grad_norm': 2.109121799468994, 'learning_rate': 1.6051339994009545e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3471, 'grad_norm': 1.4695359468460083, 'learning_rate': 1.6045408848675726e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3691, 'grad_norm': 2.1247634887695312, 'learning_rate': 1.6039477703341907e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3372, 'grad_norm': 2.2233493328094482, 'learning_rate': 1.6033546558008088e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3392, 'grad_norm': 2.30322265625, 'learning_rate': 1.6027615412674265e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3524, 'grad_norm': 1.4617351293563843, 'learning_rate': 1.6021684267340446e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3597, 'grad_norm': 1.5029029846191406, 'learning_rate': 1.6015753122006624e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3636, 'grad_norm': 1.73627769947052, 'learning_rate': 1.6009821976672805e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3694, 'grad_norm': 2.4902520179748535, 'learning_rate': 1.6003890831338986e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3474, 'grad_norm': 1.5916916131973267, 'learning_rate': 1.5997959686005167e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3241, 'grad_norm': 1.5825453996658325, 'learning_rate': 1.5992028540671347e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3713, 'grad_norm': 1.7370809316635132, 'learning_rate': 1.598609739533753e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3661, 'grad_norm': 1.5763866901397705, 'learning_rate': 1.598016625000371e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3647, 'grad_norm': 1.908412218093872, 'learning_rate': 1.597423510466989e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3708, 'grad_norm': 2.3839645385742188, 'learning_rate': 1.5968303959336068e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3634, 'grad_norm': 2.2917683124542236, 'learning_rate': 1.596237281400225e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3695, 'grad_norm': 1.5769844055175781, 'learning_rate': 1.595644166866843e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3496, 'grad_norm': 1.723410725593567, 'learning_rate': 1.595051052333461e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3348, 'grad_norm': 1.8749364614486694, 'learning_rate': 1.594457937800079e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3601, 'grad_norm': 1.5259391069412231, 'learning_rate': 1.5938648232666972e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3636, 'grad_norm': 2.570220470428467, 'learning_rate': 1.5932717087333153e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3593, 'grad_norm': 1.9342612028121948, 'learning_rate': 1.592684525345267e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3673, 'grad_norm': 2.005850315093994, 'learning_rate': 1.592097341957219e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3399, 'grad_norm': 1.6206135749816895, 'learning_rate': 1.591504227423837e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3515, 'grad_norm': 2.3024728298187256, 'learning_rate': 1.5909111128904547e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3616, 'grad_norm': 2.4586055278778076, 'learning_rate': 1.5903179983570728e-05, 'epoch': 0.61}\n",
      "{'loss': 0.3749, 'grad_norm': 1.9013283252716064, 'learning_rate': 1.589724883823691e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3591, 'grad_norm': 1.4496647119522095, 'learning_rate': 1.589131769290309e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3477, 'grad_norm': 2.0381860733032227, 'learning_rate': 1.588538654756927e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3682, 'grad_norm': 1.602567434310913, 'learning_rate': 1.587945540223545e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3786, 'grad_norm': 2.034367799758911, 'learning_rate': 1.5873524256901632e-05, 'epoch': 0.62}\n",
      "{'loss': 0.371, 'grad_norm': 1.4464157819747925, 'learning_rate': 1.5867593111567813e-05, 'epoch': 0.62}\n",
      "{'loss': 0.358, 'grad_norm': 2.5693745613098145, 'learning_rate': 1.586166196623399e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3574, 'grad_norm': 1.9342353343963623, 'learning_rate': 1.585573082090017e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3803, 'grad_norm': 2.153334379196167, 'learning_rate': 1.584979967556635e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3498, 'grad_norm': 2.0810067653656006, 'learning_rate': 1.584386853023253e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3526, 'grad_norm': 1.4861934185028076, 'learning_rate': 1.583793738489871e-05, 'epoch': 0.62}\n",
      "{'loss': 0.3559, 'grad_norm': 2.058744430541992, 'learning_rate': 1.5832006239564892e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3618, 'grad_norm': 1.3608942031860352, 'learning_rate': 1.5826075094231073e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3619, 'grad_norm': 1.7903715372085571, 'learning_rate': 1.5820143948897254e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3512, 'grad_norm': 1.808228611946106, 'learning_rate': 1.5814212803563435e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3645, 'grad_norm': 2.561519145965576, 'learning_rate': 1.5808281658229615e-05, 'epoch': 0.63}\n",
      "{'loss': 0.351, 'grad_norm': 1.6599799394607544, 'learning_rate': 1.5802350512895793e-05, 'epoch': 0.63}\n",
      "{'loss': 0.345, 'grad_norm': 1.674666166305542, 'learning_rate': 1.5796419367561974e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3462, 'grad_norm': 1.8669261932373047, 'learning_rate': 1.5790488222228155e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3457, 'grad_norm': 1.9711508750915527, 'learning_rate': 1.5784557076894336e-05, 'epoch': 0.63}\n",
      "{'loss': 0.348, 'grad_norm': 2.0369608402252197, 'learning_rate': 1.5778625931560517e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3364, 'grad_norm': 1.3269633054733276, 'learning_rate': 1.5772694786226697e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3411, 'grad_norm': 1.774549126625061, 'learning_rate': 1.5766763640892875e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3577, 'grad_norm': 2.4761788845062256, 'learning_rate': 1.5760832495559056e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3516, 'grad_norm': 2.1064352989196777, 'learning_rate': 1.5754901350225237e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3746, 'grad_norm': 1.3510985374450684, 'learning_rate': 1.5748970204891418e-05, 'epoch': 0.64}\n",
      "{'loss': 0.367, 'grad_norm': 2.6998350620269775, 'learning_rate': 1.5743039059557595e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3643, 'grad_norm': 2.2759668827056885, 'learning_rate': 1.5737107914223776e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3774, 'grad_norm': 1.7194688320159912, 'learning_rate': 1.5731176768889957e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3466, 'grad_norm': 2.06087064743042, 'learning_rate': 1.5725245623556138e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3454, 'grad_norm': 1.764512062072754, 'learning_rate': 1.571931447822232e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3432, 'grad_norm': 1.4449291229248047, 'learning_rate': 1.57133833328885e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3355, 'grad_norm': 2.6323697566986084, 'learning_rate': 1.570745218755468e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3363, 'grad_norm': 1.9052116870880127, 'learning_rate': 1.5701521042220858e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3399, 'grad_norm': 1.9422221183776855, 'learning_rate': 1.569558989688704e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3765, 'grad_norm': 2.253844976425171, 'learning_rate': 1.568965875155322e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3489, 'grad_norm': 2.2107555866241455, 'learning_rate': 1.56837276062194e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3578, 'grad_norm': 1.9716107845306396, 'learning_rate': 1.5677796460885582e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.342, 'grad_norm': 2.083807945251465, 'learning_rate': 1.5671865315551763e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3699, 'grad_norm': 1.6334643363952637, 'learning_rate': 1.566593417021794e-05, 'epoch': 0.65}\n",
      "{'loss': 0.36, 'grad_norm': 2.050875425338745, 'learning_rate': 1.566000302488412e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3297, 'grad_norm': 1.505798101425171, 'learning_rate': 1.5654071879550302e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3384, 'grad_norm': 2.950136423110962, 'learning_rate': 1.5648140734216483e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3569, 'grad_norm': 2.1349496841430664, 'learning_rate': 1.564220958888266e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3485, 'grad_norm': 2.5790579319000244, 'learning_rate': 1.563627844354884e-05, 'epoch': 0.65}\n",
      "{'loss': 0.3504, 'grad_norm': 2.0920703411102295, 'learning_rate': 1.563040660966836e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3736, 'grad_norm': 2.055309772491455, 'learning_rate': 1.562447546433454e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3537, 'grad_norm': 2.0544002056121826, 'learning_rate': 1.5618544319000723e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3498, 'grad_norm': 1.3809335231781006, 'learning_rate': 1.5612613173666903e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3558, 'grad_norm': 1.8017276525497437, 'learning_rate': 1.560674133978642e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3508, 'grad_norm': 1.856546401977539, 'learning_rate': 1.56008101944526e-05, 'epoch': 0.66}\n",
      "{'loss': 0.352, 'grad_norm': 1.740978479385376, 'learning_rate': 1.559487904911878e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3496, 'grad_norm': 1.5899072885513306, 'learning_rate': 1.5588947903784962e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3481, 'grad_norm': 2.228058338165283, 'learning_rate': 1.5583016758451143e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3477, 'grad_norm': 1.897340178489685, 'learning_rate': 1.557708561311732e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3171, 'grad_norm': 2.1341753005981445, 'learning_rate': 1.55711544677835e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3486, 'grad_norm': 2.5920112133026123, 'learning_rate': 1.5565223322449682e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3585, 'grad_norm': 1.554774522781372, 'learning_rate': 1.5559292177115863e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3218, 'grad_norm': 2.008470058441162, 'learning_rate': 1.5553361031782044e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3763, 'grad_norm': 2.0647785663604736, 'learning_rate': 1.5547429886448225e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3654, 'grad_norm': 1.7110494375228882, 'learning_rate': 1.5541498741114406e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3552, 'grad_norm': 2.2168960571289062, 'learning_rate': 1.5535567595780583e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3486, 'grad_norm': 1.597428560256958, 'learning_rate': 1.5529636450446764e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3587, 'grad_norm': 2.132585048675537, 'learning_rate': 1.5523705305112945e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3445, 'grad_norm': 1.7650315761566162, 'learning_rate': 1.5517774159779126e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3607, 'grad_norm': 1.994950771331787, 'learning_rate': 1.5511843014445304e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3583, 'grad_norm': 2.230631113052368, 'learning_rate': 1.5505911869111485e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3722, 'grad_norm': 1.8976242542266846, 'learning_rate': 1.5499980723777665e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.358, 'grad_norm': 2.234049081802368, 'learning_rate': 1.5494049578443846e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3489, 'grad_norm': 1.233158826828003, 'learning_rate': 1.5488118433110027e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3363, 'grad_norm': 1.5557630062103271, 'learning_rate': 1.5482187287776208e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3666, 'grad_norm': 2.204155206680298, 'learning_rate': 1.5476256142442386e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3598, 'grad_norm': 2.1272923946380615, 'learning_rate': 1.5470324997108567e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3421, 'grad_norm': 1.9412739276885986, 'learning_rate': 1.5464393851774748e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3621, 'grad_norm': 1.9826289415359497, 'learning_rate': 1.545846270644093e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3665, 'grad_norm': 1.5946378707885742, 'learning_rate': 1.545253156110711e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3578, 'grad_norm': 1.7217895984649658, 'learning_rate': 1.544660041577329e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3515, 'grad_norm': 1.990248441696167, 'learning_rate': 1.544066927043947e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3528, 'grad_norm': 3.0098135471343994, 'learning_rate': 1.5434738125105652e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3491, 'grad_norm': 1.1708580255508423, 'learning_rate': 1.542880697977183e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3626, 'grad_norm': 2.238115072250366, 'learning_rate': 1.542287583443801e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3544, 'grad_norm': 1.948023796081543, 'learning_rate': 1.5416944689104188e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3401, 'grad_norm': 2.120476007461548, 'learning_rate': 1.541101354377037e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3567, 'grad_norm': 1.8030482530593872, 'learning_rate': 1.540508239843655e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3549, 'grad_norm': 1.6876271963119507, 'learning_rate': 1.539915125310273e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3362, 'grad_norm': 1.4791712760925293, 'learning_rate': 1.539322010776891e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3644, 'grad_norm': 2.1561458110809326, 'learning_rate': 1.5387288962435092e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3589, 'grad_norm': 1.460478663444519, 'learning_rate': 1.5381357817101273e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3384, 'grad_norm': 2.7229225635528564, 'learning_rate': 1.5375426671767454e-05, 'epoch': 0.69}\n",
      "{'loss': 0.35, 'grad_norm': 2.017484426498413, 'learning_rate': 1.5369495526433632e-05, 'epoch': 0.69}\n",
      "{'loss': 0.358, 'grad_norm': 2.359815835952759, 'learning_rate': 1.5363564381099813e-05, 'epoch': 0.7}\n",
      "{'loss': 0.332, 'grad_norm': 1.4508371353149414, 'learning_rate': 1.5357633235765994e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3698, 'grad_norm': 2.1049563884735107, 'learning_rate': 1.5351761401885513e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3456, 'grad_norm': 2.08941388130188, 'learning_rate': 1.5345830256551694e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3339, 'grad_norm': 1.6522088050842285, 'learning_rate': 1.533989911121787e-05, 'epoch': 0.7}\n",
      "{'loss': 0.352, 'grad_norm': 2.0864813327789307, 'learning_rate': 1.5333967965884052e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3332, 'grad_norm': 2.248274087905884, 'learning_rate': 1.5328036820550233e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3591, 'grad_norm': 2.4254226684570312, 'learning_rate': 1.5322105675216414e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3374, 'grad_norm': 2.3143820762634277, 'learning_rate': 1.5316174529882595e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3364, 'grad_norm': 1.9886208772659302, 'learning_rate': 1.5310243384548776e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3483, 'grad_norm': 2.0012705326080322, 'learning_rate': 1.5304312239214957e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3305, 'grad_norm': 1.4955097436904907, 'learning_rate': 1.5298381093881138e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3363, 'grad_norm': 2.4774038791656494, 'learning_rate': 1.5292449948547315e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3672, 'grad_norm': 1.9916993379592896, 'learning_rate': 1.5286518803213496e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3643, 'grad_norm': 2.2609336376190186, 'learning_rate': 1.5280587657879674e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3485, 'grad_norm': 1.4591602087020874, 'learning_rate': 1.5274715823999196e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3224, 'grad_norm': 1.6195417642593384, 'learning_rate': 1.5268784678665377e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3441, 'grad_norm': 1.6980534791946411, 'learning_rate': 1.5262853533331555e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.347, 'grad_norm': 2.545186996459961, 'learning_rate': 1.5256922387997734e-05, 'epoch': 0.71}\n",
      "{'loss': 0.337, 'grad_norm': 1.648086428642273, 'learning_rate': 1.5250991242663915e-05, 'epoch': 0.71}\n",
      "{'loss': 0.344, 'grad_norm': 1.7556350231170654, 'learning_rate': 1.5245060097330096e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3625, 'grad_norm': 1.6308951377868652, 'learning_rate': 1.5239128951996277e-05, 'epoch': 0.71}\n",
      "{'loss': 0.338, 'grad_norm': 1.8287514448165894, 'learning_rate': 1.5233197806662456e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3525, 'grad_norm': 2.032339334487915, 'learning_rate': 1.5227266661328637e-05, 'epoch': 0.72}\n",
      "{'loss': 0.3415, 'grad_norm': 2.1945886611938477, 'learning_rate': 1.5221335515994818e-05, 'epoch': 0.72}\n",
      "{'loss': 0.3331, 'grad_norm': 1.7449347972869873, 'learning_rate': 1.5215404370660997e-05, 'epoch': 0.72}\n",
      "{'loss': 0.363, 'grad_norm': 1.697418212890625, 'learning_rate': 1.5209473225327178e-05, 'epoch': 0.72}\n",
      "{'loss': 0.3365, 'grad_norm': 1.7814373970031738, 'learning_rate': 1.5203542079993359e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3289, 'grad_norm': 2.8560142517089844, 'learning_rate': 1.519761093465954e-05, 'epoch': 0.72}\n",
      "{'loss': 0.306, 'grad_norm': 1.9431025981903076, 'learning_rate': 1.5191679789325719e-05, 'epoch': 0.72}\n",
      "{'loss': 0.348, 'grad_norm': 1.7066410779953003, 'learning_rate': 1.51857486439919e-05, 'epoch': 0.72}\n",
      "{'loss': 0.3687, 'grad_norm': 2.2068982124328613, 'learning_rate': 1.517981749865808e-05, 'epoch': 0.72}\n",
      "{'loss': 0.3409, 'grad_norm': 1.6843502521514893, 'learning_rate': 1.5173886353324262e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3584, 'grad_norm': 1.959341049194336, 'learning_rate': 1.516795520799044e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3575, 'grad_norm': 1.9872844219207764, 'learning_rate': 1.516202406265662e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3498, 'grad_norm': 3.4557406902313232, 'learning_rate': 1.51560929173228e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3699, 'grad_norm': 1.9416232109069824, 'learning_rate': 1.515016177198898e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3534, 'grad_norm': 1.991627812385559, 'learning_rate': 1.5144230626655161e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3438, 'grad_norm': 2.0068540573120117, 'learning_rate': 1.5138299481321342e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3469, 'grad_norm': 1.8104561567306519, 'learning_rate': 1.5132368335987521e-05, 'epoch': 0.73}\n",
      "{'loss': 0.346, 'grad_norm': 2.705209493637085, 'learning_rate': 1.5126437190653702e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3282, 'grad_norm': 2.330453872680664, 'learning_rate': 1.5120506045319883e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3391, 'grad_norm': 1.836767554283142, 'learning_rate': 1.5114574899986064e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3503, 'grad_norm': 1.5269275903701782, 'learning_rate': 1.5108643754652243e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3367, 'grad_norm': 2.040302038192749, 'learning_rate': 1.5102712609318424e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3563, 'grad_norm': 1.5437090396881104, 'learning_rate': 1.5096781463984605e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3177, 'grad_norm': 1.7519304752349854, 'learning_rate': 1.5090850318650784e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3411, 'grad_norm': 2.284918785095215, 'learning_rate': 1.5084919173316965e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3401, 'grad_norm': 2.6957943439483643, 'learning_rate': 1.5078988027983146e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3408, 'grad_norm': 1.9123759269714355, 'learning_rate': 1.5073056882649327e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3367, 'grad_norm': 1.6278793811798096, 'learning_rate': 1.5067125737315506e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3355, 'grad_norm': 2.2456955909729004, 'learning_rate': 1.5061194591981685e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3576, 'grad_norm': 1.4654438495635986, 'learning_rate': 1.5055263446647866e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3469, 'grad_norm': 1.464094638824463, 'learning_rate': 1.5049332301314045e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3659, 'grad_norm': 2.5234615802764893, 'learning_rate': 1.5043401155980226e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3465, 'grad_norm': 1.3908286094665527, 'learning_rate': 1.5037470010646407e-05, 'epoch': 0.74}\n",
      "{'loss': 0.3333, 'grad_norm': 1.0624250173568726, 'learning_rate': 1.5031538865312586e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3425, 'grad_norm': 2.930856704711914, 'learning_rate': 1.5025667031432106e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3557, 'grad_norm': 1.8010379076004028, 'learning_rate': 1.5019735886098285e-05, 'epoch': 0.75}\n",
      "{'loss': 0.33, 'grad_norm': 2.0125722885131836, 'learning_rate': 1.5013804740764466e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3344, 'grad_norm': 1.4333983659744263, 'learning_rate': 1.5007873595430647e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3186, 'grad_norm': 2.061952829360962, 'learning_rate': 1.5001942450096828e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3488, 'grad_norm': 1.8299003839492798, 'learning_rate': 1.4996011304763007e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3583, 'grad_norm': 1.6070705652236938, 'learning_rate': 1.4990080159429188e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3203, 'grad_norm': 1.334832787513733, 'learning_rate': 1.4984149014095369e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3541, 'grad_norm': 1.9332342147827148, 'learning_rate': 1.4978217868761548e-05, 'epoch': 0.75}\n",
      "{'loss': 0.3297, 'grad_norm': 2.0723135471343994, 'learning_rate': 1.4972286723427729e-05, 'epoch': 0.75}\n",
      "{'loss': 0.328, 'grad_norm': 1.9213240146636963, 'learning_rate': 1.496635557809391e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3528, 'grad_norm': 1.6871254444122314, 'learning_rate': 1.496042443276009e-05, 'epoch': 0.76}\n",
      "{'loss': 0.348, 'grad_norm': 1.741520643234253, 'learning_rate': 1.495449328742627e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3387, 'grad_norm': 1.8477730751037598, 'learning_rate': 1.494856214209245e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3339, 'grad_norm': 1.8330905437469482, 'learning_rate': 1.4942630996758632e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3347, 'grad_norm': 1.8958569765090942, 'learning_rate': 1.4936699851424809e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.343, 'grad_norm': 2.1155893802642822, 'learning_rate': 1.493076870609099e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3691, 'grad_norm': 1.2089014053344727, 'learning_rate': 1.4924837560757171e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3394, 'grad_norm': 1.9069740772247314, 'learning_rate': 1.491890641542335e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3507, 'grad_norm': 1.476556658744812, 'learning_rate': 1.4912975270089531e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3519, 'grad_norm': 1.7841929197311401, 'learning_rate': 1.4907044124755712e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.319, 'grad_norm': 2.0771267414093018, 'learning_rate': 1.4901112979421893e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3379, 'grad_norm': 2.3848531246185303, 'learning_rate': 1.4895181834088072e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3364, 'grad_norm': 1.711635947227478, 'learning_rate': 1.4889250688754253e-05, 'epoch': 0.77}\n",
      "{'loss': 0.318, 'grad_norm': 1.4749003648757935, 'learning_rate': 1.4883319543420434e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3219, 'grad_norm': 2.06486177444458, 'learning_rate': 1.4877388398086615e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.333, 'grad_norm': 2.6913583278656006, 'learning_rate': 1.4871457252752794e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3344, 'grad_norm': 1.9200547933578491, 'learning_rate': 1.4865526107418975e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3294, 'grad_norm': 2.201974391937256, 'learning_rate': 1.4859594962085156e-05, 'epoch': 0.77}\n",
      "{'loss': 0.321, 'grad_norm': 1.6954319477081299, 'learning_rate': 1.4853723128204673e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3208, 'grad_norm': 2.4123127460479736, 'learning_rate': 1.4847791982870854e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3424, 'grad_norm': 1.4856934547424316, 'learning_rate': 1.4841860837537034e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3323, 'grad_norm': 1.4022371768951416, 'learning_rate': 1.4835929692203214e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3547, 'grad_norm': 2.0493364334106445, 'learning_rate': 1.4829998546869395e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3448, 'grad_norm': 2.073878288269043, 'learning_rate': 1.4824067401535576e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3335, 'grad_norm': 1.9063293933868408, 'learning_rate': 1.4818136256201755e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3265, 'grad_norm': 1.790976881980896, 'learning_rate': 1.4812205110867935e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3375, 'grad_norm': 1.5545434951782227, 'learning_rate': 1.4806273965534116e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3394, 'grad_norm': 2.4571104049682617, 'learning_rate': 1.4800402131653635e-05, 'epoch': 0.78}\n",
      "{'loss': 0.32, 'grad_norm': 7.269729137420654, 'learning_rate': 1.4794470986319816e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3331, 'grad_norm': 1.7451140880584717, 'learning_rate': 1.4788539840985993e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.353, 'grad_norm': 1.841494083404541, 'learning_rate': 1.4782608695652174e-05, 'epoch': 0.78}\n",
      "{'loss': 0.3211, 'grad_norm': 1.9828671216964722, 'learning_rate': 1.4776677550318355e-05, 'epoch': 0.78}\n",
      "{'loss': 0.358, 'grad_norm': 2.153998374938965, 'learning_rate': 1.4770746404984534e-05, 'epoch': 0.78}\n",
      "{'loss': 0.339, 'grad_norm': 1.3611514568328857, 'learning_rate': 1.4764815259650715e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3351, 'grad_norm': 1.5846995115280151, 'learning_rate': 1.4758884114316896e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3268, 'grad_norm': 2.100780725479126, 'learning_rate': 1.4752952968983077e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3443, 'grad_norm': 1.825653314590454, 'learning_rate': 1.4747021823649256e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3262, 'grad_norm': 1.7106446027755737, 'learning_rate': 1.4741090678315437e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3172, 'grad_norm': 2.1613237857818604, 'learning_rate': 1.4735159532981618e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3242, 'grad_norm': 2.4922778606414795, 'learning_rate': 1.4729228387647797e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3251, 'grad_norm': 1.9941904544830322, 'learning_rate': 1.4723297242313978e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3147, 'grad_norm': 1.7913432121276855, 'learning_rate': 1.471736609698016e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3211, 'grad_norm': 1.8257508277893066, 'learning_rate': 1.471143495164634e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3544, 'grad_norm': 2.194927215576172, 'learning_rate': 1.470550380631252e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3347, 'grad_norm': 2.0214920043945312, 'learning_rate': 1.46995726609787e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3448, 'grad_norm': 2.0487444400787354, 'learning_rate': 1.4693641515644881e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3496, 'grad_norm': 2.3506946563720703, 'learning_rate': 1.4687710370311059e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3489, 'grad_norm': 1.6219241619110107, 'learning_rate': 1.468177922497724e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3219, 'grad_norm': 1.6143790483474731, 'learning_rate': 1.467584807964342e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3342, 'grad_norm': 1.5790292024612427, 'learning_rate': 1.46699169343096e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3577, 'grad_norm': 2.5423619747161865, 'learning_rate': 1.466398578897578e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3316, 'grad_norm': 1.5999476909637451, 'learning_rate': 1.4658054643641961e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3509, 'grad_norm': 2.4371626377105713, 'learning_rate': 1.4652182809761479e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3451, 'grad_norm': 2.2760331630706787, 'learning_rate': 1.464625166442766e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3544, 'grad_norm': 1.6008095741271973, 'learning_rate': 1.4640320519093841e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.334, 'grad_norm': 1.8875092267990112, 'learning_rate': 1.463438937376002e-05, 'epoch': 0.81}\n",
      "{'loss': 0.339, 'grad_norm': 2.1328792572021484, 'learning_rate': 1.4628458228426201e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3488, 'grad_norm': 3.044229507446289, 'learning_rate': 1.4622527083092382e-05, 'epoch': 0.81}\n",
      "{'loss': 0.338, 'grad_norm': 2.0585076808929443, 'learning_rate': 1.4616595937758563e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3483, 'grad_norm': 1.684507966041565, 'learning_rate': 1.4610664792424742e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.35, 'grad_norm': 1.588983178138733, 'learning_rate': 1.4604733647090923e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3212, 'grad_norm': 1.6942377090454102, 'learning_rate': 1.4598802501757104e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3365, 'grad_norm': 1.7705988883972168, 'learning_rate': 1.4592871356423283e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3308, 'grad_norm': 2.167301654815674, 'learning_rate': 1.4586940211089464e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3496, 'grad_norm': 2.119778633117676, 'learning_rate': 1.4581009065755645e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3161, 'grad_norm': 1.7662925720214844, 'learning_rate': 1.4575077920421826e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3201, 'grad_norm': 1.7526801824569702, 'learning_rate': 1.4569146775088005e-05, 'epoch': 0.81}\n",
      "{'loss': 0.3488, 'grad_norm': 1.839202880859375, 'learning_rate': 1.4563215629754184e-05, 'epoch': 0.82}\n",
      "{'loss': 0.331, 'grad_norm': 2.31497859954834, 'learning_rate': 1.4557284484420365e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3375, 'grad_norm': 1.523452639579773, 'learning_rate': 1.4551353339086544e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3246, 'grad_norm': 1.9074550867080688, 'learning_rate': 1.4545422193752725e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3376, 'grad_norm': 2.3806955814361572, 'learning_rate': 1.4539491048418906e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3431, 'grad_norm': 1.8524726629257202, 'learning_rate': 1.4533559903085085e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3496, 'grad_norm': 2.226811170578003, 'learning_rate': 1.4527628757751266e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3462, 'grad_norm': 1.8493191003799438, 'learning_rate': 1.4521697612417447e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3186, 'grad_norm': 2.0841097831726074, 'learning_rate': 1.4515766467083628e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3406, 'grad_norm': 1.8757083415985107, 'learning_rate': 1.4509835321749807e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3278, 'grad_norm': 2.2920279502868652, 'learning_rate': 1.4503904176415988e-05, 'epoch': 0.82}\n",
      "{'loss': 0.3379, 'grad_norm': 1.7523304224014282, 'learning_rate': 1.4497973031082169e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3361, 'grad_norm': 2.011385679244995, 'learning_rate': 1.449204188574835e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3206, 'grad_norm': 1.9326167106628418, 'learning_rate': 1.4486110740414529e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3396, 'grad_norm': 2.0018134117126465, 'learning_rate': 1.448017959508071e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3369, 'grad_norm': 2.376472234725952, 'learning_rate': 1.4474248449746891e-05, 'epoch': 0.83}\n",
      "{'loss': 0.329, 'grad_norm': 1.7318227291107178, 'learning_rate': 1.446831730441307e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3458, 'grad_norm': 1.3624236583709717, 'learning_rate': 1.446238615907925e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3462, 'grad_norm': 1.6960443258285522, 'learning_rate': 1.445645501374543e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3252, 'grad_norm': 1.3844133615493774, 'learning_rate': 1.445052386841161e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3417, 'grad_norm': 1.363602876663208, 'learning_rate': 1.444459272307779e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3636, 'grad_norm': 1.67449951171875, 'learning_rate': 1.4438661577743971e-05, 'epoch': 0.83}\n",
      "{'loss': 0.3144, 'grad_norm': 1.2807790040969849, 'learning_rate': 1.4432730432410152e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3574, 'grad_norm': 2.033961057662964, 'learning_rate': 1.4426799287076331e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3121, 'grad_norm': 1.345816731452942, 'learning_rate': 1.4420868141742512e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3209, 'grad_norm': 1.4326239824295044, 'learning_rate': 1.4414936996408693e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3334, 'grad_norm': 2.3731377124786377, 'learning_rate': 1.4409005851074872e-05, 'epoch': 0.84}\n",
      "{'loss': 0.361, 'grad_norm': 2.0584163665771484, 'learning_rate': 1.4403074705741053e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3562, 'grad_norm': 2.309464454650879, 'learning_rate': 1.4397143560407234e-05, 'epoch': 0.84}\n",
      "{'loss': 0.333, 'grad_norm': 1.8589447736740112, 'learning_rate': 1.4391212415073415e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3285, 'grad_norm': 1.5174843072891235, 'learning_rate': 1.4385281269739594e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3307, 'grad_norm': 2.065835475921631, 'learning_rate': 1.4379350124405775e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3301, 'grad_norm': 2.0026023387908936, 'learning_rate': 1.4373418979071956e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3319, 'grad_norm': 1.9089938402175903, 'learning_rate': 1.4367487833738137e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3189, 'grad_norm': 2.0857956409454346, 'learning_rate': 1.4361615999857655e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3418, 'grad_norm': 1.4451442956924438, 'learning_rate': 1.4355684854523834e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3368, 'grad_norm': 1.745518445968628, 'learning_rate': 1.4349753709190015e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3366, 'grad_norm': 2.250429391860962, 'learning_rate': 1.4343822563856196e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.355, 'grad_norm': 1.8815499544143677, 'learning_rate': 1.4337891418522373e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3288, 'grad_norm': 1.7138713598251343, 'learning_rate': 1.4331960273188554e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3361, 'grad_norm': 2.4169139862060547, 'learning_rate': 1.4326088439308075e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3657, 'grad_norm': 1.256125569343567, 'learning_rate': 1.4320157293974254e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3333, 'grad_norm': 1.6169325113296509, 'learning_rate': 1.4314226148640435e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3313, 'grad_norm': 1.4723858833312988, 'learning_rate': 1.4308295003306613e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3121, 'grad_norm': 4.132963180541992, 'learning_rate': 1.4302363857972794e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3441, 'grad_norm': 1.5377341508865356, 'learning_rate': 1.4296432712638975e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3435, 'grad_norm': 1.7029430866241455, 'learning_rate': 1.4290501567305156e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3468, 'grad_norm': 1.7466678619384766, 'learning_rate': 1.4284570421971335e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3643, 'grad_norm': 2.357485771179199, 'learning_rate': 1.4278639276637516e-05, 'epoch': 0.86}\n",
      "{'loss': 0.319, 'grad_norm': 2.4853224754333496, 'learning_rate': 1.4272708131303697e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3428, 'grad_norm': 1.346843957901001, 'learning_rate': 1.4266776985969877e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3649, 'grad_norm': 1.3334524631500244, 'learning_rate': 1.4260845840636057e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3429, 'grad_norm': 1.9811279773712158, 'learning_rate': 1.4254914695302238e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3109, 'grad_norm': 1.6730875968933105, 'learning_rate': 1.4248983549968418e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3207, 'grad_norm': 1.7340550422668457, 'learning_rate': 1.42430524046346e-05, 'epoch': 0.86}\n",
      "{'loss': 0.3418, 'grad_norm': 1.5509443283081055, 'learning_rate': 1.4237121259300779e-05, 'epoch': 0.86}\n",
      "{'loss': 0.347, 'grad_norm': 1.9403220415115356, 'learning_rate': 1.423119011396696e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3287, 'grad_norm': 1.9976359605789185, 'learning_rate': 1.422525896863314e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3503, 'grad_norm': 1.5472594499588013, 'learning_rate': 1.421932782329932e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3231, 'grad_norm': 2.0058696269989014, 'learning_rate': 1.4213396677965499e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3285, 'grad_norm': 1.7101202011108398, 'learning_rate': 1.420746553263168e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3386, 'grad_norm': 2.1852519512176514, 'learning_rate': 1.4201534387297859e-05, 'epoch': 0.87}\n",
      "{'loss': 0.32, 'grad_norm': 1.8199185132980347, 'learning_rate': 1.419560324196404e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3303, 'grad_norm': 1.979166030883789, 'learning_rate': 1.418967209663022e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3143, 'grad_norm': 1.89044988155365, 'learning_rate': 1.4183740951296402e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3388, 'grad_norm': 1.7511789798736572, 'learning_rate': 1.4177809805962581e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3373, 'grad_norm': 2.2166874408721924, 'learning_rate': 1.4171878660628762e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3478, 'grad_norm': 1.5652834177017212, 'learning_rate': 1.4165947515294943e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3184, 'grad_norm': 1.9543607234954834, 'learning_rate': 1.4160016369961122e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3082, 'grad_norm': 1.8170685768127441, 'learning_rate': 1.4154085224627303e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3393, 'grad_norm': 1.5958093404769897, 'learning_rate': 1.4148154079293484e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3457, 'grad_norm': 1.927412509918213, 'learning_rate': 1.4142222933959665e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3306, 'grad_norm': 2.328996181488037, 'learning_rate': 1.4136291788625844e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3129, 'grad_norm': 1.8618611097335815, 'learning_rate': 1.4130419954745363e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3164, 'grad_norm': 2.134925365447998, 'learning_rate': 1.4124488809411542e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3582, 'grad_norm': 1.652441382408142, 'learning_rate': 1.4118557664077723e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3225, 'grad_norm': 2.801138162612915, 'learning_rate': 1.4112626518743904e-05, 'epoch': 0.88}\n",
      "{'loss': 0.3185, 'grad_norm': 1.577609896659851, 'learning_rate': 1.4106695373410083e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.34, 'grad_norm': 1.9656685590744019, 'learning_rate': 1.4100764228076264e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3441, 'grad_norm': 1.6927874088287354, 'learning_rate': 1.4094833082742445e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3306, 'grad_norm': 1.4762486219406128, 'learning_rate': 1.4088901937408626e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3414, 'grad_norm': 2.1256892681121826, 'learning_rate': 1.4082970792074804e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3263, 'grad_norm': 1.9726670980453491, 'learning_rate': 1.4077039646740985e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3174, 'grad_norm': 1.7461531162261963, 'learning_rate': 1.4071108501407165e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3324, 'grad_norm': 2.0362327098846436, 'learning_rate': 1.4065177356073345e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3336, 'grad_norm': 1.4945436716079712, 'learning_rate': 1.4059305522192862e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3265, 'grad_norm': 1.451083779335022, 'learning_rate': 1.4053374376859043e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3136, 'grad_norm': 1.557611107826233, 'learning_rate': 1.4047443231525224e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3312, 'grad_norm': 1.6777602434158325, 'learning_rate': 1.4041512086191405e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3525, 'grad_norm': 1.93336021900177, 'learning_rate': 1.4035580940857584e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3239, 'grad_norm': 2.8369905948638916, 'learning_rate': 1.4029649795523765e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3438, 'grad_norm': 4.979805946350098, 'learning_rate': 1.4023718650189946e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3208, 'grad_norm': 1.7054377794265747, 'learning_rate': 1.4017787504856127e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3637, 'grad_norm': 1.4640562534332275, 'learning_rate': 1.4011856359522306e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3426, 'grad_norm': 2.72625732421875, 'learning_rate': 1.4005925214188487e-05, 'epoch': 0.9}\n",
      "{'loss': 0.328, 'grad_norm': 2.365290880203247, 'learning_rate': 1.3999994068854668e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3169, 'grad_norm': 1.652677059173584, 'learning_rate': 1.3994062923520849e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3311, 'grad_norm': 1.784619927406311, 'learning_rate': 1.3988131778187028e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3383, 'grad_norm': 1.6818183660507202, 'learning_rate': 1.3982200632853209e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3371, 'grad_norm': 2.2062923908233643, 'learning_rate': 1.397626948751939e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3574, 'grad_norm': 1.605724573135376, 'learning_rate': 1.3970338342185569e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3129, 'grad_norm': 1.8465558290481567, 'learning_rate': 1.396440719685175e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3051, 'grad_norm': 2.1249778270721436, 'learning_rate': 1.395847605151793e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3259, 'grad_norm': 2.2990925312042236, 'learning_rate': 1.3952544906184108e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3236, 'grad_norm': 1.4972522258758545, 'learning_rate': 1.394661376085029e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3243, 'grad_norm': 1.8975356817245483, 'learning_rate': 1.394068261551647e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3363, 'grad_norm': 1.8040717840194702, 'learning_rate': 1.3934751470182651e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3421, 'grad_norm': 2.0147039890289307, 'learning_rate': 1.392882032484883e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3213, 'grad_norm': 1.4071117639541626, 'learning_rate': 1.3922889179515011e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3342, 'grad_norm': 1.4782423973083496, 'learning_rate': 1.3916958034181192e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3354, 'grad_norm': 1.4563217163085938, 'learning_rate': 1.3911026888847371e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3319, 'grad_norm': 1.7905094623565674, 'learning_rate': 1.3905095743513552e-05, 'epoch': 0.91}\n",
      "{'loss': 0.3308, 'grad_norm': 1.6553853750228882, 'learning_rate': 1.3899164598179733e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3401, 'grad_norm': 1.4006145000457764, 'learning_rate': 1.3893233452845914e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3485, 'grad_norm': 2.412836790084839, 'learning_rate': 1.3887302307512093e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3288, 'grad_norm': 1.815682053565979, 'learning_rate': 1.3881371162178274e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3159, 'grad_norm': 2.348896026611328, 'learning_rate': 1.3875440016844455e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3428, 'grad_norm': 1.734052300453186, 'learning_rate': 1.3869508871510636e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3334, 'grad_norm': 1.3681849241256714, 'learning_rate': 1.3863577726176813e-05, 'epoch': 0.92}\n",
      "{'loss': 0.317, 'grad_norm': 1.835523009300232, 'learning_rate': 1.3857646580842994e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3102, 'grad_norm': 2.2250967025756836, 'learning_rate': 1.3851715435509174e-05, 'epoch': 0.92}\n",
      "{'loss': 0.3181, 'grad_norm': 2.4080922603607178, 'learning_rate': 1.3845784290175354e-05, 'epoch': 0.92}\n",
      "{'loss': 0.327, 'grad_norm': 2.2111942768096924, 'learning_rate': 1.3839853144841535e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3546, 'grad_norm': 2.112675666809082, 'learning_rate': 1.3833981310961053e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3453, 'grad_norm': 2.0414037704467773, 'learning_rate': 1.3828050165627234e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3333, 'grad_norm': 1.8939971923828125, 'learning_rate': 1.3822119020293415e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3545, 'grad_norm': 1.9051772356033325, 'learning_rate': 1.3816187874959594e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3203, 'grad_norm': 2.2848899364471436, 'learning_rate': 1.3810256729625775e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3191, 'grad_norm': 2.1287803649902344, 'learning_rate': 1.3804325584291956e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3182, 'grad_norm': 1.337101936340332, 'learning_rate': 1.3798394438958135e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3107, 'grad_norm': 1.9323899745941162, 'learning_rate': 1.3792463293624316e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3185, 'grad_norm': 2.451918840408325, 'learning_rate': 1.3786532148290497e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3172, 'grad_norm': 1.8417083024978638, 'learning_rate': 1.3780601002956678e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3369, 'grad_norm': 1.7102242708206177, 'learning_rate': 1.3774669857622857e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3438, 'grad_norm': 2.2098348140716553, 'learning_rate': 1.3768738712289038e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3229, 'grad_norm': 1.7191245555877686, 'learning_rate': 1.3762807566955219e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3217, 'grad_norm': 1.523969292640686, 'learning_rate': 1.37568764216214e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3328, 'grad_norm': 1.831519365310669, 'learning_rate': 1.3750945276287579e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3433, 'grad_norm': 1.4764373302459717, 'learning_rate': 1.374501413095376e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3541, 'grad_norm': 2.8280372619628906, 'learning_rate': 1.373908298561994e-05, 'epoch': 0.94}\n",
      "{'loss': 0.337, 'grad_norm': 2.019796848297119, 'learning_rate': 1.3733151840286118e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3271, 'grad_norm': 1.8712701797485352, 'learning_rate': 1.37272206949523e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3212, 'grad_norm': 1.994258165359497, 'learning_rate': 1.372128954961848e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3271, 'grad_norm': 1.5840034484863281, 'learning_rate': 1.371535840428466e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3175, 'grad_norm': 1.367444634437561, 'learning_rate': 1.370942725895084e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3205, 'grad_norm': 1.8272323608398438, 'learning_rate': 1.3703496113617021e-05, 'epoch': 0.94}\n",
      "{'loss': 0.3157, 'grad_norm': 1.8474135398864746, 'learning_rate': 1.3697564968283202e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3273, 'grad_norm': 1.1437346935272217, 'learning_rate': 1.3691633822949381e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3262, 'grad_norm': 1.8189797401428223, 'learning_rate': 1.3685702677615562e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3195, 'grad_norm': 1.7691198587417603, 'learning_rate': 1.3679771532281743e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3365, 'grad_norm': 1.5199371576309204, 'learning_rate': 1.3673840386947922e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3339, 'grad_norm': 1.9309288263320923, 'learning_rate': 1.3667909241614103e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3053, 'grad_norm': 1.5498197078704834, 'learning_rate': 1.3661978096280284e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.308, 'grad_norm': 1.4411581754684448, 'learning_rate': 1.3656046950946465e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3436, 'grad_norm': 2.0188941955566406, 'learning_rate': 1.3650115805612644e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3037, 'grad_norm': 2.055527687072754, 'learning_rate': 1.3644184660278825e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3239, 'grad_norm': 2.7443017959594727, 'learning_rate': 1.3638253514945004e-05, 'epoch': 0.95}\n",
      "{'loss': 0.3263, 'grad_norm': 1.864717960357666, 'learning_rate': 1.3632322369611183e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3282, 'grad_norm': 1.7298372983932495, 'learning_rate': 1.3626391224277364e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3445, 'grad_norm': 1.8036134243011475, 'learning_rate': 1.3620460078943545e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3143, 'grad_norm': 1.37547767162323, 'learning_rate': 1.3614528933609724e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3149, 'grad_norm': 1.9498374462127686, 'learning_rate': 1.3608597788275905e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3122, 'grad_norm': 1.2900454998016357, 'learning_rate': 1.3602666642942086e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3175, 'grad_norm': 1.6555438041687012, 'learning_rate': 1.3596735497608267e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3141, 'grad_norm': 1.724222183227539, 'learning_rate': 1.3590804352274446e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3342, 'grad_norm': 1.5180131196975708, 'learning_rate': 1.3584873206940627e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3505, 'grad_norm': 1.333846092224121, 'learning_rate': 1.3578942061606808e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3315, 'grad_norm': 2.160740375518799, 'learning_rate': 1.3573010916272989e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3238, 'grad_norm': 1.8138415813446045, 'learning_rate': 1.3567079770939168e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3135, 'grad_norm': 1.8090894222259521, 'learning_rate': 1.356114862560535e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3132, 'grad_norm': 1.334107518196106, 'learning_rate': 1.355521748027153e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3086, 'grad_norm': 1.7085918188095093, 'learning_rate': 1.3549286334937711e-05, 'epoch': 0.97}\n",
      "{'loss': 0.316, 'grad_norm': 1.6777398586273193, 'learning_rate': 1.354335518960389e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.341, 'grad_norm': 7.902106285095215, 'learning_rate': 1.353742404427007e-05, 'epoch': 0.97}\n",
      "{'loss': 0.298, 'grad_norm': 1.6353057622909546, 'learning_rate': 1.3531492898936249e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3132, 'grad_norm': 2.2591140270233154, 'learning_rate': 1.352556175360243e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3423, 'grad_norm': 1.5575103759765625, 'learning_rate': 1.351963060826861e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3168, 'grad_norm': 2.0415103435516357, 'learning_rate': 1.3513699462934791e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3219, 'grad_norm': 2.116485357284546, 'learning_rate': 1.350776831760097e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3376, 'grad_norm': 1.5244805812835693, 'learning_rate': 1.3501837172267151e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3459, 'grad_norm': 1.9811413288116455, 'learning_rate': 1.3495906026933332e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3257, 'grad_norm': 2.0385329723358154, 'learning_rate': 1.3489974881599513e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3135, 'grad_norm': 1.6121447086334229, 'learning_rate': 1.3484043736265692e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3219, 'grad_norm': 1.6776225566864014, 'learning_rate': 1.3478112590931873e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3262, 'grad_norm': 3.004035711288452, 'learning_rate': 1.3472181445598054e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3136, 'grad_norm': 1.635622501373291, 'learning_rate': 1.3466250300264233e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3307, 'grad_norm': 1.5360161066055298, 'learning_rate': 1.3460319154930414e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3033, 'grad_norm': 1.6232292652130127, 'learning_rate': 1.3454388009596595e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3575, 'grad_norm': 2.0539956092834473, 'learning_rate': 1.3448456864262776e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3243, 'grad_norm': 1.9249348640441895, 'learning_rate': 1.3442525718928955e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3082, 'grad_norm': 2.6808059215545654, 'learning_rate': 1.3436594573595135e-05, 'epoch': 0.98}\n",
      "{'loss': 0.3168, 'grad_norm': 1.8502732515335083, 'learning_rate': 1.3430663428261314e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3093, 'grad_norm': 1.8061827421188354, 'learning_rate': 1.3424732282927495e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3246, 'grad_norm': 1.6735472679138184, 'learning_rate': 1.3418801137593676e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3129, 'grad_norm': 2.0764107704162598, 'learning_rate': 1.3412869992259857e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3365, 'grad_norm': 1.4265940189361572, 'learning_rate': 1.3406938846926036e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3288, 'grad_norm': 2.0296223163604736, 'learning_rate': 1.3401007701592217e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3342, 'grad_norm': 1.421405553817749, 'learning_rate': 1.3395076556258398e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.333, 'grad_norm': 1.6019964218139648, 'learning_rate': 1.3389145410924578e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3258, 'grad_norm': 1.648276925086975, 'learning_rate': 1.3383214265590758e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3171, 'grad_norm': 2.3689303398132324, 'learning_rate': 1.3377283120256939e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3358, 'grad_norm': 1.7423583269119263, 'learning_rate': 1.337135197492312e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3392, 'grad_norm': 1.7138949632644653, 'learning_rate': 1.33654208295893e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.318, 'grad_norm': 1.8524022102355957, 'learning_rate': 1.335948968425548e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3161, 'grad_norm': 2.215914011001587, 'learning_rate': 1.335355853892166e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3361, 'grad_norm': 1.9805488586425781, 'learning_rate': 1.3347627393587841e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3418, 'grad_norm': 1.9597703218460083, 'learning_rate': 1.3341755559707359e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3324, 'grad_norm': 2.0659339427948, 'learning_rate': 1.3335883725826877e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07697d98354a4035ab5225f03ae4c383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29251235723495483, 'eval_bleu': 39.876794053624174, 'eval_runtime': 123423.7976, 'eval_samples_per_second': 0.911, 'eval_steps_per_second': 0.114, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3191, 'grad_norm': 1.8297916650772095, 'learning_rate': 1.3329952580493058e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3299, 'grad_norm': 1.0202221870422363, 'learning_rate': 1.3324021435159239e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3201, 'grad_norm': 1.852668046951294, 'learning_rate': 1.3318090289825418e-05, 'epoch': 1.0}\n",
      "{'loss': 0.2893, 'grad_norm': 1.75568687915802, 'learning_rate': 1.3312159144491599e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3046, 'grad_norm': 1.6716464757919312, 'learning_rate': 1.330622799915778e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3179, 'grad_norm': 1.738011360168457, 'learning_rate': 1.3300296853823959e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3146, 'grad_norm': 1.445665955543518, 'learning_rate': 1.329436570849014e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3169, 'grad_norm': 1.8272666931152344, 'learning_rate': 1.3288434563156319e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3161, 'grad_norm': 2.1388907432556152, 'learning_rate': 1.3282503417822498e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3143, 'grad_norm': 1.8864635229110718, 'learning_rate': 1.3276572272488679e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3274, 'grad_norm': 1.6117258071899414, 'learning_rate': 1.327064112715486e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3122, 'grad_norm': 1.9439232349395752, 'learning_rate': 1.326470998182104e-05, 'epoch': 1.01}\n",
      "{'loss': 0.2923, 'grad_norm': 2.2267277240753174, 'learning_rate': 1.325877883648722e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3033, 'grad_norm': 1.0306588411331177, 'learning_rate': 1.3252847691153401e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3367, 'grad_norm': 1.8733519315719604, 'learning_rate': 1.3246916545819582e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3104, 'grad_norm': 1.5411818027496338, 'learning_rate': 1.3240985400485761e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3085, 'grad_norm': 1.6659541130065918, 'learning_rate': 1.3235054255151942e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3305, 'grad_norm': 2.3295438289642334, 'learning_rate': 1.3229123109818123e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3055, 'grad_norm': 2.2930028438568115, 'learning_rate': 1.3223191964484304e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3047, 'grad_norm': 1.930359125137329, 'learning_rate': 1.3217260819150483e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3038, 'grad_norm': 2.0349838733673096, 'learning_rate': 1.3211329673816664e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3493, 'grad_norm': 2.0448718070983887, 'learning_rate': 1.3205398528482845e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3054, 'grad_norm': 1.2396160364151, 'learning_rate': 1.3199467383149026e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3156, 'grad_norm': 1.7801406383514404, 'learning_rate': 1.3193536237815205e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3373, 'grad_norm': 2.2719228267669678, 'learning_rate': 1.3187605092481384e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3141, 'grad_norm': 1.6971880197525024, 'learning_rate': 1.3181673947147563e-05, 'epoch': 1.02}\n",
      "{'loss': 0.2935, 'grad_norm': 2.6086666584014893, 'learning_rate': 1.3175742801813744e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3392, 'grad_norm': 2.2543420791625977, 'learning_rate': 1.3169811656479925e-05, 'epoch': 1.02}\n",
      "{'loss': 0.3261, 'grad_norm': 1.8760334253311157, 'learning_rate': 1.3163880511146106e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3098, 'grad_norm': 1.6757363080978394, 'learning_rate': 1.3157949365812285e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.292, 'grad_norm': 1.7014670372009277, 'learning_rate': 1.3152018220478466e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2997, 'grad_norm': 1.7175767421722412, 'learning_rate': 1.3146087075144647e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3086, 'grad_norm': 2.419776678085327, 'learning_rate': 1.3140155929810828e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3193, 'grad_norm': 1.6066412925720215, 'learning_rate': 1.3134224784477007e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2906, 'grad_norm': 1.3701289892196655, 'learning_rate': 1.3128293639143188e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3072, 'grad_norm': 1.1373789310455322, 'learning_rate': 1.3122362493809369e-05, 'epoch': 1.03}\n",
      "{'loss': 0.2951, 'grad_norm': 1.787158727645874, 'learning_rate': 1.3116490659928887e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3225, 'grad_norm': 1.7493493556976318, 'learning_rate': 1.3110559514595068e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3158, 'grad_norm': 1.603039026260376, 'learning_rate': 1.3104628369261247e-05, 'epoch': 1.03}\n",
      "{'loss': 0.3381, 'grad_norm': 1.7735488414764404, 'learning_rate': 1.3098697223927428e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3113, 'grad_norm': 2.1295058727264404, 'learning_rate': 1.3092766078593609e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3116, 'grad_norm': 1.8338273763656616, 'learning_rate': 1.308683493325979e-05, 'epoch': 1.04}\n",
      "{'loss': 0.2966, 'grad_norm': 1.3906550407409668, 'learning_rate': 1.3080903787925969e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3456, 'grad_norm': 2.089536666870117, 'learning_rate': 1.307497264259215e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3146, 'grad_norm': 1.3023583889007568, 'learning_rate': 1.306904149725833e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3087, 'grad_norm': 1.668761134147644, 'learning_rate': 1.3063110351924508e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3229, 'grad_norm': 1.1842371225357056, 'learning_rate': 1.3057179206590689e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3074, 'grad_norm': 2.297480821609497, 'learning_rate': 1.305124806125687e-05, 'epoch': 1.04}\n",
      "{'loss': 0.3184, 'grad_norm': 2.431183338165283, 'learning_rate': 1.3045316915923049e-05, 'epoch': 1.04}\n",
      "{'loss': 0.333, 'grad_norm': 2.361436367034912, 'learning_rate': 1.303938577058923e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2933, 'grad_norm': 1.7721813917160034, 'learning_rate': 1.303345462525541e-05, 'epoch': 1.05}\n",
      "{'loss': 0.306, 'grad_norm': 1.817062497138977, 'learning_rate': 1.3027523479921592e-05, 'epoch': 1.05}\n",
      "{'loss': 0.312, 'grad_norm': 1.7655168771743774, 'learning_rate': 1.3021592334587771e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3235, 'grad_norm': 1.8425313234329224, 'learning_rate': 1.3015661189253952e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3094, 'grad_norm': 1.8907084465026855, 'learning_rate': 1.3009730043920133e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2997, 'grad_norm': 2.337935447692871, 'learning_rate': 1.3003798898586314e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3103, 'grad_norm': 2.3246796131134033, 'learning_rate': 1.2997867753252493e-05, 'epoch': 1.05}\n",
      "{'loss': 0.318, 'grad_norm': 2.0457146167755127, 'learning_rate': 1.2991936607918674e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3198, 'grad_norm': 1.5548676252365112, 'learning_rate': 1.2986005462584855e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3176, 'grad_norm': 1.996869683265686, 'learning_rate': 1.2980074317251034e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3206, 'grad_norm': 2.059760570526123, 'learning_rate': 1.2974143171917215e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3243, 'grad_norm': 1.8135813474655151, 'learning_rate': 1.2968212026583396e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3147, 'grad_norm': 2.0135834217071533, 'learning_rate': 1.2962280881249573e-05, 'epoch': 1.06}\n",
      "{'loss': 0.318, 'grad_norm': 1.6854180097579956, 'learning_rate': 1.2956409047369094e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3186, 'grad_norm': 1.3396954536437988, 'learning_rate': 1.2950477902035275e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3072, 'grad_norm': 2.314105272293091, 'learning_rate': 1.2944546756701454e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3133, 'grad_norm': 1.2621926069259644, 'learning_rate': 1.2938674922820974e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3286, 'grad_norm': 1.2188938856124878, 'learning_rate': 1.2932803088940491e-05, 'epoch': 1.06}\n",
      "{'loss': 0.315, 'grad_norm': 1.666048526763916, 'learning_rate': 1.292687194360667e-05, 'epoch': 1.06}\n",
      "{'loss': 0.2959, 'grad_norm': 1.2596184015274048, 'learning_rate': 1.2920940798272852e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3103, 'grad_norm': 1.3596057891845703, 'learning_rate': 1.2915009652939032e-05, 'epoch': 1.06}\n",
      "{'loss': 0.3045, 'grad_norm': 1.8105313777923584, 'learning_rate': 1.2909078507605213e-05, 'epoch': 1.06}\n",
      "{'loss': 0.331, 'grad_norm': 1.6876070499420166, 'learning_rate': 1.2903147362271393e-05, 'epoch': 1.06}\n",
      "{'loss': 0.2953, 'grad_norm': 2.312885046005249, 'learning_rate': 1.2897216216937573e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3139, 'grad_norm': 2.143123149871826, 'learning_rate': 1.2891285071603754e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3249, 'grad_norm': 2.1332473754882812, 'learning_rate': 1.2885353926269932e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3009, 'grad_norm': 1.8009952306747437, 'learning_rate': 1.2879422780936113e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3152, 'grad_norm': 1.6075586080551147, 'learning_rate': 1.2873491635602294e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2976, 'grad_norm': 1.7721573114395142, 'learning_rate': 1.2867560490268473e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2962, 'grad_norm': 2.5362415313720703, 'learning_rate': 1.2861629344934654e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2739, 'grad_norm': 1.9693816900253296, 'learning_rate': 1.2855698199600835e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2903, 'grad_norm': 1.7917248010635376, 'learning_rate': 1.2849767054267016e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2916, 'grad_norm': 1.3879495859146118, 'learning_rate': 1.2843835908933195e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2961, 'grad_norm': 2.0995752811431885, 'learning_rate': 1.2837904763599376e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3157, 'grad_norm': 2.124943971633911, 'learning_rate': 1.2831973618265557e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3145, 'grad_norm': 2.284487009048462, 'learning_rate': 1.2826042472931738e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3154, 'grad_norm': 1.5843371152877808, 'learning_rate': 1.2820111327597917e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3206, 'grad_norm': 1.5752910375595093, 'learning_rate': 1.2814180182264098e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2963, 'grad_norm': 1.7063326835632324, 'learning_rate': 1.2808249036930279e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2956, 'grad_norm': 1.183434247970581, 'learning_rate': 1.2802317891596458e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.321, 'grad_norm': 1.867453932762146, 'learning_rate': 1.2796386746262639e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3165, 'grad_norm': 1.8972193002700806, 'learning_rate': 1.279045560092882e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3225, 'grad_norm': 1.3666071891784668, 'learning_rate': 1.2784524455594997e-05, 'epoch': 1.08}\n",
      "{'loss': 0.3136, 'grad_norm': 1.585676908493042, 'learning_rate': 1.2778593310261178e-05, 'epoch': 1.08}\n",
      "{'loss': 0.2993, 'grad_norm': 1.752936840057373, 'learning_rate': 1.2772662164927359e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3174, 'grad_norm': 2.688701629638672, 'learning_rate': 1.276673101959354e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3032, 'grad_norm': 1.7444041967391968, 'learning_rate': 1.2760799874259719e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3094, 'grad_norm': 1.8674495220184326, 'learning_rate': 1.27548687289259e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3056, 'grad_norm': 1.4939395189285278, 'learning_rate': 1.274893758359208e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3241, 'grad_norm': 2.2964658737182617, 'learning_rate': 1.274300643825826e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3173, 'grad_norm': 1.597281575202942, 'learning_rate': 1.2737075292924441e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3122, 'grad_norm': 1.9238756895065308, 'learning_rate': 1.2731144147590622e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2945, 'grad_norm': 1.569544792175293, 'learning_rate': 1.2725213002256803e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2909, 'grad_norm': 1.6774698495864868, 'learning_rate': 1.2719281856922982e-05, 'epoch': 1.09}\n",
      "{'loss': 0.3164, 'grad_norm': 2.1413049697875977, 'learning_rate': 1.2713350711589163e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3193, 'grad_norm': 1.982613205909729, 'learning_rate': 1.2707419566255344e-05, 'epoch': 1.09}\n",
      "{'loss': 0.322, 'grad_norm': 1.7176810503005981, 'learning_rate': 1.2701488420921525e-05, 'epoch': 1.1}\n",
      "{'loss': 0.319, 'grad_norm': 1.8826959133148193, 'learning_rate': 1.2695557275587704e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2998, 'grad_norm': 1.7533814907073975, 'learning_rate': 1.2689626130253885e-05, 'epoch': 1.1}\n",
      "{'loss': 0.3008, 'grad_norm': 2.5060718059539795, 'learning_rate': 1.2683694984920062e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3013, 'grad_norm': 1.3157421350479126, 'learning_rate': 1.2677763839586243e-05, 'epoch': 1.1}\n",
      "{'loss': 0.314, 'grad_norm': 1.7153793573379517, 'learning_rate': 1.2671832694252424e-05, 'epoch': 1.1}\n",
      "{'loss': 0.2981, 'grad_norm': 1.3432224988937378, 'learning_rate': 1.2665901548918605e-05, 'epoch': 1.1}\n",
      "{'loss': 0.305, 'grad_norm': 1.488553524017334, 'learning_rate': 1.2659970403584784e-05, 'epoch': 1.1}\n",
      "{'loss': 0.3023, 'grad_norm': 1.8769654035568237, 'learning_rate': 1.2654039258250965e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3007, 'grad_norm': 1.938642978668213, 'learning_rate': 1.2648108112917146e-05, 'epoch': 1.1}\n",
      "{'loss': 0.29, 'grad_norm': 2.4792535305023193, 'learning_rate': 1.2642176967583327e-05, 'epoch': 1.1}\n",
      "{'loss': 0.3073, 'grad_norm': 1.785590648651123, 'learning_rate': 1.2636245822249506e-05, 'epoch': 1.1}\n",
      "{'loss': 0.3348, 'grad_norm': 1.6384693384170532, 'learning_rate': 1.2630314676915687e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3088, 'grad_norm': 1.6366044282913208, 'learning_rate': 1.2624383531581868e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3013, 'grad_norm': 1.772375226020813, 'learning_rate': 1.2618452386248047e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3238, 'grad_norm': 1.8620549440383911, 'learning_rate': 1.2612521240914228e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3081, 'grad_norm': 1.9497400522232056, 'learning_rate': 1.2606590095580409e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2943, 'grad_norm': 1.9534028768539429, 'learning_rate': 1.260065895024659e-05, 'epoch': 1.11}\n",
      "{'loss': 0.302, 'grad_norm': 1.5345485210418701, 'learning_rate': 1.2594727804912769e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3189, 'grad_norm': 1.3258934020996094, 'learning_rate': 1.2588796659578948e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3084, 'grad_norm': 1.0746738910675049, 'learning_rate': 1.2582865514245129e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3031, 'grad_norm': 1.8634064197540283, 'learning_rate': 1.2576934368911308e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3198, 'grad_norm': 1.990985631942749, 'learning_rate': 1.257100322357749e-05, 'epoch': 1.11}\n",
      "{'loss': 0.3032, 'grad_norm': 1.4923326969146729, 'learning_rate': 1.256507207824367e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3216, 'grad_norm': 1.7945722341537476, 'learning_rate': 1.255914093290985e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3179, 'grad_norm': 2.0480024814605713, 'learning_rate': 1.255320978757603e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3196, 'grad_norm': 1.589587688446045, 'learning_rate': 1.2547278642242211e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3259, 'grad_norm': 1.7125087976455688, 'learning_rate': 1.2541347496908392e-05, 'epoch': 1.12}\n",
      "{'loss': 0.2982, 'grad_norm': 2.0339467525482178, 'learning_rate': 1.2535416351574571e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3194, 'grad_norm': 1.8664708137512207, 'learning_rate': 1.2529485206240752e-05, 'epoch': 1.12}\n",
      "{'loss': 0.2939, 'grad_norm': 1.6941787004470825, 'learning_rate': 1.2523554060906933e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3102, 'grad_norm': 1.4772528409957886, 'learning_rate': 1.2517622915573114e-05, 'epoch': 1.12}\n",
      "{'loss': 0.3293, 'grad_norm': 2.0016136169433594, 'learning_rate': 1.2511691770239293e-05, 'epoch': 1.12}\n",
      "{'loss': 0.2967, 'grad_norm': 1.8283129930496216, 'learning_rate': 1.2505760624905474e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3421, 'grad_norm': 2.327883720397949, 'learning_rate': 1.2499829479571655e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3196, 'grad_norm': 2.155024290084839, 'learning_rate': 1.2493957645691173e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3317, 'grad_norm': 1.5499860048294067, 'learning_rate': 1.2488026500357354e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3207, 'grad_norm': 2.2235288619995117, 'learning_rate': 1.2482095355023533e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3301, 'grad_norm': 1.2940359115600586, 'learning_rate': 1.2476164209689714e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3072, 'grad_norm': 2.0452024936676025, 'learning_rate': 1.2470233064355895e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2998, 'grad_norm': 1.5507749319076538, 'learning_rate': 1.2464301919022075e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3183, 'grad_norm': 1.83125901222229, 'learning_rate': 1.2458370773688253e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2903, 'grad_norm': 1.5428179502487183, 'learning_rate': 1.2452439628354434e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3149, 'grad_norm': 1.5446228981018066, 'learning_rate': 1.2446508483020615e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3134, 'grad_norm': 1.6424014568328857, 'learning_rate': 1.2440577337686794e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3133, 'grad_norm': 1.2372524738311768, 'learning_rate': 1.2434646192352975e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3043, 'grad_norm': 1.5584489107131958, 'learning_rate': 1.2428715047019156e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2917, 'grad_norm': 2.284257411956787, 'learning_rate': 1.2422783901685335e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3095, 'grad_norm': 2.6034491062164307, 'learning_rate': 1.2416852756351516e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3271, 'grad_norm': 1.883455753326416, 'learning_rate': 1.2410921611017697e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3183, 'grad_norm': 2.1365649700164795, 'learning_rate': 1.2404990465683878e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3037, 'grad_norm': 1.4054615497589111, 'learning_rate': 1.2399059320350057e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3128, 'grad_norm': 2.057020902633667, 'learning_rate': 1.2393128175016238e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3085, 'grad_norm': 1.3363151550292969, 'learning_rate': 1.2387197029682419e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3153, 'grad_norm': 1.6081736087799072, 'learning_rate': 1.23812658843486e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3044, 'grad_norm': 2.135194778442383, 'learning_rate': 1.2375334739014779e-05, 'epoch': 1.14}\n",
      "{'loss': 0.3116, 'grad_norm': 2.119990348815918, 'learning_rate': 1.236940359368096e-05, 'epoch': 1.15}\n",
      "{'loss': 0.304, 'grad_norm': 1.5088791847229004, 'learning_rate': 1.2363472448347137e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3249, 'grad_norm': 2.012251615524292, 'learning_rate': 1.2357541303013318e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3264, 'grad_norm': 1.7785929441452026, 'learning_rate': 1.2351610157679499e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3036, 'grad_norm': 1.7938728332519531, 'learning_rate': 1.234567901234568e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3022, 'grad_norm': 1.9176713228225708, 'learning_rate': 1.233974786701186e-05, 'epoch': 1.15}\n",
      "{'loss': 0.2921, 'grad_norm': 2.1937522888183594, 'learning_rate': 1.233381672167804e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3151, 'grad_norm': 1.4618463516235352, 'learning_rate': 1.2327885576344221e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3148, 'grad_norm': 1.3414509296417236, 'learning_rate': 1.2321954431010402e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3048, 'grad_norm': 1.628092646598816, 'learning_rate': 1.2316023285676581e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3227, 'grad_norm': 2.370432138442993, 'learning_rate': 1.2310092140342762e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3204, 'grad_norm': 1.3887429237365723, 'learning_rate': 1.2304160995008943e-05, 'epoch': 1.15}\n",
      "{'loss': 0.3292, 'grad_norm': 1.5329757928848267, 'learning_rate': 1.2298229849675122e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3242, 'grad_norm': 1.906692624092102, 'learning_rate': 1.2292298704341303e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3013, 'grad_norm': 1.8841103315353394, 'learning_rate': 1.2286367559007484e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3167, 'grad_norm': 1.54049813747406, 'learning_rate': 1.2280436413673665e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3401, 'grad_norm': 1.7503107786178589, 'learning_rate': 1.2274505268339844e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2985, 'grad_norm': 1.8515739440917969, 'learning_rate': 1.2268633434459363e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3258, 'grad_norm': 1.7273340225219727, 'learning_rate': 1.2262702289125543e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3069, 'grad_norm': 1.9984451532363892, 'learning_rate': 1.2256771143791724e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2975, 'grad_norm': 1.4687594175338745, 'learning_rate': 1.2250839998457904e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3, 'grad_norm': 2.226149797439575, 'learning_rate': 1.2244908853124085e-05, 'epoch': 1.16}\n",
      "{'loss': 0.3109, 'grad_norm': 2.0063364505767822, 'learning_rate': 1.2238977707790263e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2975, 'grad_norm': 2.216716766357422, 'learning_rate': 1.2233046562456444e-05, 'epoch': 1.17}\n",
      "{'loss': 0.3012, 'grad_norm': 1.519568681716919, 'learning_rate': 1.2227115417122623e-05, 'epoch': 1.17}\n",
      "{'loss': 0.3217, 'grad_norm': 1.634383201599121, 'learning_rate': 1.2221184271788804e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2917, 'grad_norm': 2.3148064613342285, 'learning_rate': 1.2215253126454985e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2993, 'grad_norm': 1.531678318977356, 'learning_rate': 1.2209321981121166e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2907, 'grad_norm': 1.5014735460281372, 'learning_rate': 1.2203390835787345e-05, 'epoch': 1.17}\n",
      "{'loss': 0.298, 'grad_norm': 1.9219850301742554, 'learning_rate': 1.2197459690453526e-05, 'epoch': 1.17}\n",
      "{'loss': 0.308, 'grad_norm': 1.7615395784378052, 'learning_rate': 1.2191528545119707e-05, 'epoch': 1.17}\n",
      "{'loss': 0.3532, 'grad_norm': 1.5563678741455078, 'learning_rate': 1.2185597399785886e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2924, 'grad_norm': 2.0481655597686768, 'learning_rate': 1.2179666254452067e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2979, 'grad_norm': 1.7898117303848267, 'learning_rate': 1.2173735109118248e-05, 'epoch': 1.17}\n",
      "{'loss': 0.3191, 'grad_norm': 1.493219256401062, 'learning_rate': 1.2167803963784429e-05, 'epoch': 1.18}\n",
      "{'loss': 0.301, 'grad_norm': 1.5677413940429688, 'learning_rate': 1.2161872818450608e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3223, 'grad_norm': 2.0431864261627197, 'learning_rate': 1.2155941673116789e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3239, 'grad_norm': 2.85972261428833, 'learning_rate': 1.215001052778297e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2976, 'grad_norm': 1.9812474250793457, 'learning_rate': 1.214407938244915e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3142, 'grad_norm': 2.600057363510132, 'learning_rate': 1.2138148237115328e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3063, 'grad_norm': 1.8226169347763062, 'learning_rate': 1.2132217091781509e-05, 'epoch': 1.18}\n",
      "{'loss': 0.308, 'grad_norm': 2.632873773574829, 'learning_rate': 1.2126285946447688e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3132, 'grad_norm': 1.6621356010437012, 'learning_rate': 1.2120354801113869e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3035, 'grad_norm': 1.7221509218215942, 'learning_rate': 1.211442365578005e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3031, 'grad_norm': 2.22768235206604, 'learning_rate': 1.2108492510446231e-05, 'epoch': 1.18}\n",
      "{'loss': 0.3014, 'grad_norm': 1.53789222240448, 'learning_rate': 1.210256136511241e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2912, 'grad_norm': 1.919627070426941, 'learning_rate': 1.209668953123193e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3416, 'grad_norm': 2.1244349479675293, 'learning_rate': 1.2090758385898109e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3053, 'grad_norm': 1.4156185388565063, 'learning_rate': 1.208482724056429e-05, 'epoch': 1.19}\n",
      "{'loss': 0.285, 'grad_norm': 1.9779714345932007, 'learning_rate': 1.207889609523047e-05, 'epoch': 1.19}\n",
      "{'loss': 0.315, 'grad_norm': 1.9958194494247437, 'learning_rate': 1.2072964949896651e-05, 'epoch': 1.19}\n",
      "{'loss': 0.32, 'grad_norm': 1.755549430847168, 'learning_rate': 1.2067093116016169e-05, 'epoch': 1.19}\n",
      "{'loss': 0.307, 'grad_norm': 1.6212166547775269, 'learning_rate': 1.2061161970682348e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3254, 'grad_norm': 1.8279824256896973, 'learning_rate': 1.205523082534853e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3278, 'grad_norm': 1.4786711931228638, 'learning_rate': 1.204929968001471e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2993, 'grad_norm': 1.323286771774292, 'learning_rate': 1.2043368534680891e-05, 'epoch': 1.19}\n",
      "{'loss': 0.31, 'grad_norm': 1.3746416568756104, 'learning_rate': 1.203743738934707e-05, 'epoch': 1.19}\n",
      "{'loss': 0.3133, 'grad_norm': 1.851439356803894, 'learning_rate': 1.2031506244013251e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3141, 'grad_norm': 1.2952144145965576, 'learning_rate': 1.2025575098679432e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2913, 'grad_norm': 1.973264217376709, 'learning_rate': 1.2019643953345613e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2876, 'grad_norm': 1.298148274421692, 'learning_rate': 1.2013712808011792e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2829, 'grad_norm': 2.463000774383545, 'learning_rate': 1.2007781662677973e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3154, 'grad_norm': 1.840319275856018, 'learning_rate': 1.2001850517344154e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2962, 'grad_norm': 1.8422273397445679, 'learning_rate': 1.1995919372010333e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2968, 'grad_norm': 1.8159680366516113, 'learning_rate': 1.1989988226676514e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2985, 'grad_norm': 2.012209177017212, 'learning_rate': 1.1984057081342693e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3159, 'grad_norm': 2.0023224353790283, 'learning_rate': 1.1978125936008872e-05, 'epoch': 1.2}\n",
      "{'loss': 0.2982, 'grad_norm': 1.5599281787872314, 'learning_rate': 1.1972194790675053e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3231, 'grad_norm': 1.4914109706878662, 'learning_rate': 1.1966263645341234e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3025, 'grad_norm': 1.5354654788970947, 'learning_rate': 1.1960332500007415e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2914, 'grad_norm': 1.4136983156204224, 'learning_rate': 1.1954401354673594e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3051, 'grad_norm': 1.6390178203582764, 'learning_rate': 1.1948470209339775e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3164, 'grad_norm': 1.6296466588974, 'learning_rate': 1.1942539064005956e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2998, 'grad_norm': 2.3664841651916504, 'learning_rate': 1.1936607918672135e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3116, 'grad_norm': 1.8936301469802856, 'learning_rate': 1.1930676773338316e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3076, 'grad_norm': 1.6637767553329468, 'learning_rate': 1.1924745628004497e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3065, 'grad_norm': 1.3343701362609863, 'learning_rate': 1.1918814482670678e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3221, 'grad_norm': 1.6958916187286377, 'learning_rate': 1.1912883337336857e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3076, 'grad_norm': 1.2288886308670044, 'learning_rate': 1.1906952192003038e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2971, 'grad_norm': 1.7640575170516968, 'learning_rate': 1.1901021046669219e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3027, 'grad_norm': 1.9197245836257935, 'learning_rate': 1.18950899013354e-05, 'epoch': 1.22}\n",
      "{'loss': 0.2817, 'grad_norm': 1.580702781677246, 'learning_rate': 1.188915875600158e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3133, 'grad_norm': 2.1572625637054443, 'learning_rate': 1.1883227610667758e-05, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3028, 'grad_norm': 1.6729789972305298, 'learning_rate': 1.1877296465333938e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3029, 'grad_norm': 1.3678308725357056, 'learning_rate': 1.1871365320000119e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3, 'grad_norm': 0.9604301452636719, 'learning_rate': 1.18654341746663e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3118, 'grad_norm': 1.82767653465271, 'learning_rate': 1.185950302933248e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3167, 'grad_norm': 1.9825068712234497, 'learning_rate': 1.185357188399866e-05, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3138, 'grad_norm': 2.7148168087005615, 'learning_rate': 1.184764073866484e-05, 'epoch': 1.22}\n",
      "{'loss': 0.3058, 'grad_norm': 1.808942437171936, 'learning_rate': 1.1841709593331021e-05, 'epoch': 1.22}\n",
      "{'loss': 0.2923, 'grad_norm': 1.3878052234649658, 'learning_rate': 1.1835778447997202e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2942, 'grad_norm': 1.6071971654891968, 'learning_rate': 1.1829847302663381e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3188, 'grad_norm': 1.935194730758667, 'learning_rate': 1.1823916157329562e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3124, 'grad_norm': 1.9305667877197266, 'learning_rate': 1.1817985011995743e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3027, 'grad_norm': 1.1530264616012573, 'learning_rate': 1.1812053866661924e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3013, 'grad_norm': 1.5156309604644775, 'learning_rate': 1.1806122721328103e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3289, 'grad_norm': 1.436010479927063, 'learning_rate': 1.1800191575994284e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3113, 'grad_norm': 2.216869592666626, 'learning_rate': 1.1794260430660465e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3358, 'grad_norm': 2.6020102500915527, 'learning_rate': 1.1788329285326643e-05, 'epoch': 1.23}\n",
      "{'loss': 0.311, 'grad_norm': 1.6669572591781616, 'learning_rate': 1.1782398139992824e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3175, 'grad_norm': 2.290677547454834, 'learning_rate': 1.1776466994659004e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3119, 'grad_norm': 1.5449769496917725, 'learning_rate': 1.1770535849325184e-05, 'epoch': 1.23}\n",
      "{'loss': 0.3319, 'grad_norm': 1.9563676118850708, 'learning_rate': 1.1764604703991365e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3161, 'grad_norm': 2.1972575187683105, 'learning_rate': 1.1758673558657546e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3061, 'grad_norm': 1.4091429710388184, 'learning_rate': 1.1752742413323726e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3028, 'grad_norm': 1.4627553224563599, 'learning_rate': 1.1746811267989906e-05, 'epoch': 1.24}\n",
      "{'loss': 0.2885, 'grad_norm': 1.8480829000473022, 'learning_rate': 1.1740880122656087e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3218, 'grad_norm': 3.537492513656616, 'learning_rate': 1.1734948977322267e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3223, 'grad_norm': 2.058779716491699, 'learning_rate': 1.1729017831988447e-05, 'epoch': 1.24}\n",
      "{'loss': 0.2939, 'grad_norm': 1.8925087451934814, 'learning_rate': 1.1723086686654628e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3047, 'grad_norm': 6.026725769042969, 'learning_rate': 1.1717155541320808e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3001, 'grad_norm': 1.9666080474853516, 'learning_rate': 1.171122439598699e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3187, 'grad_norm': 1.6939464807510376, 'learning_rate': 1.1705352562106507e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3129, 'grad_norm': 1.3591344356536865, 'learning_rate': 1.1699421416772688e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3169, 'grad_norm': 1.791860580444336, 'learning_rate': 1.1693490271438867e-05, 'epoch': 1.25}\n",
      "{'loss': 0.296, 'grad_norm': 1.4067606925964355, 'learning_rate': 1.1687618437558385e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2806, 'grad_norm': 1.5148347616195679, 'learning_rate': 1.1681687292224566e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2991, 'grad_norm': 2.2135329246520996, 'learning_rate': 1.1675756146890747e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2981, 'grad_norm': 1.6762434244155884, 'learning_rate': 1.1669825001556928e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3057, 'grad_norm': 1.9450198411941528, 'learning_rate': 1.1663893856223107e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3137, 'grad_norm': 1.7129385471343994, 'learning_rate': 1.1657962710889288e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2969, 'grad_norm': 1.7429282665252686, 'learning_rate': 1.1652031565555469e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3061, 'grad_norm': 1.6867191791534424, 'learning_rate': 1.164610042022165e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3059, 'grad_norm': 1.4028043746948242, 'learning_rate': 1.1640169274887829e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3051, 'grad_norm': 1.8711998462677002, 'learning_rate': 1.1634238129554008e-05, 'epoch': 1.26}\n",
      "{'loss': 0.3137, 'grad_norm': 2.0241615772247314, 'learning_rate': 1.1628306984220187e-05, 'epoch': 1.26}\n",
      "{'loss': 0.2994, 'grad_norm': 1.966113567352295, 'learning_rate': 1.1622375838886368e-05, 'epoch': 1.26}\n",
      "{'loss': 0.3083, 'grad_norm': 1.3989537954330444, 'learning_rate': 1.1616444693552549e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3142, 'grad_norm': 1.5335092544555664, 'learning_rate': 1.161051354821873e-05, 'epoch': 1.26}\n",
      "{'loss': 0.3095, 'grad_norm': 2.1736841201782227, 'learning_rate': 1.1604582402884909e-05, 'epoch': 1.26}\n",
      "{'loss': 0.2923, 'grad_norm': 1.5918360948562622, 'learning_rate': 1.159865125755109e-05, 'epoch': 1.26}\n",
      "{'loss': 0.3243, 'grad_norm': 1.7288457155227661, 'learning_rate': 1.159272011221727e-05, 'epoch': 1.26}\n",
      "{'loss': 0.3322, 'grad_norm': 1.7850661277770996, 'learning_rate': 1.1586788966883452e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2822, 'grad_norm': 1.9035496711730957, 'learning_rate': 1.1580857821549631e-05, 'epoch': 1.26}\n",
      "{'loss': 0.3005, 'grad_norm': 1.8983575105667114, 'learning_rate': 1.1574926676215812e-05, 'epoch': 1.26}\n",
      "{'loss': 0.318, 'grad_norm': 1.6644082069396973, 'learning_rate': 1.1568995530881993e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3164, 'grad_norm': 1.8846790790557861, 'learning_rate': 1.1563064385548172e-05, 'epoch': 1.27}\n",
      "{'loss': 0.302, 'grad_norm': 1.3638650178909302, 'learning_rate': 1.1557133240214353e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2916, 'grad_norm': 1.8297255039215088, 'learning_rate': 1.1551202094880534e-05, 'epoch': 1.27}\n",
      "{'loss': 0.2978, 'grad_norm': 1.8197407722473145, 'learning_rate': 1.1545270949546715e-05, 'epoch': 1.27}\n",
      "{'loss': 0.299, 'grad_norm': 1.6660971641540527, 'learning_rate': 1.1539339804212894e-05, 'epoch': 1.27}\n",
      "{'loss': 0.2906, 'grad_norm': 1.7589550018310547, 'learning_rate': 1.1533408658879073e-05, 'epoch': 1.27}\n",
      "{'loss': 0.2912, 'grad_norm': 1.8669507503509521, 'learning_rate': 1.1527477513545254e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3058, 'grad_norm': 2.133517026901245, 'learning_rate': 1.1521546368211433e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3073, 'grad_norm': 1.5448250770568848, 'learning_rate': 1.1515615222877614e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3233, 'grad_norm': 2.2049472332000732, 'learning_rate': 1.1509684077543795e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3218, 'grad_norm': 1.6418416500091553, 'learning_rate': 1.1503752932209974e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3334, 'grad_norm': 1.6957646608352661, 'learning_rate': 1.1497821786876155e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2923, 'grad_norm': 1.9793965816497803, 'learning_rate': 1.1491890641542336e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3308, 'grad_norm': 1.221191644668579, 'learning_rate': 1.1485959496208517e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3114, 'grad_norm': 1.3846001625061035, 'learning_rate': 1.1480028350874696e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3181, 'grad_norm': 2.119717597961426, 'learning_rate': 1.1474097205540877e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2776, 'grad_norm': 1.4001104831695557, 'learning_rate': 1.1468166060207058e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2932, 'grad_norm': 1.7999132871627808, 'learning_rate': 1.1462234914873239e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2858, 'grad_norm': 1.9181851148605347, 'learning_rate': 1.1456303769539418e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2964, 'grad_norm': 1.226818561553955, 'learning_rate': 1.1450372624205599e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3031, 'grad_norm': 2.083757162094116, 'learning_rate': 1.144444147887178e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2904, 'grad_norm': 1.9353278875350952, 'learning_rate': 1.1438510333537957e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3106, 'grad_norm': 1.9283487796783447, 'learning_rate': 1.1432579188204138e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2821, 'grad_norm': 1.4842082262039185, 'learning_rate': 1.1426648042870319e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3176, 'grad_norm': 1.2439937591552734, 'learning_rate': 1.1420776208989839e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3193, 'grad_norm': 1.9931310415267944, 'learning_rate': 1.141484506365602e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3089, 'grad_norm': 1.2043441534042358, 'learning_rate': 1.1408913918322197e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2967, 'grad_norm': 1.565255880355835, 'learning_rate': 1.1402982772988378e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3064, 'grad_norm': 1.9323714971542358, 'learning_rate': 1.1397051627654559e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2934, 'grad_norm': 1.000363826751709, 'learning_rate': 1.139112048232074e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3166, 'grad_norm': 2.0913736820220947, 'learning_rate': 1.1385189336986919e-05, 'epoch': 1.29}\n",
      "{'loss': 0.3064, 'grad_norm': 1.6889725923538208, 'learning_rate': 1.13792581916531e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2808, 'grad_norm': 1.256611943244934, 'learning_rate': 1.137332704631928e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2988, 'grad_norm': 1.5529248714447021, 'learning_rate': 1.136739590098546e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3065, 'grad_norm': 1.8835102319717407, 'learning_rate': 1.136146475565164e-05, 'epoch': 1.3}\n",
      "{'loss': 0.2933, 'grad_norm': 1.6626522541046143, 'learning_rate': 1.1355533610317822e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3396, 'grad_norm': 1.377238392829895, 'learning_rate': 1.1349602464984003e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2991, 'grad_norm': 1.7534255981445312, 'learning_rate': 1.1343671319650182e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3017, 'grad_norm': 2.089646100997925, 'learning_rate': 1.1337740174316363e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3209, 'grad_norm': 1.450034499168396, 'learning_rate': 1.133186834043588e-05, 'epoch': 1.3}\n",
      "{'loss': 0.2979, 'grad_norm': 1.4428147077560425, 'learning_rate': 1.1325937195102061e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3122, 'grad_norm': 1.522089958190918, 'learning_rate': 1.1320006049768242e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2849, 'grad_norm': 2.0186493396759033, 'learning_rate': 1.1314074904434421e-05, 'epoch': 1.3}\n",
      "{'loss': 0.295, 'grad_norm': 1.1753008365631104, 'learning_rate': 1.1308143759100602e-05, 'epoch': 1.3}\n",
      "{'loss': 0.2939, 'grad_norm': 1.4565967321395874, 'learning_rate': 1.1302212613766783e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2816, 'grad_norm': 1.216050386428833, 'learning_rate': 1.1296281468432964e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3049, 'grad_norm': 2.1327273845672607, 'learning_rate': 1.1290350323099143e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3022, 'grad_norm': 1.5751848220825195, 'learning_rate': 1.1284419177765323e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3008, 'grad_norm': 1.5294138193130493, 'learning_rate': 1.1278488032431503e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3126, 'grad_norm': 2.214879274368286, 'learning_rate': 1.1272556887097683e-05, 'epoch': 1.31}\n",
      "{'loss': 0.305, 'grad_norm': 1.2203916311264038, 'learning_rate': 1.1266625741763864e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3155, 'grad_norm': 1.5860488414764404, 'learning_rate': 1.1260694596430044e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3208, 'grad_norm': 1.7375990152359009, 'learning_rate': 1.1254763451096224e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3134, 'grad_norm': 1.7517879009246826, 'learning_rate': 1.1248832305762405e-05, 'epoch': 1.31}\n",
      "{'loss': 0.2927, 'grad_norm': 2.1077587604522705, 'learning_rate': 1.1242901160428585e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3024, 'grad_norm': 1.778409719467163, 'learning_rate': 1.1236970015094766e-05, 'epoch': 1.31}\n",
      "{'loss': 0.3023, 'grad_norm': 1.5673750638961792, 'learning_rate': 1.1231038869760946e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3064, 'grad_norm': 2.5558409690856934, 'learning_rate': 1.1225107724427126e-05, 'epoch': 1.32}\n",
      "{'loss': 0.2893, 'grad_norm': 1.5047434568405151, 'learning_rate': 1.1219176579093307e-05, 'epoch': 1.32}\n",
      "{'loss': 0.3077, 'grad_norm': 1.7932343482971191, 'learning_rate': 1.1213245433759488e-05, 'epoch': 1.32}\n",
      "{'loss': 0.3013, 'grad_norm': 2.205697774887085, 'learning_rate': 1.1207314288425668e-05, 'epoch': 1.32}\n",
      "{'loss': 0.3273, 'grad_norm': 2.086639404296875, 'learning_rate': 1.1201383143091848e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2919, 'grad_norm': 1.7765446901321411, 'learning_rate': 1.119545199775803e-05, 'epoch': 1.32}\n",
      "{'loss': 0.3133, 'grad_norm': 1.3162121772766113, 'learning_rate': 1.118952085242421e-05, 'epoch': 1.32}\n",
      "{'loss': 0.3021, 'grad_norm': 1.8663315773010254, 'learning_rate': 1.1183589707090388e-05, 'epoch': 1.32}\n",
      "{'loss': 0.3088, 'grad_norm': 1.2669726610183716, 'learning_rate': 1.1177658561756569e-05, 'epoch': 1.32}\n",
      "{'loss': 0.2905, 'grad_norm': 1.63911771774292, 'learning_rate': 1.1171727416422748e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2811, 'grad_norm': 1.8716342449188232, 'learning_rate': 1.1165796271088929e-05, 'epoch': 1.33}\n",
      "{'loss': 0.2949, 'grad_norm': 1.4780550003051758, 'learning_rate': 1.115986512575511e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3147, 'grad_norm': 1.6408640146255493, 'learning_rate': 1.115393398042129e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3218, 'grad_norm': 1.926742434501648, 'learning_rate': 1.114800283508747e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3202, 'grad_norm': 1.792456865310669, 'learning_rate': 1.114207168975365e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3064, 'grad_norm': 1.1942987442016602, 'learning_rate': 1.1136140544419832e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3097, 'grad_norm': 1.5733968019485474, 'learning_rate': 1.1130209399086012e-05, 'epoch': 1.33}\n",
      "{'loss': 0.2949, 'grad_norm': 1.577974796295166, 'learning_rate': 1.1124278253752192e-05, 'epoch': 1.33}\n",
      "{'loss': 0.28, 'grad_norm': 1.7197651863098145, 'learning_rate': 1.1118347108418373e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3095, 'grad_norm': 1.9505571126937866, 'learning_rate': 1.1112415963084553e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2953, 'grad_norm': 1.7639020681381226, 'learning_rate': 1.1106484817750733e-05, 'epoch': 1.33}\n",
      "{'loss': 0.2831, 'grad_norm': 1.2677090167999268, 'learning_rate': 1.1100553672416914e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3267, 'grad_norm': 1.6932084560394287, 'learning_rate': 1.1094622527083094e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3114, 'grad_norm': 2.2113869190216064, 'learning_rate': 1.1088691381749272e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3007, 'grad_norm': 1.3966010808944702, 'learning_rate': 1.1082760236415453e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3108, 'grad_norm': 1.983778476715088, 'learning_rate': 1.1076888402534974e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2963, 'grad_norm': 2.3522329330444336, 'learning_rate': 1.1070957257201153e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3054, 'grad_norm': 1.7984375953674316, 'learning_rate': 1.1065026111867334e-05, 'epoch': 1.34}\n",
      "{'loss': 0.277, 'grad_norm': 1.5412570238113403, 'learning_rate': 1.1059154277986852e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2967, 'grad_norm': 1.6637674570083618, 'learning_rate': 1.1053223132653033e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3233, 'grad_norm': 1.9686663150787354, 'learning_rate': 1.1047291987319214e-05, 'epoch': 1.34}\n",
      "{'loss': 0.3027, 'grad_norm': 1.9590669870376587, 'learning_rate': 1.1041360841985393e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2926, 'grad_norm': 2.12233567237854, 'learning_rate': 1.1035429696651572e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3065, 'grad_norm': 1.5546228885650635, 'learning_rate': 1.1029498551317753e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3259, 'grad_norm': 1.622620701789856, 'learning_rate': 1.1023567405983932e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2996, 'grad_norm': 1.7505362033843994, 'learning_rate': 1.1017636260650113e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3016, 'grad_norm': 2.050239086151123, 'learning_rate': 1.1011705115316294e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3119, 'grad_norm': 1.5440595149993896, 'learning_rate': 1.1005773969982473e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3198, 'grad_norm': 1.5239274501800537, 'learning_rate': 1.0999842824648654e-05, 'epoch': 1.35}\n",
      "{'loss': 0.2982, 'grad_norm': 1.3014495372772217, 'learning_rate': 1.0993911679314835e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3031, 'grad_norm': 1.5672967433929443, 'learning_rate': 1.0987980533981016e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3198, 'grad_norm': 1.8861579895019531, 'learning_rate': 1.0982049388647195e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3032, 'grad_norm': 1.5221319198608398, 'learning_rate': 1.0976118243313376e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3022, 'grad_norm': 1.413724660873413, 'learning_rate': 1.0970187097979557e-05, 'epoch': 1.35}\n",
      "{'loss': 0.3184, 'grad_norm': 1.713493824005127, 'learning_rate': 1.0964255952645738e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3104, 'grad_norm': 1.054419994354248, 'learning_rate': 1.0958324807311917e-05, 'epoch': 1.36}\n",
      "{'loss': 0.2841, 'grad_norm': 1.8628876209259033, 'learning_rate': 1.0952393661978098e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3089, 'grad_norm': 1.5699621438980103, 'learning_rate': 1.0946462516644279e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3044, 'grad_norm': 2.448425769805908, 'learning_rate': 1.0940531371310458e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3009, 'grad_norm': 1.5030637979507446, 'learning_rate': 1.0934600225976637e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3153, 'grad_norm': 1.6161798238754272, 'learning_rate': 1.0928669080642818e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3035, 'grad_norm': 1.2967134714126587, 'learning_rate': 1.0922737935308997e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3042, 'grad_norm': 2.1548893451690674, 'learning_rate': 1.0916806789975178e-05, 'epoch': 1.36}\n",
      "{'loss': 0.268, 'grad_norm': 1.746369481086731, 'learning_rate': 1.09109349560947e-05, 'epoch': 1.36}\n",
      "{'loss': 0.2947, 'grad_norm': 2.1190805435180664, 'learning_rate': 1.0905003810760877e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3027, 'grad_norm': 2.1998300552368164, 'learning_rate': 1.0899072665427058e-05, 'epoch': 1.37}\n",
      "{'loss': 0.2877, 'grad_norm': 1.2329784631729126, 'learning_rate': 1.0893141520093237e-05, 'epoch': 1.37}\n",
      "{'loss': 0.3061, 'grad_norm': 1.4334020614624023, 'learning_rate': 1.0887210374759418e-05, 'epoch': 1.37}\n",
      "{'loss': 0.3112, 'grad_norm': 1.582072377204895, 'learning_rate': 1.0881279229425599e-05, 'epoch': 1.37}\n",
      "{'loss': 0.293, 'grad_norm': 1.744329810142517, 'learning_rate': 1.087534808409178e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3056, 'grad_norm': 1.825089693069458, 'learning_rate': 1.0869416938757959e-05, 'epoch': 1.37}\n",
      "{'loss': 0.3059, 'grad_norm': 1.9746971130371094, 'learning_rate': 1.086348579342414e-05, 'epoch': 1.37}\n",
      "{'loss': 0.2842, 'grad_norm': 1.756902813911438, 'learning_rate': 1.085755464809032e-05, 'epoch': 1.37}\n",
      "{'loss': 0.3052, 'grad_norm': 1.8448007106781006, 'learning_rate': 1.0851623502756502e-05, 'epoch': 1.37}\n",
      "{'loss': 0.3147, 'grad_norm': 1.454237937927246, 'learning_rate': 1.084569235742268e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2964, 'grad_norm': 1.5220507383346558, 'learning_rate': 1.0839761212088862e-05, 'epoch': 1.37}\n",
      "{'loss': 0.3041, 'grad_norm': 1.6851067543029785, 'learning_rate': 1.0833830066755043e-05, 'epoch': 1.38}\n",
      "{'loss': 0.2976, 'grad_norm': 1.3455016613006592, 'learning_rate': 1.0827898921421223e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3098, 'grad_norm': 1.5063228607177734, 'learning_rate': 1.0821967776087403e-05, 'epoch': 1.38}\n",
      "{'loss': 0.2966, 'grad_norm': 1.5062408447265625, 'learning_rate': 1.0816036630753584e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2979, 'grad_norm': 2.1218252182006836, 'learning_rate': 1.0810105485419761e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3039, 'grad_norm': 1.4486421346664429, 'learning_rate': 1.0804174340085942e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3002, 'grad_norm': 1.6038708686828613, 'learning_rate': 1.0798243194752123e-05, 'epoch': 1.38}\n",
      "{'loss': 0.2976, 'grad_norm': 1.3606623411178589, 'learning_rate': 1.0792312049418304e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3481, 'grad_norm': 1.8840059041976929, 'learning_rate': 1.0786380904084483e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3048, 'grad_norm': 2.3926658630371094, 'learning_rate': 1.0780449758750664e-05, 'epoch': 1.38}\n",
      "{'loss': 0.3007, 'grad_norm': 1.3222126960754395, 'learning_rate': 1.0774518613416845e-05, 'epoch': 1.38}\n",
      "{'loss': 0.303, 'grad_norm': 1.7214014530181885, 'learning_rate': 1.0768587468083026e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3142, 'grad_norm': nan, 'learning_rate': 1.0762715634202543e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2894, 'grad_norm': 1.6893409490585327, 'learning_rate': 1.0756784488868723e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2827, 'grad_norm': 1.4641005992889404, 'learning_rate': 1.0750853343534904e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3105, 'grad_norm': 2.4436850547790527, 'learning_rate': 1.0744922198201084e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3036, 'grad_norm': 2.4767136573791504, 'learning_rate': 1.0738991052867265e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2946, 'grad_norm': 2.017902374267578, 'learning_rate': 1.0733059907533445e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3127, 'grad_norm': 2.495004892349243, 'learning_rate': 1.0727128762199625e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2963, 'grad_norm': 1.5581756830215454, 'learning_rate': 1.0721197616865806e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2885, 'grad_norm': 2.1901628971099854, 'learning_rate': 1.0715266471531987e-05, 'epoch': 1.39}\n",
      "{'loss': 0.3138, 'grad_norm': 1.6117162704467773, 'learning_rate': 1.0709335326198166e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2973, 'grad_norm': 1.7520142793655396, 'learning_rate': 1.0703404180864347e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3084, 'grad_norm': 1.7621910572052002, 'learning_rate': 1.0697473035530528e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3062, 'grad_norm': 1.6646931171417236, 'learning_rate': 1.0691541890196707e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3087, 'grad_norm': 2.1813604831695557, 'learning_rate': 1.0685610744862887e-05, 'epoch': 1.4}\n",
      "{'loss': 0.291, 'grad_norm': 1.8476775884628296, 'learning_rate': 1.0679679599529068e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2881, 'grad_norm': 1.8920482397079468, 'learning_rate': 1.0673748454195247e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3025, 'grad_norm': 1.328169822692871, 'learning_rate': 1.0667817308861428e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2735, 'grad_norm': 1.747048020362854, 'learning_rate': 1.0661886163527609e-05, 'epoch': 1.4}\n",
      "{'loss': 0.289, 'grad_norm': 1.4081153869628906, 'learning_rate': 1.065595501819379e-05, 'epoch': 1.4}\n",
      "{'loss': 0.304, 'grad_norm': 1.1841427087783813, 'learning_rate': 1.0650023872859969e-05, 'epoch': 1.4}\n",
      "{'loss': 0.294, 'grad_norm': 1.5653905868530273, 'learning_rate': 1.064409272752615e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2962, 'grad_norm': 1.8933815956115723, 'learning_rate': 1.063816158219233e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2795, 'grad_norm': 1.907797932624817, 'learning_rate': 1.063223043685851e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2919, 'grad_norm': 1.5725709199905396, 'learning_rate': 1.062629929152469e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3056, 'grad_norm': 2.191922664642334, 'learning_rate': 1.0620368146190872e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3308, 'grad_norm': 1.3448532819747925, 'learning_rate': 1.0614437000857052e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2859, 'grad_norm': 2.5563228130340576, 'learning_rate': 1.0608505855523232e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3008, 'grad_norm': 1.6280877590179443, 'learning_rate': 1.0602574710189413e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2756, 'grad_norm': 2.0824544429779053, 'learning_rate': 1.0596643564855593e-05, 'epoch': 1.41}\n",
      "{'loss': 0.298, 'grad_norm': 2.5578055381774902, 'learning_rate': 1.0590712419521774e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3155, 'grad_norm': 1.713874101638794, 'learning_rate': 1.0584781274187952e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2964, 'grad_norm': 1.9689464569091797, 'learning_rate': 1.0578850128854133e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3161, 'grad_norm': 2.302400588989258, 'learning_rate': 1.0572918983520312e-05, 'epoch': 1.41}\n",
      "{'loss': 0.3019, 'grad_norm': 2.3845884799957275, 'learning_rate': 1.0566987838186493e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3022, 'grad_norm': 1.9941869974136353, 'learning_rate': 1.0561056692852674e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3016, 'grad_norm': 1.8054754734039307, 'learning_rate': 1.0555125547518855e-05, 'epoch': 1.42}\n",
      "{'loss': 0.2835, 'grad_norm': 1.917069673538208, 'learning_rate': 1.0549194402185034e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2872, 'grad_norm': 1.3575258255004883, 'learning_rate': 1.0543263256851215e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3001, 'grad_norm': 2.465630292892456, 'learning_rate': 1.0537332111517396e-05, 'epoch': 1.42}\n",
      "{'loss': 0.2922, 'grad_norm': 1.2304315567016602, 'learning_rate': 1.0531400966183577e-05, 'epoch': 1.42}\n",
      "{'loss': 0.2994, 'grad_norm': 1.6136510372161865, 'learning_rate': 1.0525469820849756e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3068, 'grad_norm': 2.2627363204956055, 'learning_rate': 1.0519538675515937e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2971, 'grad_norm': 1.9229635000228882, 'learning_rate': 1.0513607530182118e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3046, 'grad_norm': 1.6280698776245117, 'learning_rate': 1.0507735696301635e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3111, 'grad_norm': 1.2910213470458984, 'learning_rate': 1.0501804550967816e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2889, 'grad_norm': 1.0070884227752686, 'learning_rate': 1.0495873405633995e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2926, 'grad_norm': 1.383317232131958, 'learning_rate': 1.0489942260300176e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3017, 'grad_norm': 1.6510275602340698, 'learning_rate': 1.0484011114966357e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2952, 'grad_norm': 2.782827615737915, 'learning_rate': 1.0478079969632538e-05, 'epoch': 1.43}\n",
      "{'loss': 0.3034, 'grad_norm': 1.7833003997802734, 'learning_rate': 1.0472148824298717e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2993, 'grad_norm': 1.4774688482284546, 'learning_rate': 1.0466217678964898e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2937, 'grad_norm': 1.370589256286621, 'learning_rate': 1.0460286533631077e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3033, 'grad_norm': 2.640535354614258, 'learning_rate': 1.0454355388297257e-05, 'epoch': 1.43}\n",
      "{'loss': 0.3017, 'grad_norm': 1.6497925519943237, 'learning_rate': 1.0448424242963438e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2846, 'grad_norm': 1.5513018369674683, 'learning_rate': 1.0442493097629618e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2965, 'grad_norm': 2.077017068862915, 'learning_rate': 1.0436561952295798e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2961, 'grad_norm': 2.012038230895996, 'learning_rate': 1.0430630806961979e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3076, 'grad_norm': 1.7887686491012573, 'learning_rate': 1.042469966162816e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2722, 'grad_norm': 1.3695615530014038, 'learning_rate': 1.041876851629434e-05, 'epoch': 1.44}\n",
      "{'loss': 0.298, 'grad_norm': 2.1856026649475098, 'learning_rate': 1.0412896682413858e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2931, 'grad_norm': 1.4513651132583618, 'learning_rate': 1.0406965537080039e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3103, 'grad_norm': 2.0735104084014893, 'learning_rate': 1.0401034391746218e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2909, 'grad_norm': 1.6447131633758545, 'learning_rate': 1.0395103246412399e-05, 'epoch': 1.44}\n",
      "{'loss': 0.2927, 'grad_norm': 1.5275663137435913, 'learning_rate': 1.038917210107858e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3106, 'grad_norm': 1.7188700437545776, 'learning_rate': 1.038324095574476e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3073, 'grad_norm': 1.4117318391799927, 'learning_rate': 1.037730981041094e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3221, 'grad_norm': 1.9126900434494019, 'learning_rate': 1.0371378665077121e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2815, 'grad_norm': 1.558607578277588, 'learning_rate': 1.0365447519743302e-05, 'epoch': 1.45}\n",
      "{'loss': 0.2881, 'grad_norm': 1.810770034790039, 'learning_rate': 1.0359516374409481e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3199, 'grad_norm': 1.8769686222076416, 'learning_rate': 1.0353585229075662e-05, 'epoch': 1.45}\n",
      "{'loss': 0.2821, 'grad_norm': 2.030336380004883, 'learning_rate': 1.0347654083741843e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3046, 'grad_norm': 1.8337268829345703, 'learning_rate': 1.0341722938408024e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3198, 'grad_norm': 1.8614193201065063, 'learning_rate': 1.0335791793074201e-05, 'epoch': 1.45}\n",
      "{'loss': 0.2979, 'grad_norm': 2.4362733364105225, 'learning_rate': 1.0329860647740382e-05, 'epoch': 1.45}\n",
      "{'loss': 0.2885, 'grad_norm': 1.609728217124939, 'learning_rate': 1.0323929502406561e-05, 'epoch': 1.45}\n",
      "{'loss': 0.308, 'grad_norm': 1.7093956470489502, 'learning_rate': 1.0318057668526083e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3014, 'grad_norm': 1.3659123182296753, 'learning_rate': 1.0312126523192263e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2791, 'grad_norm': 1.5710387229919434, 'learning_rate': 1.0306195377858441e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3218, 'grad_norm': 2.5763096809387207, 'learning_rate': 1.0300264232524622e-05, 'epoch': 1.46}\n",
      "{'loss': 0.2962, 'grad_norm': 1.46466863155365, 'learning_rate': 1.0294333087190803e-05, 'epoch': 1.46}\n",
      "{'loss': 0.2911, 'grad_norm': 1.3064746856689453, 'learning_rate': 1.0288401941856982e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3071, 'grad_norm': 2.149204969406128, 'learning_rate': 1.0282470796523163e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2951, 'grad_norm': 2.066965103149414, 'learning_rate': 1.0276539651189344e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3093, 'grad_norm': 2.192031145095825, 'learning_rate': 1.0270608505855523e-05, 'epoch': 1.46}\n",
      "{'loss': 0.2943, 'grad_norm': 2.337397575378418, 'learning_rate': 1.0264677360521704e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3241, 'grad_norm': 2.3150649070739746, 'learning_rate': 1.0258746215187885e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3127, 'grad_norm': 2.141166925430298, 'learning_rate': 1.0252815069854066e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2811, 'grad_norm': 1.7234255075454712, 'learning_rate': 1.0246883924520245e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3097, 'grad_norm': 1.5166759490966797, 'learning_rate': 1.0240952779186426e-05, 'epoch': 1.46}\n",
      "{'loss': 0.3084, 'grad_norm': 1.637536883354187, 'learning_rate': 1.0235021633852607e-05, 'epoch': 1.47}\n",
      "{'loss': 0.3041, 'grad_norm': 1.7881807088851929, 'learning_rate': 1.0229090488518788e-05, 'epoch': 1.47}\n",
      "{'loss': 0.3005, 'grad_norm': 2.016097068786621, 'learning_rate': 1.0223159343184967e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3079, 'grad_norm': 1.8235845565795898, 'learning_rate': 1.0217228197851148e-05, 'epoch': 1.47}\n",
      "{'loss': 0.2868, 'grad_norm': 1.8700884580612183, 'learning_rate': 1.0211297052517329e-05, 'epoch': 1.47}\n",
      "{'loss': 0.298, 'grad_norm': 3.9948482513427734, 'learning_rate': 1.0205365907183506e-05, 'epoch': 1.47}\n",
      "{'loss': 0.3203, 'grad_norm': 2.3408286571502686, 'learning_rate': 1.0199434761849687e-05, 'epoch': 1.47}\n",
      "{'loss': 0.2976, 'grad_norm': 1.0069704055786133, 'learning_rate': 1.0193503616515868e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2889, 'grad_norm': 1.7329164743423462, 'learning_rate': 1.0187572471182047e-05, 'epoch': 1.47}\n",
      "{'loss': 0.2753, 'grad_norm': 1.7351279258728027, 'learning_rate': 1.0181641325848228e-05, 'epoch': 1.47}\n",
      "{'loss': 0.2861, 'grad_norm': 1.4642997980117798, 'learning_rate': 1.0175710180514409e-05, 'epoch': 1.47}\n",
      "{'loss': 0.3152, 'grad_norm': 1.7254269123077393, 'learning_rate': 1.016977903518059e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2934, 'grad_norm': 1.7406305074691772, 'learning_rate': 1.0163847889846769e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2826, 'grad_norm': 2.2119362354278564, 'learning_rate': 1.015791674451295e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3125, 'grad_norm': 1.5049712657928467, 'learning_rate': 1.0151985599179131e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3237, 'grad_norm': 1.8084073066711426, 'learning_rate': 1.0146054453845312e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2934, 'grad_norm': 1.9712772369384766, 'learning_rate': 1.0140123308511491e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2939, 'grad_norm': 2.876284599304199, 'learning_rate': 1.0134192163177672e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2977, 'grad_norm': 1.953789472579956, 'learning_rate': 1.0128261017843853e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2857, 'grad_norm': 1.2584997415542603, 'learning_rate': 1.0122329872510032e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2955, 'grad_norm': 1.656076431274414, 'learning_rate': 1.0116398727176213e-05, 'epoch': 1.48}\n",
      "{'loss': 0.3108, 'grad_norm': 1.7254624366760254, 'learning_rate': 1.0110467581842392e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2945, 'grad_norm': 2.053722620010376, 'learning_rate': 1.0104536436508571e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3102, 'grad_norm': 2.1289327144622803, 'learning_rate': 1.0098605291174752e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2986, 'grad_norm': 1.9611743688583374, 'learning_rate': 1.0092674145840933e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2999, 'grad_norm': 2.067511558532715, 'learning_rate': 1.0086743000507114e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2909, 'grad_norm': 1.1618317365646362, 'learning_rate': 1.0080811855173293e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2994, 'grad_norm': 1.9003713130950928, 'learning_rate': 1.0074880709839474e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2881, 'grad_norm': 2.8345792293548584, 'learning_rate': 1.0068949564505655e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3117, 'grad_norm': 1.5721988677978516, 'learning_rate': 1.0063018419171834e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3039, 'grad_norm': 1.9319370985031128, 'learning_rate': 1.0057087273838015e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2972, 'grad_norm': 2.57489275932312, 'learning_rate': 1.0051156128504196e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2854, 'grad_norm': 1.6672309637069702, 'learning_rate': 1.0045224983170377e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2883, 'grad_norm': 1.1468347311019897, 'learning_rate': 1.0039293837836556e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3152, 'grad_norm': 1.6427340507507324, 'learning_rate': 1.0033362692502737e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2986, 'grad_norm': 1.459883689880371, 'learning_rate': 1.0027490858622255e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2908, 'grad_norm': 1.2473598718643188, 'learning_rate': 1.0021559713288436e-05, 'epoch': 1.5}\n",
      "{'loss': 0.291, 'grad_norm': 2.186678886413574, 'learning_rate': 1.0015628567954617e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.28, 'grad_norm': 1.773072361946106, 'learning_rate': 1.0009697422620796e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2994, 'grad_norm': 2.25822377204895, 'learning_rate': 1.0003766277286977e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2941, 'grad_norm': 1.0425686836242676, 'learning_rate': 9.997835131953156e-06, 'epoch': 1.5}\n",
      "{'loss': 0.2957, 'grad_norm': 1.170681357383728, 'learning_rate': 9.991903986619337e-06, 'epoch': 1.5}\n",
      "{'loss': 0.2739, 'grad_norm': 1.4418474435806274, 'learning_rate': 9.985972841285518e-06, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2953, 'grad_norm': 1.4256998300552368, 'learning_rate': 9.980041695951697e-06, 'epoch': 1.5}\n",
      "{'loss': 0.3025, 'grad_norm': 1.6187715530395508, 'learning_rate': 9.974110550617878e-06, 'epoch': 1.5}\n",
      "{'loss': 0.2895, 'grad_norm': 1.5997393131256104, 'learning_rate': 9.968179405284059e-06, 'epoch': 1.51}\n",
      "{'loss': 0.296, 'grad_norm': 1.8442879915237427, 'learning_rate': 9.96224825995024e-06, 'epoch': 1.51}\n",
      "{'loss': 0.3105, 'grad_norm': 1.4498581886291504, 'learning_rate': 9.956317114616419e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2982, 'grad_norm': 1.4315036535263062, 'learning_rate': 9.950445280735938e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2914, 'grad_norm': 1.4603224992752075, 'learning_rate': 9.944514135402117e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2766, 'grad_norm': 2.521146297454834, 'learning_rate': 9.938582990068298e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2841, 'grad_norm': 1.5478700399398804, 'learning_rate': 9.932651844734478e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2643, 'grad_norm': 1.8104583024978638, 'learning_rate': 9.926720699400658e-06, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2952, 'grad_norm': 2.0178275108337402, 'learning_rate': 9.92078955406684e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2844, 'grad_norm': 1.6980125904083252, 'learning_rate': 9.914858408733019e-06, 'epoch': 1.51}\n",
      "{'loss': 0.2876, 'grad_norm': 2.2565488815307617, 'learning_rate': 9.9089272633992e-06, 'epoch': 1.51}\n",
      "{'loss': 0.3047, 'grad_norm': 1.65742826461792, 'learning_rate': 9.90299611806538e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2811, 'grad_norm': 2.5289556980133057, 'learning_rate': 9.897064972731561e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2973, 'grad_norm': 1.7807880640029907, 'learning_rate': 9.89113382739774e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2651, 'grad_norm': 1.646814227104187, 'learning_rate': 9.885202682063921e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2921, 'grad_norm': 1.5527665615081787, 'learning_rate': 9.8792715367301e-06, 'epoch': 1.52}\n",
      "{'loss': 0.3135, 'grad_norm': 1.5414080619812012, 'learning_rate': 9.873340391396281e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2901, 'grad_norm': 1.225131869316101, 'learning_rate': 9.867409246062462e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2933, 'grad_norm': 1.4311463832855225, 'learning_rate': 9.861478100728642e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2882, 'grad_norm': 1.417836308479309, 'learning_rate': 9.855546955394822e-06, 'epoch': 1.52}\n",
      "{'loss': 0.3065, 'grad_norm': 1.4902784824371338, 'learning_rate': 9.849615810061003e-06, 'epoch': 1.52}\n",
      "{'loss': 0.3003, 'grad_norm': 1.9465186595916748, 'learning_rate': 9.843684664727183e-06, 'epoch': 1.52}\n",
      "{'loss': 0.2929, 'grad_norm': 1.4418423175811768, 'learning_rate': 9.837753519393363e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2799, 'grad_norm': 1.990681529045105, 'learning_rate': 9.831822374059543e-06, 'epoch': 1.53}\n",
      "{'loss': 0.316, 'grad_norm': 1.7279913425445557, 'learning_rate': 9.825891228725724e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2894, 'grad_norm': 1.7642556428909302, 'learning_rate': 9.819960083391905e-06, 'epoch': 1.53}\n",
      "{'loss': 0.3204, 'grad_norm': 1.4863194227218628, 'learning_rate': 9.814028938058084e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2888, 'grad_norm': 1.511865258216858, 'learning_rate': 9.808097792724265e-06, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3102, 'grad_norm': 1.7324507236480713, 'learning_rate': 9.802166647390446e-06, 'epoch': 1.53}\n",
      "{'loss': 0.3041, 'grad_norm': 1.1536508798599243, 'learning_rate': 9.796235502056626e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2905, 'grad_norm': 2.067497968673706, 'learning_rate': 9.790304356722806e-06, 'epoch': 1.53}\n",
      "{'loss': 0.301, 'grad_norm': 2.068279504776001, 'learning_rate': 9.784373211388985e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2923, 'grad_norm': 1.5108964443206787, 'learning_rate': 9.778442066055166e-06, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2761, 'grad_norm': 1.4751858711242676, 'learning_rate': 9.772510920721347e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2742, 'grad_norm': 1.707607626914978, 'learning_rate': 9.766579775387528e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2927, 'grad_norm': 1.7155288457870483, 'learning_rate': 9.760648630053707e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2964, 'grad_norm': 1.5651291608810425, 'learning_rate': 9.754717484719888e-06, 'epoch': 1.54}\n",
      "{'loss': 0.3088, 'grad_norm': 1.5775015354156494, 'learning_rate': 9.748786339386069e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3083, 'grad_norm': 1.751402497291565, 'learning_rate': 9.74285519405225e-06, 'epoch': 1.54}\n",
      "{'loss': 0.3211, 'grad_norm': 1.9159772396087646, 'learning_rate': 9.736924048718429e-06, 'epoch': 1.54}\n",
      "{'loss': 0.3089, 'grad_norm': 1.128420114517212, 'learning_rate': 9.730992903384608e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2942, 'grad_norm': 1.6887098550796509, 'learning_rate': 9.725061758050789e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2978, 'grad_norm': 1.1205146312713623, 'learning_rate': 9.71913061271697e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3108, 'grad_norm': 1.4531294107437134, 'learning_rate': 9.71319946738315e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2862, 'grad_norm': 1.5627211332321167, 'learning_rate': 9.70726832204933e-06, 'epoch': 1.54}\n",
      "{'loss': 0.2755, 'grad_norm': 2.0438036918640137, 'learning_rate': 9.70133717671551e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2818, 'grad_norm': 2.2842726707458496, 'learning_rate': 9.695406031381692e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2923, 'grad_norm': 1.7262934446334839, 'learning_rate': 9.68947488604787e-06, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.288, 'grad_norm': 1.5756124258041382, 'learning_rate': 9.683543740714052e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2789, 'grad_norm': 2.060849666595459, 'learning_rate': 9.677612595380231e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2996, 'grad_norm': 2.0809097290039062, 'learning_rate': 9.671681450046412e-06, 'epoch': 1.55}\n",
      "{'loss': 0.3017, 'grad_norm': 1.8006597757339478, 'learning_rate': 9.665750304712593e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2941, 'grad_norm': 1.4382330179214478, 'learning_rate': 9.659819159378772e-06, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3143, 'grad_norm': 1.4894150495529175, 'learning_rate': 9.653888014044953e-06, 'epoch': 1.55}\n",
      "{'loss': 0.3046, 'grad_norm': 1.5501667261123657, 'learning_rate': 9.647956868711134e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2844, 'grad_norm': 1.5052857398986816, 'learning_rate': 9.642025723377315e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2856, 'grad_norm': 1.919917106628418, 'learning_rate': 9.636153889496832e-06, 'epoch': 1.56}\n",
      "{'loss': 0.3044, 'grad_norm': 1.735021710395813, 'learning_rate': 9.630222744163013e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2912, 'grad_norm': 1.8911370038986206, 'learning_rate': 9.624291598829192e-06, 'epoch': 1.56}\n",
      "{'loss': 0.274, 'grad_norm': 1.4013794660568237, 'learning_rate': 9.61841976494871e-06, 'epoch': 1.56}\n",
      "{'loss': 0.2947, 'grad_norm': 1.3530207872390747, 'learning_rate': 9.612488619614891e-06, 'epoch': 1.56}\n",
      "{'loss': 0.3062, 'grad_norm': 2.0452511310577393, 'learning_rate': 9.606557474281072e-06, 'epoch': 1.56}\n",
      "{'loss': 0.3155, 'grad_norm': 1.9220036268234253, 'learning_rate': 9.600626328947253e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2994, 'grad_norm': 1.1135199069976807, 'learning_rate': 9.594695183613432e-06, 'epoch': 1.56}\n",
      "{'loss': 0.2954, 'grad_norm': 1.863812804222107, 'learning_rate': 9.588764038279613e-06, 'epoch': 1.56}\n",
      "{'loss': 0.2748, 'grad_norm': 1.2475401163101196, 'learning_rate': 9.582832892945792e-06, 'epoch': 1.56}\n",
      "{'loss': 0.3098, 'grad_norm': 1.5182121992111206, 'learning_rate': 9.576901747611973e-06, 'epoch': 1.56}\n",
      "{'loss': 0.2797, 'grad_norm': 1.5832332372665405, 'learning_rate': 9.570970602278154e-06, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2769, 'grad_norm': 1.796877145767212, 'learning_rate': 9.565039456944333e-06, 'epoch': 1.57}\n",
      "{'loss': 0.3119, 'grad_norm': 1.80293607711792, 'learning_rate': 9.559108311610514e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2719, 'grad_norm': 1.381011962890625, 'learning_rate': 9.553177166276695e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2708, 'grad_norm': 1.6306928396224976, 'learning_rate': 9.547246020942876e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2902, 'grad_norm': 2.093017816543579, 'learning_rate': 9.541314875609055e-06, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3023, 'grad_norm': 1.8565466403961182, 'learning_rate': 9.535383730275236e-06, 'epoch': 1.57}\n",
      "{'loss': 0.3112, 'grad_norm': 1.3485265970230103, 'learning_rate': 9.529452584941415e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2683, 'grad_norm': 1.9885914325714111, 'learning_rate': 9.523521439607596e-06, 'epoch': 1.57}\n",
      "{'loss': 0.3084, 'grad_norm': 1.7500470876693726, 'learning_rate': 9.517590294273777e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2874, 'grad_norm': 1.5456316471099854, 'learning_rate': 9.511659148939956e-06, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3067, 'grad_norm': 1.935528039932251, 'learning_rate': 9.505728003606137e-06, 'epoch': 1.57}\n",
      "{'loss': 0.2863, 'grad_norm': 1.9199796915054321, 'learning_rate': 9.499796858272318e-06, 'epoch': 1.58}\n",
      "{'loss': 0.2855, 'grad_norm': 1.174890398979187, 'learning_rate': 9.493865712938499e-06, 'epoch': 1.58}\n",
      "{'loss': 0.2876, 'grad_norm': 1.4680179357528687, 'learning_rate': 9.487934567604678e-06, 'epoch': 1.58}\n",
      "{'loss': 0.2844, 'grad_norm': 1.2189828157424927, 'learning_rate': 9.482003422270857e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2856, 'grad_norm': 1.7516928911209106, 'learning_rate': 9.476072276937038e-06, 'epoch': 1.58}\n",
      "{'loss': 0.2843, 'grad_norm': 1.1931487321853638, 'learning_rate': 9.47014113160322e-06, 'epoch': 1.58}\n",
      "{'loss': 0.3075, 'grad_norm': 1.039812445640564, 'learning_rate': 9.4642099862694e-06, 'epoch': 1.58}\n",
      "{'loss': 0.2967, 'grad_norm': 1.529036045074463, 'learning_rate': 9.45827884093558e-06, 'epoch': 1.58}\n",
      "{'loss': 0.3029, 'grad_norm': 2.1464524269104004, 'learning_rate': 9.45234769560176e-06, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3159, 'grad_norm': 1.1553348302841187, 'learning_rate': 9.446416550267941e-06, 'epoch': 1.58}\n",
      "{'loss': 0.3036, 'grad_norm': 1.4454936981201172, 'learning_rate': 9.44048540493412e-06, 'epoch': 1.58}\n",
      "{'loss': 0.3151, 'grad_norm': 1.660405158996582, 'learning_rate': 9.434554259600301e-06, 'epoch': 1.59}\n",
      "{'loss': 0.3073, 'grad_norm': 15.531126022338867, 'learning_rate': 9.42862311426648e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2783, 'grad_norm': 1.9713915586471558, 'learning_rate': 9.422691968932661e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2839, 'grad_norm': 1.3655763864517212, 'learning_rate': 9.416760823598842e-06, 'epoch': 1.59}\n",
      "{'loss': 0.3037, 'grad_norm': 2.3708813190460205, 'learning_rate': 9.410829678265021e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2925, 'grad_norm': 1.620940089225769, 'learning_rate': 9.404898532931202e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2868, 'grad_norm': 1.3413465023040771, 'learning_rate': 9.39902669905072e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2785, 'grad_norm': 1.762436866760254, 'learning_rate': 9.393095553716901e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.288, 'grad_norm': 1.6261957883834839, 'learning_rate': 9.387164408383082e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2691, 'grad_norm': 1.6725438833236694, 'learning_rate': 9.381233263049263e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2824, 'grad_norm': 1.676030158996582, 'learning_rate': 9.375302117715442e-06, 'epoch': 1.59}\n",
      "{'loss': 0.2888, 'grad_norm': 1.9273205995559692, 'learning_rate': 9.369370972381623e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2947, 'grad_norm': 1.7563492059707642, 'learning_rate': 9.363439827047804e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2928, 'grad_norm': 1.7305262088775635, 'learning_rate': 9.357508681713983e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2639, 'grad_norm': 1.2708195447921753, 'learning_rate': 9.351577536380164e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2911, 'grad_norm': 1.5742714405059814, 'learning_rate': 9.345646391046343e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2867, 'grad_norm': 1.2672063112258911, 'learning_rate': 9.339715245712524e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2911, 'grad_norm': 1.293073296546936, 'learning_rate': 9.333784100378705e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3009, 'grad_norm': 1.3707222938537598, 'learning_rate': 9.327852955044884e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2967, 'grad_norm': 1.11750328540802, 'learning_rate': 9.321921809711065e-06, 'epoch': 1.6}\n",
      "{'loss': 0.3051, 'grad_norm': 2.0219128131866455, 'learning_rate': 9.315990664377246e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2879, 'grad_norm': 1.3890643119812012, 'learning_rate': 9.310059519043427e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2847, 'grad_norm': 1.5906375646591187, 'learning_rate': 9.304128373709606e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3023, 'grad_norm': 1.5054937601089478, 'learning_rate': 9.298197228375785e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2998, 'grad_norm': 2.0706162452697754, 'learning_rate': 9.292266083041966e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2966, 'grad_norm': 1.8456655740737915, 'learning_rate': 9.286334937708147e-06, 'epoch': 1.61}\n",
      "{'loss': 0.3035, 'grad_norm': 1.8425607681274414, 'learning_rate': 9.280403792374328e-06, 'epoch': 1.61}\n",
      "{'loss': 0.3008, 'grad_norm': 1.4630130529403687, 'learning_rate': 9.274472647040507e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3017, 'grad_norm': 1.146711826324463, 'learning_rate': 9.268541501706688e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2857, 'grad_norm': 2.1831369400024414, 'learning_rate': 9.262610356372869e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2994, 'grad_norm': 1.25519597530365, 'learning_rate': 9.256679211039048e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2945, 'grad_norm': 1.4387351274490356, 'learning_rate': 9.250748065705229e-06, 'epoch': 1.61}\n",
      "{'loss': 0.3062, 'grad_norm': 2.198850154876709, 'learning_rate': 9.244816920371408e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2679, 'grad_norm': 1.6270289421081543, 'learning_rate': 9.238885775037589e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2927, 'grad_norm': 1.7001315355300903, 'learning_rate': 9.23295462970377e-06, 'epoch': 1.62}\n",
      "{'loss': 0.3355, 'grad_norm': 2.6843316555023193, 'learning_rate': 9.227023484369951e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2759, 'grad_norm': 1.8421550989151, 'learning_rate': 9.22109233903613e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2757, 'grad_norm': 1.6666065454483032, 'learning_rate': 9.215161193702311e-06, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3067, 'grad_norm': 1.5381009578704834, 'learning_rate': 9.20923004836849e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2908, 'grad_norm': 2.3700907230377197, 'learning_rate': 9.203298903034671e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2705, 'grad_norm': 2.0579116344451904, 'learning_rate': 9.197367757700852e-06, 'epoch': 1.62}\n",
      "{'loss': 0.3053, 'grad_norm': 2.192542552947998, 'learning_rate': 9.191436612367031e-06, 'epoch': 1.62}\n",
      "{'loss': 0.3052, 'grad_norm': 1.4017091989517212, 'learning_rate': 9.185505467033212e-06, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3139, 'grad_norm': 1.3109444379806519, 'learning_rate': 9.179574321699393e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2905, 'grad_norm': 1.6481982469558716, 'learning_rate': 9.173643176365572e-06, 'epoch': 1.62}\n",
      "{'loss': 0.3084, 'grad_norm': 2.6073780059814453, 'learning_rate': 9.167712031031753e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2871, 'grad_norm': 1.5292664766311646, 'learning_rate': 9.161780885697934e-06, 'epoch': 1.63}\n",
      "{'loss': 0.3013, 'grad_norm': 2.1737122535705566, 'learning_rate': 9.155849740364113e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3035, 'grad_norm': 1.9140580892562866, 'learning_rate': 9.149918595030294e-06, 'epoch': 1.63}\n",
      "{'loss': 0.3027, 'grad_norm': 2.068121910095215, 'learning_rate': 9.143987449696473e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2981, 'grad_norm': 1.3913031816482544, 'learning_rate': 9.138056304362654e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2796, 'grad_norm': 1.964725375175476, 'learning_rate': 9.132125159028835e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2833, 'grad_norm': 1.8831307888031006, 'learning_rate': 9.126253325148353e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2998, 'grad_norm': 1.3096659183502197, 'learning_rate': 9.120322179814534e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2767, 'grad_norm': 1.9707305431365967, 'learning_rate': 9.114391034480715e-06, 'epoch': 1.63}\n",
      "{'loss': 0.3243, 'grad_norm': 2.062269687652588, 'learning_rate': 9.108459889146894e-06, 'epoch': 1.63}\n",
      "{'loss': 0.2893, 'grad_norm': 1.6593947410583496, 'learning_rate': 9.102528743813075e-06, 'epoch': 1.64}\n",
      "{'loss': 0.3046, 'grad_norm': 1.7958170175552368, 'learning_rate': 9.096597598479256e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2965, 'grad_norm': 1.7490708827972412, 'learning_rate': 9.090666453145437e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2983, 'grad_norm': 2.0774447917938232, 'learning_rate': 9.084735307811616e-06, 'epoch': 1.64}\n",
      "{'loss': 0.299, 'grad_norm': 2.195728063583374, 'learning_rate': 9.078804162477795e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2949, 'grad_norm': 1.4354369640350342, 'learning_rate': 9.072873017143976e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2849, 'grad_norm': 1.3160967826843262, 'learning_rate': 9.066941871810157e-06, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2932, 'grad_norm': 1.987798810005188, 'learning_rate': 9.061010726476338e-06, 'epoch': 1.64}\n",
      "{'loss': 0.3118, 'grad_norm': 1.712493896484375, 'learning_rate': 9.055079581142517e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2865, 'grad_norm': 2.0144476890563965, 'learning_rate': 9.049148435808698e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2966, 'grad_norm': 1.6983933448791504, 'learning_rate': 9.043217290474879e-06, 'epoch': 1.64}\n",
      "{'loss': 0.282, 'grad_norm': 2.0049614906311035, 'learning_rate': 9.037286145141058e-06, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2861, 'grad_norm': 1.8059337139129639, 'learning_rate': 9.031354999807239e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2947, 'grad_norm': 2.0881125926971436, 'learning_rate': 9.025423854473418e-06, 'epoch': 1.65}\n",
      "{'loss': 0.3097, 'grad_norm': 1.4554133415222168, 'learning_rate': 9.019492709139599e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2996, 'grad_norm': 2.50681734085083, 'learning_rate': 9.01356156380578e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2931, 'grad_norm': 1.934840202331543, 'learning_rate': 9.007630418471959e-06, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2909, 'grad_norm': 1.560036540031433, 'learning_rate': 9.00169927313814e-06, 'epoch': 1.65}\n",
      "{'loss': 0.286, 'grad_norm': 2.019566297531128, 'learning_rate': 8.995768127804321e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2872, 'grad_norm': 1.6936098337173462, 'learning_rate': 8.989836982470502e-06, 'epoch': 1.65}\n",
      "{'loss': 0.3036, 'grad_norm': 2.138721466064453, 'learning_rate': 8.983905837136681e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2903, 'grad_norm': 1.8637653589248657, 'learning_rate': 8.97797469180286e-06, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2903, 'grad_norm': 1.4358378648757935, 'learning_rate': 8.972043546469041e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2905, 'grad_norm': 1.410823941230774, 'learning_rate': 8.966112401135222e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2826, 'grad_norm': 1.5690535306930542, 'learning_rate': 8.960240567254741e-06, 'epoch': 1.66}\n",
      "{'loss': 0.3019, 'grad_norm': 1.4385110139846802, 'learning_rate': 8.95430942192092e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2905, 'grad_norm': 2.0533764362335205, 'learning_rate': 8.948378276587102e-06, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3135, 'grad_norm': 1.3780330419540405, 'learning_rate': 8.94244713125328e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2989, 'grad_norm': 1.9567595720291138, 'learning_rate': 8.936515985919462e-06, 'epoch': 1.66}\n",
      "{'loss': 0.301, 'grad_norm': 1.6034210920333862, 'learning_rate': 8.930584840585643e-06, 'epoch': 1.66}\n",
      "{'loss': 0.3118, 'grad_norm': 1.6377824544906616, 'learning_rate': 8.924653695251822e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2828, 'grad_norm': 1.9143208265304565, 'learning_rate': 8.918722549918003e-06, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2971, 'grad_norm': 1.4508397579193115, 'learning_rate': 8.912791404584184e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2786, 'grad_norm': 1.578255534172058, 'learning_rate': 8.906860259250363e-06, 'epoch': 1.66}\n",
      "{'loss': 0.2844, 'grad_norm': 1.8291009664535522, 'learning_rate': 8.900929113916544e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2952, 'grad_norm': 1.938989281654358, 'learning_rate': 8.894997968582723e-06, 'epoch': 1.67}\n",
      "{'loss': 0.284, 'grad_norm': 2.055610179901123, 'learning_rate': 8.889066823248904e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2922, 'grad_norm': 1.8041731119155884, 'learning_rate': 8.883135677915085e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2726, 'grad_norm': 1.5774835348129272, 'learning_rate': 8.877204532581266e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2838, 'grad_norm': 2.424135684967041, 'learning_rate': 8.871273387247445e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2893, 'grad_norm': 2.423133373260498, 'learning_rate': 8.865342241913626e-06, 'epoch': 1.67}\n",
      "{'loss': 0.3121, 'grad_norm': 1.7004823684692383, 'learning_rate': 8.859411096579805e-06, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3022, 'grad_norm': 1.9486701488494873, 'learning_rate': 8.853479951245986e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2797, 'grad_norm': 1.6314815282821655, 'learning_rate': 8.847548805912167e-06, 'epoch': 1.67}\n",
      "{'loss': 0.3098, 'grad_norm': 1.705845594406128, 'learning_rate': 8.841676972031686e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2955, 'grad_norm': 1.3938766717910767, 'learning_rate': 8.835745826697865e-06, 'epoch': 1.68}\n",
      "{'loss': 0.278, 'grad_norm': 1.2458620071411133, 'learning_rate': 8.829814681364045e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.278, 'grad_norm': 1.533705711364746, 'learning_rate': 8.823883536030225e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2973, 'grad_norm': 1.3521465063095093, 'learning_rate': 8.817952390696406e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2909, 'grad_norm': 1.52116858959198, 'learning_rate': 8.812021245362587e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2937, 'grad_norm': 2.019834518432617, 'learning_rate': 8.806090100028766e-06, 'epoch': 1.68}\n",
      "{'loss': 0.3084, 'grad_norm': 1.8110675811767578, 'learning_rate': 8.800158954694947e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3097, 'grad_norm': 1.338530421257019, 'learning_rate': 8.794227809361128e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2946, 'grad_norm': 1.73046875, 'learning_rate': 8.788296664027307e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2817, 'grad_norm': 1.9122941493988037, 'learning_rate': 8.782365518693488e-06, 'epoch': 1.68}\n",
      "{'loss': 0.3185, 'grad_norm': 2.5095856189727783, 'learning_rate': 8.776434373359668e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2807, 'grad_norm': 2.451321601867676, 'learning_rate': 8.770503228025848e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.278, 'grad_norm': 1.069690227508545, 'learning_rate': 8.76457208269203e-06, 'epoch': 1.69}\n",
      "{'loss': 0.28, 'grad_norm': 1.8835861682891846, 'learning_rate': 8.758640937358209e-06, 'epoch': 1.69}\n",
      "{'loss': 0.2808, 'grad_norm': 1.1769819259643555, 'learning_rate': 8.75270979202439e-06, 'epoch': 1.69}\n",
      "{'loss': 0.2827, 'grad_norm': 1.2513456344604492, 'learning_rate': 8.74677864669057e-06, 'epoch': 1.69}\n",
      "{'loss': 0.3029, 'grad_norm': 1.6101187467575073, 'learning_rate': 8.740847501356751e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2805, 'grad_norm': 2.2789218425750732, 'learning_rate': 8.73491635602293e-06, 'epoch': 1.69}\n",
      "{'loss': 0.2731, 'grad_norm': 1.347847580909729, 'learning_rate': 8.72898521068911e-06, 'epoch': 1.69}\n",
      "{'loss': 0.3016, 'grad_norm': 1.4489818811416626, 'learning_rate': 8.72305406535529e-06, 'epoch': 1.69}\n",
      "{'loss': 0.3137, 'grad_norm': 2.03303861618042, 'learning_rate': 8.717122920021472e-06, 'epoch': 1.69}\n",
      "{'loss': 0.2957, 'grad_norm': 1.4879570007324219, 'learning_rate': 8.711191774687652e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2747, 'grad_norm': 1.1891050338745117, 'learning_rate': 8.70531994080717e-06, 'epoch': 1.69}\n",
      "{'loss': 0.301, 'grad_norm': 1.9516942501068115, 'learning_rate': 8.699388795473351e-06, 'epoch': 1.7}\n",
      "{'loss': 0.3006, 'grad_norm': 1.9095765352249146, 'learning_rate': 8.69345765013953e-06, 'epoch': 1.7}\n",
      "{'loss': 0.2758, 'grad_norm': 1.8377091884613037, 'learning_rate': 8.687526504805711e-06, 'epoch': 1.7}\n",
      "{'loss': 0.2975, 'grad_norm': 1.7597402334213257, 'learning_rate': 8.681595359471892e-06, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2806, 'grad_norm': 1.6571013927459717, 'learning_rate': 8.675664214138071e-06, 'epoch': 1.7}\n",
      "{'loss': 0.3042, 'grad_norm': 1.9259204864501953, 'learning_rate': 8.669733068804252e-06, 'epoch': 1.7}\n",
      "{'loss': 0.274, 'grad_norm': 1.7201192378997803, 'learning_rate': 8.663801923470433e-06, 'epoch': 1.7}\n",
      "{'loss': 0.2832, 'grad_norm': 1.7870937585830688, 'learning_rate': 8.657870778136612e-06, 'epoch': 1.7}\n",
      "{'loss': 0.3, 'grad_norm': 1.6631442308425903, 'learning_rate': 8.651939632802793e-06, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2769, 'grad_norm': 1.3313425779342651, 'learning_rate': 8.646008487468972e-06, 'epoch': 1.7}\n",
      "{'loss': 0.293, 'grad_norm': 1.5129481554031372, 'learning_rate': 8.640077342135153e-06, 'epoch': 1.7}\n",
      "{'loss': 0.3067, 'grad_norm': 1.352091908454895, 'learning_rate': 8.634146196801334e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2809, 'grad_norm': 1.7971997261047363, 'learning_rate': 8.628215051467515e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2863, 'grad_norm': 1.477497935295105, 'learning_rate': 8.622283906133694e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2857, 'grad_norm': 1.588112473487854, 'learning_rate': 8.616352760799875e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2844, 'grad_norm': 1.5369929075241089, 'learning_rate': 8.610421615466056e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2893, 'grad_norm': 1.8274904489517212, 'learning_rate': 8.604490470132235e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2894, 'grad_norm': 1.3524610996246338, 'learning_rate': 8.598559324798416e-06, 'epoch': 1.71}\n",
      "{'loss': 0.3177, 'grad_norm': 1.6154510974884033, 'learning_rate': 8.592628179464595e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2764, 'grad_norm': 2.4625182151794434, 'learning_rate': 8.586697034130776e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2805, 'grad_norm': 1.3094698190689087, 'learning_rate': 8.580765888796957e-06, 'epoch': 1.71}\n",
      "{'loss': 0.3088, 'grad_norm': 1.9405646324157715, 'learning_rate': 8.574834743463138e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2912, 'grad_norm': 1.470513939857483, 'learning_rate': 8.568903598129317e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2801, 'grad_norm': 1.7418925762176514, 'learning_rate': 8.562972452795498e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.327, 'grad_norm': 1.698620319366455, 'learning_rate': 8.557100618915016e-06, 'epoch': 1.72}\n",
      "{'loss': 0.3135, 'grad_norm': 1.9728251695632935, 'learning_rate': 8.551169473581197e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2863, 'grad_norm': 1.6395281553268433, 'learning_rate': 8.545238328247378e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2902, 'grad_norm': 1.6804794073104858, 'learning_rate': 8.539307182913557e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2862, 'grad_norm': 1.424309253692627, 'learning_rate': 8.533376037579736e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2976, 'grad_norm': 2.386904001235962, 'learning_rate': 8.527444892245917e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2808, 'grad_norm': 1.3957351446151733, 'learning_rate': 8.521513746912098e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2998, 'grad_norm': 1.5890730619430542, 'learning_rate': 8.515582601578279e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2896, 'grad_norm': 2.3224172592163086, 'learning_rate': 8.509651456244458e-06, 'epoch': 1.72}\n",
      "{'loss': 0.2778, 'grad_norm': 1.9420737028121948, 'learning_rate': 8.503720310910639e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2718, 'grad_norm': 1.9230937957763672, 'learning_rate': 8.49778916557682e-06, 'epoch': 1.73}\n",
      "{'loss': 0.283, 'grad_norm': 1.9029403924942017, 'learning_rate': 8.491858020243e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2915, 'grad_norm': 1.5409173965454102, 'learning_rate': 8.48592687490918e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2664, 'grad_norm': 2.0615041255950928, 'learning_rate': 8.47999572957536e-06, 'epoch': 1.73}\n",
      "{'loss': 0.3173, 'grad_norm': 1.3421355485916138, 'learning_rate': 8.47406458424154e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2901, 'grad_norm': 1.8367440700531006, 'learning_rate': 8.468133438907721e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2966, 'grad_norm': 1.2835590839385986, 'learning_rate': 8.462202293573902e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2987, 'grad_norm': 2.0468077659606934, 'learning_rate': 8.456271148240081e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2876, 'grad_norm': 1.8855923414230347, 'learning_rate': 8.450340002906262e-06, 'epoch': 1.73}\n",
      "{'loss': 0.2757, 'grad_norm': 1.3794007301330566, 'learning_rate': 8.444408857572443e-06, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.291, 'grad_norm': 1.9360647201538086, 'learning_rate': 8.438477712238624e-06, 'epoch': 1.73}\n",
      "{'loss': 0.3017, 'grad_norm': 1.6690512895584106, 'learning_rate': 8.432546566904803e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2877, 'grad_norm': 1.889865517616272, 'learning_rate': 8.426615421570982e-06, 'epoch': 1.74}\n",
      "{'loss': 0.3018, 'grad_norm': 1.7274432182312012, 'learning_rate': 8.420684276237163e-06, 'epoch': 1.74}\n",
      "{'loss': 0.3015, 'grad_norm': 1.7602354288101196, 'learning_rate': 8.414753130903344e-06, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2878, 'grad_norm': 2.2358591556549072, 'learning_rate': 8.408821985569525e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2783, 'grad_norm': 1.8753125667572021, 'learning_rate': 8.402890840235704e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2821, 'grad_norm': 2.0944392681121826, 'learning_rate': 8.396959694901885e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2573, 'grad_norm': 2.0068013668060303, 'learning_rate': 8.391028549568066e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2905, 'grad_norm': 1.499685525894165, 'learning_rate': 8.385097404234245e-06, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2811, 'grad_norm': 1.540961503982544, 'learning_rate': 8.379166258900426e-06, 'epoch': 1.74}\n",
      "{'loss': 0.2902, 'grad_norm': 1.5790199041366577, 'learning_rate': 8.373235113566605e-06, 'epoch': 1.74}\n",
      "{'loss': 0.3004, 'grad_norm': 1.3184579610824585, 'learning_rate': 8.367303968232786e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2793, 'grad_norm': 1.5925383567810059, 'learning_rate': 8.361372822898967e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2921, 'grad_norm': 1.6569031476974487, 'learning_rate': 8.355500989018485e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3058, 'grad_norm': 1.694341778755188, 'learning_rate': 8.349569843684666e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2718, 'grad_norm': 1.2554011344909668, 'learning_rate': 8.343638698350845e-06, 'epoch': 1.75}\n",
      "{'loss': 0.289, 'grad_norm': 1.3394825458526611, 'learning_rate': 8.337707553017026e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2916, 'grad_norm': 2.380309820175171, 'learning_rate': 8.331776407683207e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2976, 'grad_norm': 1.78192138671875, 'learning_rate': 8.325845262349388e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2928, 'grad_norm': 1.598381757736206, 'learning_rate': 8.319914117015567e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2922, 'grad_norm': 1.477309226989746, 'learning_rate': 8.313982971681748e-06, 'epoch': 1.75}\n",
      "{'loss': 0.3045, 'grad_norm': 2.2401175498962402, 'learning_rate': 8.308051826347927e-06, 'epoch': 1.75}\n",
      "{'loss': 0.3002, 'grad_norm': 1.0166033506393433, 'learning_rate': 8.302120681014108e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2979, 'grad_norm': 1.7123106718063354, 'learning_rate': 8.296189535680289e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3055, 'grad_norm': 1.3415026664733887, 'learning_rate': 8.290258390346468e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2772, 'grad_norm': 2.3477516174316406, 'learning_rate': 8.284327245012649e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2971, 'grad_norm': 1.6243995428085327, 'learning_rate': 8.27839609967883e-06, 'epoch': 1.76}\n",
      "{'loss': 0.3079, 'grad_norm': 1.905205249786377, 'learning_rate': 8.272464954345009e-06, 'epoch': 1.76}\n",
      "{'loss': 0.293, 'grad_norm': 1.6528950929641724, 'learning_rate': 8.26653380901119e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2813, 'grad_norm': 1.4040125608444214, 'learning_rate': 8.26060266367737e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2714, 'grad_norm': 1.3183122873306274, 'learning_rate': 8.25467151834355e-06, 'epoch': 1.76}\n",
      "{'loss': 0.3118, 'grad_norm': 1.5997964143753052, 'learning_rate': 8.248740373009731e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2823, 'grad_norm': 1.9221463203430176, 'learning_rate': 8.24280922767591e-06, 'epoch': 1.76}\n",
      "{'loss': 0.2884, 'grad_norm': 2.0234482288360596, 'learning_rate': 8.236878082342091e-06, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2857, 'grad_norm': 1.5759302377700806, 'learning_rate': 8.230946937008272e-06, 'epoch': 1.77}\n",
      "{'loss': 0.2811, 'grad_norm': 1.5046610832214355, 'learning_rate': 8.225015791674453e-06, 'epoch': 1.77}\n",
      "{'loss': 0.2764, 'grad_norm': 1.9987281560897827, 'learning_rate': 8.219084646340632e-06, 'epoch': 1.77}\n",
      "{'loss': 0.3072, 'grad_norm': 2.0211591720581055, 'learning_rate': 8.213153501006813e-06, 'epoch': 1.77}\n",
      "{'loss': 0.3003, 'grad_norm': 1.4877890348434448, 'learning_rate': 8.207222355672992e-06, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2986, 'grad_norm': 1.5027374029159546, 'learning_rate': 8.201291210339173e-06, 'epoch': 1.77}\n",
      "{'loss': 0.2936, 'grad_norm': 1.4532675743103027, 'learning_rate': 8.195419376458692e-06, 'epoch': 1.77}\n",
      "{'loss': 0.3032, 'grad_norm': 1.431235671043396, 'learning_rate': 8.189488231124873e-06, 'epoch': 1.77}\n",
      "{'loss': 0.2877, 'grad_norm': 1.8960957527160645, 'learning_rate': 8.183557085791052e-06, 'epoch': 1.77}\n",
      "{'loss': 0.2861, 'grad_norm': 1.2015596628189087, 'learning_rate': 8.177625940457232e-06, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2695, 'grad_norm': 2.0877010822296143, 'learning_rate': 8.171694795123413e-06, 'epoch': 1.77}\n",
      "{'loss': 0.2916, 'grad_norm': 2.130547523498535, 'learning_rate': 8.165763649789594e-06, 'epoch': 1.78}\n",
      "{'loss': 0.2623, 'grad_norm': 1.8289214372634888, 'learning_rate': 8.159832504455774e-06, 'epoch': 1.78}\n",
      "{'loss': 0.2791, 'grad_norm': 1.8409994840621948, 'learning_rate': 8.153901359121954e-06, 'epoch': 1.78}\n",
      "{'loss': 0.2898, 'grad_norm': 1.7134850025177002, 'learning_rate': 8.147970213788135e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2746, 'grad_norm': 1.7671682834625244, 'learning_rate': 8.142039068454315e-06, 'epoch': 1.78}\n",
      "{'loss': 0.2605, 'grad_norm': 1.9315282106399536, 'learning_rate': 8.136107923120495e-06, 'epoch': 1.78}\n",
      "{'loss': 0.303, 'grad_norm': 1.7073220014572144, 'learning_rate': 8.130176777786676e-06, 'epoch': 1.78}\n",
      "{'loss': 0.3139, 'grad_norm': 1.576796293258667, 'learning_rate': 8.124245632452855e-06, 'epoch': 1.78}\n",
      "{'loss': 0.2949, 'grad_norm': 2.2037105560302734, 'learning_rate': 8.118314487119036e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2555, 'grad_norm': 1.907172679901123, 'learning_rate': 8.112383341785217e-06, 'epoch': 1.78}\n",
      "{'loss': 0.302, 'grad_norm': 1.6152771711349487, 'learning_rate': 8.106452196451396e-06, 'epoch': 1.78}\n",
      "{'loss': 0.2602, 'grad_norm': 1.2680909633636475, 'learning_rate': 8.100521051117577e-06, 'epoch': 1.79}\n",
      "{'loss': 0.288, 'grad_norm': 1.677534818649292, 'learning_rate': 8.094589905783758e-06, 'epoch': 1.79}\n",
      "{'loss': 0.301, 'grad_norm': 1.812229871749878, 'learning_rate': 8.088658760449938e-06, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2911, 'grad_norm': 1.2384439706802368, 'learning_rate': 8.082727615116118e-06, 'epoch': 1.79}\n",
      "{'loss': 0.2888, 'grad_norm': 1.6877431869506836, 'learning_rate': 8.076796469782297e-06, 'epoch': 1.79}\n",
      "{'loss': 0.2772, 'grad_norm': 2.4517626762390137, 'learning_rate': 8.070865324448478e-06, 'epoch': 1.79}\n",
      "{'loss': 0.3107, 'grad_norm': 1.6604580879211426, 'learning_rate': 8.064934179114659e-06, 'epoch': 1.79}\n",
      "{'loss': 0.2714, 'grad_norm': 1.7106547355651855, 'learning_rate': 8.05900303378084e-06, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2951, 'grad_norm': 1.6357357501983643, 'learning_rate': 8.053071888447019e-06, 'epoch': 1.79}\n",
      "{'loss': 0.283, 'grad_norm': 2.091794967651367, 'learning_rate': 8.0471407431132e-06, 'epoch': 1.79}\n",
      "{'loss': 0.2733, 'grad_norm': 1.1131744384765625, 'learning_rate': 8.04120959777938e-06, 'epoch': 1.79}\n",
      "{'loss': 0.2814, 'grad_norm': 1.6279871463775635, 'learning_rate': 8.035278452445561e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2801, 'grad_norm': 1.170544981956482, 'learning_rate': 8.02934730711174e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2927, 'grad_norm': 1.4357300996780396, 'learning_rate': 8.02341616177792e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2779, 'grad_norm': 1.8842138051986694, 'learning_rate': 8.0174850164441e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2812, 'grad_norm': 1.3173282146453857, 'learning_rate': 8.011553871110282e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2954, 'grad_norm': 1.6049293279647827, 'learning_rate': 8.005622725776463e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2891, 'grad_norm': 1.5348619222640991, 'learning_rate': 7.999691580442642e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.301, 'grad_norm': 1.819999098777771, 'learning_rate': 7.993760435108823e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2913, 'grad_norm': 1.6357710361480713, 'learning_rate': 7.987829289775004e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2796, 'grad_norm': 2.852238893508911, 'learning_rate': 7.981898144441183e-06, 'epoch': 1.8}\n",
      "{'loss': 0.2843, 'grad_norm': 1.8999824523925781, 'learning_rate': 7.975966999107364e-06, 'epoch': 1.8}\n",
      "{'loss': 0.3139, 'grad_norm': 1.8879042863845825, 'learning_rate': 7.970035853773543e-06, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2942, 'grad_norm': 2.01967191696167, 'learning_rate': 7.964104708439724e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2769, 'grad_norm': 1.3138790130615234, 'learning_rate': 7.958173563105905e-06, 'epoch': 1.81}\n",
      "{'loss': 0.3023, 'grad_norm': 1.5184669494628906, 'learning_rate': 7.952301729225422e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2582, 'grad_norm': 1.4761301279067993, 'learning_rate': 7.946370583891603e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2946, 'grad_norm': 1.344015121459961, 'learning_rate': 7.940439438557783e-06, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3024, 'grad_norm': 1.435274362564087, 'learning_rate': 7.934508293223963e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2907, 'grad_norm': 1.727259635925293, 'learning_rate': 7.928577147890144e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2814, 'grad_norm': 1.8912551403045654, 'learning_rate': 7.922646002556325e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2824, 'grad_norm': 1.7635678052902222, 'learning_rate': 7.916714857222504e-06, 'epoch': 1.81}\n",
      "{'loss': 0.3006, 'grad_norm': 1.7668557167053223, 'learning_rate': 7.910783711888685e-06, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2468, 'grad_norm': 1.43247652053833, 'learning_rate': 7.904911878008203e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2889, 'grad_norm': 1.6293206214904785, 'learning_rate': 7.898980732674384e-06, 'epoch': 1.82}\n",
      "{'loss': 0.3013, 'grad_norm': 2.4732468128204346, 'learning_rate': 7.893049587340565e-06, 'epoch': 1.82}\n",
      "{'loss': 0.2809, 'grad_norm': 1.387330412864685, 'learning_rate': 7.887118442006744e-06, 'epoch': 1.82}\n",
      "{'loss': 0.281, 'grad_norm': 1.7324955463409424, 'learning_rate': 7.881187296672923e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3006, 'grad_norm': 1.7019342184066772, 'learning_rate': 7.875256151339104e-06, 'epoch': 1.82}\n",
      "{'loss': 0.2682, 'grad_norm': 1.2519279718399048, 'learning_rate': 7.869325006005285e-06, 'epoch': 1.82}\n",
      "{'loss': 0.2834, 'grad_norm': 1.1468336582183838, 'learning_rate': 7.863393860671466e-06, 'epoch': 1.82}\n",
      "{'loss': 0.2729, 'grad_norm': 2.22247052192688, 'learning_rate': 7.857462715337645e-06, 'epoch': 1.82}\n",
      "{'loss': 0.282, 'grad_norm': 2.143141984939575, 'learning_rate': 7.851531570003826e-06, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2977, 'grad_norm': 1.2544057369232178, 'learning_rate': 7.845600424670007e-06, 'epoch': 1.82}\n",
      "{'loss': 0.2906, 'grad_norm': 2.167257785797119, 'learning_rate': 7.839669279336188e-06, 'epoch': 1.82}\n",
      "{'loss': 0.2805, 'grad_norm': 1.6854509115219116, 'learning_rate': 7.833738134002367e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2882, 'grad_norm': 1.7243250608444214, 'learning_rate': 7.827806988668546e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2997, 'grad_norm': 1.594356894493103, 'learning_rate': 7.821875843334727e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2842, 'grad_norm': 1.7159594297409058, 'learning_rate': 7.815944698000908e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2671, 'grad_norm': 0.8860944509506226, 'learning_rate': 7.810013552667089e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2826, 'grad_norm': 1.7533074617385864, 'learning_rate': 7.804082407333268e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2846, 'grad_norm': 1.667330026626587, 'learning_rate': 7.79815126199945e-06, 'epoch': 1.83}\n",
      "{'loss': 0.28, 'grad_norm': 1.8699826002120972, 'learning_rate': 7.79222011666563e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2927, 'grad_norm': 1.2973097562789917, 'learning_rate': 7.786288971331811e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2685, 'grad_norm': 2.215773582458496, 'learning_rate': 7.78035782599799e-06, 'epoch': 1.83}\n",
      "{'loss': 0.286, 'grad_norm': 1.891567349433899, 'learning_rate': 7.77442668066417e-06, 'epoch': 1.83}\n",
      "{'loss': 0.2962, 'grad_norm': 1.9565033912658691, 'learning_rate': 7.768554846783689e-06, 'epoch': 1.84}\n",
      "{'loss': 0.3211, 'grad_norm': 1.7833155393600464, 'learning_rate': 7.76262370144987e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2802, 'grad_norm': 1.3787678480148315, 'learning_rate': 7.756692556116049e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2917, 'grad_norm': 1.7014789581298828, 'learning_rate': 7.75076141078223e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2839, 'grad_norm': 1.7688735723495483, 'learning_rate': 7.744830265448409e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2889, 'grad_norm': 1.6508214473724365, 'learning_rate': 7.73889912011459e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2875, 'grad_norm': 1.8396048545837402, 'learning_rate': 7.73296797478077e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.267, 'grad_norm': 1.5613840818405151, 'learning_rate': 7.727036829446952e-06, 'epoch': 1.84}\n",
      "{'loss': 0.3082, 'grad_norm': 1.4436097145080566, 'learning_rate': 7.721105684113131e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2864, 'grad_norm': 1.778782844543457, 'learning_rate': 7.715174538779312e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2969, 'grad_norm': 1.1826571226119995, 'learning_rate': 7.709243393445493e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2921, 'grad_norm': 1.8191272020339966, 'learning_rate': 7.703312248111672e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.293, 'grad_norm': 1.1470563411712646, 'learning_rate': 7.697381102777853e-06, 'epoch': 1.85}\n",
      "{'loss': 0.3056, 'grad_norm': 1.2198820114135742, 'learning_rate': 7.691449957444032e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2898, 'grad_norm': 1.8208683729171753, 'learning_rate': 7.685518812110213e-06, 'epoch': 1.85}\n",
      "{'loss': 0.291, 'grad_norm': 1.6228843927383423, 'learning_rate': 7.679587666776394e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2778, 'grad_norm': 1.4360647201538086, 'learning_rate': 7.673656521442575e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3057, 'grad_norm': 2.3080177307128906, 'learning_rate': 7.667725376108754e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2972, 'grad_norm': 1.4309885501861572, 'learning_rate': 7.661794230774935e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2796, 'grad_norm': 1.8850092887878418, 'learning_rate': 7.655863085441114e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2927, 'grad_norm': 1.69282066822052, 'learning_rate': 7.649931940107295e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2914, 'grad_norm': 1.6301710605621338, 'learning_rate': 7.644000794773476e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2851, 'grad_norm': 2.062931776046753, 'learning_rate': 7.638069649439655e-06, 'epoch': 1.85}\n",
      "{'loss': 0.287, 'grad_norm': 1.3700615167617798, 'learning_rate': 7.632138504105836e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2909, 'grad_norm': 2.3256869316101074, 'learning_rate': 7.626207358772016e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2895, 'grad_norm': 1.2196581363677979, 'learning_rate': 7.620276213438197e-06, 'epoch': 1.86}\n",
      "{'loss': 0.3015, 'grad_norm': 2.8277788162231445, 'learning_rate': 7.614345068104377e-06, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.294, 'grad_norm': 1.5976585149765015, 'learning_rate': 7.608413922770557e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2823, 'grad_norm': 1.5020220279693604, 'learning_rate': 7.602482777436737e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2754, 'grad_norm': 1.3894399404525757, 'learning_rate': 7.596551632102917e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2701, 'grad_norm': 1.738306999206543, 'learning_rate': 7.590620486769098e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2804, 'grad_norm': 1.4583812952041626, 'learning_rate': 7.584689341435278e-06, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3033, 'grad_norm': 2.142369270324707, 'learning_rate': 7.578758196101459e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2905, 'grad_norm': 1.5421518087387085, 'learning_rate': 7.572886362220977e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2795, 'grad_norm': 1.783133625984192, 'learning_rate': 7.567014528340496e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2626, 'grad_norm': 1.923677682876587, 'learning_rate': 7.561083383006676e-06, 'epoch': 1.87}\n",
      "{'loss': 0.3026, 'grad_norm': 1.43154776096344, 'learning_rate': 7.555152237672856e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.292, 'grad_norm': 1.084773063659668, 'learning_rate': 7.549221092339036e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2849, 'grad_norm': 2.637521743774414, 'learning_rate': 7.543289947005216e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2922, 'grad_norm': 2.308483362197876, 'learning_rate': 7.537358801671397e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2847, 'grad_norm': 1.4957674741744995, 'learning_rate': 7.531427656337577e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2881, 'grad_norm': 2.05061674118042, 'learning_rate': 7.525496511003758e-06, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2867, 'grad_norm': 1.9478362798690796, 'learning_rate': 7.519565365669938e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2851, 'grad_norm': 1.450135350227356, 'learning_rate': 7.513634220336119e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2967, 'grad_norm': 1.0446968078613281, 'learning_rate': 7.507703075002299e-06, 'epoch': 1.87}\n",
      "{'loss': 0.2895, 'grad_norm': 1.4278618097305298, 'learning_rate': 7.5017719296684784e-06, 'epoch': 1.88}\n",
      "{'loss': 0.2894, 'grad_norm': 1.6289210319519043, 'learning_rate': 7.495840784334659e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.285, 'grad_norm': 1.7649372816085815, 'learning_rate': 7.489909639000839e-06, 'epoch': 1.88}\n",
      "{'loss': 0.299, 'grad_norm': 1.35751211643219, 'learning_rate': 7.48397849366702e-06, 'epoch': 1.88}\n",
      "{'loss': 0.2928, 'grad_norm': 1.4609651565551758, 'learning_rate': 7.4780473483332e-06, 'epoch': 1.88}\n",
      "{'loss': 0.2983, 'grad_norm': 1.9628705978393555, 'learning_rate': 7.472116202999381e-06, 'epoch': 1.88}\n",
      "{'loss': 0.2844, 'grad_norm': 1.447655200958252, 'learning_rate': 7.466185057665561e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2901, 'grad_norm': 1.2739973068237305, 'learning_rate': 7.460253912331742e-06, 'epoch': 1.88}\n",
      "{'loss': 0.3095, 'grad_norm': 2.0486268997192383, 'learning_rate': 7.454322766997921e-06, 'epoch': 1.88}\n",
      "{'loss': 0.278, 'grad_norm': 1.3257877826690674, 'learning_rate': 7.4483916216641015e-06, 'epoch': 1.88}\n",
      "{'loss': 0.2916, 'grad_norm': 1.6121643781661987, 'learning_rate': 7.442460476330282e-06, 'epoch': 1.88}\n",
      "{'loss': 0.2884, 'grad_norm': 1.9929168224334717, 'learning_rate': 7.4365293309964624e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2842, 'grad_norm': 1.6847045421600342, 'learning_rate': 7.430598185662643e-06, 'epoch': 1.89}\n",
      "{'loss': 0.291, 'grad_norm': 1.4736359119415283, 'learning_rate': 7.424667040328823e-06, 'epoch': 1.89}\n",
      "{'loss': 0.2972, 'grad_norm': 1.4822015762329102, 'learning_rate': 7.418735894995004e-06, 'epoch': 1.89}\n",
      "{'loss': 0.2808, 'grad_norm': 1.6252226829528809, 'learning_rate': 7.412804749661184e-06, 'epoch': 1.89}\n",
      "{'loss': 0.2982, 'grad_norm': 1.7216213941574097, 'learning_rate': 7.4068736043273636e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2826, 'grad_norm': 1.3822308778762817, 'learning_rate': 7.4009424589935445e-06, 'epoch': 1.89}\n",
      "{'loss': 0.2882, 'grad_norm': 1.609621524810791, 'learning_rate': 7.3950113136597245e-06, 'epoch': 1.89}\n",
      "{'loss': 0.2886, 'grad_norm': 1.5157091617584229, 'learning_rate': 7.389080168325905e-06, 'epoch': 1.89}\n",
      "{'loss': 0.3017, 'grad_norm': 1.7755160331726074, 'learning_rate': 7.3831490229920855e-06, 'epoch': 1.89}\n",
      "{'loss': 0.281, 'grad_norm': 1.8751243352890015, 'learning_rate': 7.3772178776582655e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2794, 'grad_norm': 1.5812197923660278, 'learning_rate': 7.3712867323244464e-06, 'epoch': 1.9}\n",
      "{'loss': 0.276, 'grad_norm': 0.9789910912513733, 'learning_rate': 7.3653555869906265e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2823, 'grad_norm': 1.2650216817855835, 'learning_rate': 7.359424441656807e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2824, 'grad_norm': 2.275301694869995, 'learning_rate': 7.353493296322987e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2909, 'grad_norm': 1.771801471710205, 'learning_rate': 7.347562150989167e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2896, 'grad_norm': 1.659661054611206, 'learning_rate': 7.3416310056553476e-06, 'epoch': 1.9}\n",
      "{'loss': 0.28, 'grad_norm': 1.854679822921753, 'learning_rate': 7.335699860321528e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2762, 'grad_norm': 2.071655750274658, 'learning_rate': 7.3297687149877085e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2824, 'grad_norm': 2.2184085845947266, 'learning_rate': 7.3238375696538886e-06, 'epoch': 1.9}\n",
      "{'loss': 0.294, 'grad_norm': 1.4015530347824097, 'learning_rate': 7.3179064243200695e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2931, 'grad_norm': 1.8967379331588745, 'learning_rate': 7.3119752789862495e-06, 'epoch': 1.9}\n",
      "{'loss': 0.277, 'grad_norm': 1.3739196062088013, 'learning_rate': 7.306044133652429e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2912, 'grad_norm': 1.5705422163009644, 'learning_rate': 7.30011298831861e-06, 'epoch': 1.91}\n",
      "{'loss': 0.274, 'grad_norm': 1.5757540464401245, 'learning_rate': 7.29418184298479e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2942, 'grad_norm': 2.7178635597229004, 'learning_rate': 7.288250697650971e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2968, 'grad_norm': 1.687639594078064, 'learning_rate': 7.282319552317151e-06, 'epoch': 1.91}\n",
      "{'loss': 0.3001, 'grad_norm': 1.8594028949737549, 'learning_rate': 7.2763884069833316e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2946, 'grad_norm': 1.8433350324630737, 'learning_rate': 7.270457261649512e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2961, 'grad_norm': 1.6711878776550293, 'learning_rate': 7.2645261163156925e-06, 'epoch': 1.91}\n",
      "{'loss': 0.269, 'grad_norm': 1.1166666746139526, 'learning_rate': 7.2585949709818726e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2972, 'grad_norm': 1.4577341079711914, 'learning_rate': 7.252663825648052e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2813, 'grad_norm': 1.505910038948059, 'learning_rate': 7.246732680314233e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2888, 'grad_norm': 1.8203926086425781, 'learning_rate': 7.240801534980413e-06, 'epoch': 1.91}\n",
      "{'loss': 0.2944, 'grad_norm': 1.4326227903366089, 'learning_rate': 7.234870389646594e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2562, 'grad_norm': 1.590897798538208, 'learning_rate': 7.228939244312774e-06, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2606, 'grad_norm': 1.6441211700439453, 'learning_rate': 7.223008098978954e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2993, 'grad_norm': 1.9046440124511719, 'learning_rate': 7.217076953645135e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2823, 'grad_norm': 1.2485638856887817, 'learning_rate': 7.211205119764652e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2623, 'grad_norm': 1.6234229803085327, 'learning_rate': 7.205273974430833e-06, 'epoch': 1.92}\n",
      "{'loss': 0.299, 'grad_norm': 1.4749783277511597, 'learning_rate': 7.199342829097013e-06, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.29, 'grad_norm': 1.784385323524475, 'learning_rate': 7.193411683763194e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2958, 'grad_norm': 2.3046228885650635, 'learning_rate': 7.187480538429374e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2911, 'grad_norm': 1.756290078163147, 'learning_rate': 7.1815493930955535e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2847, 'grad_norm': 1.6331020593643188, 'learning_rate': 7.175618247761734e-06, 'epoch': 1.92}\n",
      "{'loss': 0.2795, 'grad_norm': 1.1172274351119995, 'learning_rate': 7.1696871024279144e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2724, 'grad_norm': 1.6320548057556152, 'learning_rate': 7.163755957094095e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2965, 'grad_norm': 1.2157697677612305, 'learning_rate': 7.157824811760275e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2828, 'grad_norm': 1.4021902084350586, 'learning_rate': 7.151893666426456e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2997, 'grad_norm': 1.7279123067855835, 'learning_rate': 7.145962521092636e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2729, 'grad_norm': 1.5361473560333252, 'learning_rate': 7.140031375758817e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2682, 'grad_norm': 1.2899144887924194, 'learning_rate': 7.134100230424997e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2946, 'grad_norm': 2.1239240169525146, 'learning_rate': 7.1281690850911765e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2874, 'grad_norm': 2.137113094329834, 'learning_rate': 7.122237939757357e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2891, 'grad_norm': 1.5744214057922363, 'learning_rate': 7.1163067944235375e-06, 'epoch': 1.93}\n",
      "{'loss': 0.2908, 'grad_norm': 1.7978262901306152, 'learning_rate': 7.110375649089718e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2796, 'grad_norm': 1.826943278312683, 'learning_rate': 7.1044445037558984e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2886, 'grad_norm': 1.7660982608795166, 'learning_rate': 7.0985133584220785e-06, 'epoch': 1.94}\n",
      "{'loss': 0.279, 'grad_norm': 1.369372844696045, 'learning_rate': 7.092582213088259e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2904, 'grad_norm': 1.5313888788223267, 'learning_rate': 7.0866510677544394e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2872, 'grad_norm': 1.5719008445739746, 'learning_rate': 7.0807199224206195e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2991, 'grad_norm': 1.2820945978164673, 'learning_rate': 7.0747887770867996e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2812, 'grad_norm': 1.4249675273895264, 'learning_rate': 7.06885763175298e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2913, 'grad_norm': 1.5666896104812622, 'learning_rate': 7.0629264864191605e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2968, 'grad_norm': 1.8114458322525024, 'learning_rate': 7.0569953410853406e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2654, 'grad_norm': 1.4824270009994507, 'learning_rate': 7.0510641957515215e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2812, 'grad_norm': 1.5973960161209106, 'learning_rate': 7.045192361871039e-06, 'epoch': 1.94}\n",
      "{'loss': 0.2896, 'grad_norm': 1.7520396709442139, 'learning_rate': 7.03926121653722e-06, 'epoch': 1.94}\n",
      "{'loss': 0.306, 'grad_norm': 1.6596391201019287, 'learning_rate': 7.0333300712034e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2842, 'grad_norm': 3.5334689617156982, 'learning_rate': 7.027398925869581e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2802, 'grad_norm': 2.185026168823242, 'learning_rate': 7.021467780535761e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2845, 'grad_norm': 1.8609464168548584, 'learning_rate': 7.015536635201942e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2831, 'grad_norm': 2.500211000442505, 'learning_rate': 7.009605489868122e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2707, 'grad_norm': 1.9016274213790894, 'learning_rate': 7.003674344534301e-06, 'epoch': 1.95}\n",
      "{'loss': 0.292, 'grad_norm': 1.8328986167907715, 'learning_rate': 6.997743199200482e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2905, 'grad_norm': 1.43524968624115, 'learning_rate': 6.991812053866662e-06, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2892, 'grad_norm': 1.408163070678711, 'learning_rate': 6.985880908532843e-06, 'epoch': 1.95}\n",
      "{'loss': 0.282, 'grad_norm': 1.5636343955993652, 'learning_rate': 6.979949763199023e-06, 'epoch': 1.95}\n",
      "{'loss': 0.3004, 'grad_norm': 1.3105157613754272, 'learning_rate': 6.974018617865203e-06, 'epoch': 1.95}\n",
      "{'loss': 0.2843, 'grad_norm': 1.950864315032959, 'learning_rate': 6.968087472531384e-06, 'epoch': 1.96}\n",
      "{'loss': 0.2693, 'grad_norm': 1.9841046333312988, 'learning_rate': 6.962156327197564e-06, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2799, 'grad_norm': 1.9832987785339355, 'learning_rate': 6.956225181863744e-06, 'epoch': 1.96}\n",
      "{'loss': 0.2867, 'grad_norm': 1.592392086982727, 'learning_rate': 6.950294036529924e-06, 'epoch': 1.96}\n",
      "{'loss': 0.263, 'grad_norm': 1.841377854347229, 'learning_rate': 6.944362891196104e-06, 'epoch': 1.96}\n",
      "{'loss': 0.273, 'grad_norm': 1.669487476348877, 'learning_rate': 6.938431745862285e-06, 'epoch': 1.96}\n",
      "{'loss': 0.2625, 'grad_norm': 1.6762027740478516, 'learning_rate': 6.932500600528465e-06, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2969, 'grad_norm': 1.248454213142395, 'learning_rate': 6.926569455194646e-06, 'epoch': 1.96}\n",
      "{'loss': 0.2628, 'grad_norm': 1.7185630798339844, 'learning_rate': 6.920638309860826e-06, 'epoch': 1.96}\n",
      "{'loss': 0.3018, 'grad_norm': 1.7451895475387573, 'learning_rate': 6.914707164527007e-06, 'epoch': 1.96}\n",
      "{'loss': 0.2789, 'grad_norm': 1.372743010520935, 'learning_rate': 6.908776019193187e-06, 'epoch': 1.96}\n",
      "{'loss': 0.2916, 'grad_norm': 1.652501106262207, 'learning_rate': 6.902904185312706e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2893, 'grad_norm': 2.1798930168151855, 'learning_rate': 6.896973039978886e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2876, 'grad_norm': 2.4247963428497314, 'learning_rate': 6.891041894645067e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2826, 'grad_norm': 1.8176109790802002, 'learning_rate': 6.885110749311247e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2758, 'grad_norm': 1.7003010511398315, 'learning_rate': 6.879179603977426e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2803, 'grad_norm': 1.9149823188781738, 'learning_rate': 6.873248458643607e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2975, 'grad_norm': 2.6304450035095215, 'learning_rate': 6.867317313309787e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2834, 'grad_norm': 1.3194535970687866, 'learning_rate': 6.861386167975968e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2787, 'grad_norm': 1.172663688659668, 'learning_rate': 6.855514334095486e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2657, 'grad_norm': 0.9965096712112427, 'learning_rate': 6.849583188761666e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2991, 'grad_norm': 1.2114179134368896, 'learning_rate': 6.8436520434278465e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2794, 'grad_norm': 1.5332306623458862, 'learning_rate': 6.8377208980940266e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2665, 'grad_norm': 1.617709755897522, 'learning_rate': 6.8317897527602075e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2835, 'grad_norm': 1.564178466796875, 'learning_rate': 6.8258586074263875e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2996, 'grad_norm': 1.2369192838668823, 'learning_rate': 6.8199274620925684e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2918, 'grad_norm': 2.5097873210906982, 'learning_rate': 6.8139963167587485e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.282, 'grad_norm': 1.3963454961776733, 'learning_rate': 6.808065171424929e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2927, 'grad_norm': 1.8316190242767334, 'learning_rate': 6.802134026091109e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2919, 'grad_norm': 1.8907381296157837, 'learning_rate': 6.796202880757289e-06, 'epoch': 1.98}\n",
      "{'loss': 0.3148, 'grad_norm': 1.6706182956695557, 'learning_rate': 6.790331046876808e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2762, 'grad_norm': 1.4528290033340454, 'learning_rate': 6.784399901542988e-06, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2928, 'grad_norm': 1.8181015253067017, 'learning_rate': 6.778468756209168e-06, 'epoch': 1.98}\n",
      "{'loss': 0.3003, 'grad_norm': 1.7217082977294922, 'learning_rate': 6.772537610875348e-06, 'epoch': 1.98}\n",
      "{'loss': 0.2828, 'grad_norm': 1.8116127252578735, 'learning_rate': 6.766606465541528e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2796, 'grad_norm': 1.7563239336013794, 'learning_rate': 6.760675320207709e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2797, 'grad_norm': 2.0692005157470703, 'learning_rate': 6.754744174873889e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.276, 'grad_norm': 1.57343327999115, 'learning_rate': 6.74881302954007e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2822, 'grad_norm': 1.8369865417480469, 'learning_rate': 6.74288188420625e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2663, 'grad_norm': 1.635561466217041, 'learning_rate': 6.736950738872431e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2865, 'grad_norm': 1.5398391485214233, 'learning_rate': 6.731019593538611e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2807, 'grad_norm': 1.8988854885101318, 'learning_rate': 6.72508844820479e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2754, 'grad_norm': 2.042898416519165, 'learning_rate': 6.719157302870971e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2822, 'grad_norm': 1.9758961200714111, 'learning_rate': 6.713226157537151e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2676, 'grad_norm': 1.2087992429733276, 'learning_rate': 6.707295012203332e-06, 'epoch': 1.99}\n",
      "{'loss': 0.2832, 'grad_norm': 1.9041897058486938, 'learning_rate': 6.701363866869512e-06, 'epoch': 2.0}\n",
      "{'loss': 0.2809, 'grad_norm': 1.849419355392456, 'learning_rate': 6.695432721535693e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2802, 'grad_norm': 1.6787266731262207, 'learning_rate': 6.689501576201873e-06, 'epoch': 2.0}\n",
      "{'loss': 0.2936, 'grad_norm': 2.1008543968200684, 'learning_rate': 6.683570430868054e-06, 'epoch': 2.0}\n",
      "{'loss': 0.2654, 'grad_norm': 1.6201448440551758, 'learning_rate': 6.677698596987572e-06, 'epoch': 2.0}\n",
      "{'loss': 0.2991, 'grad_norm': 2.0426013469696045, 'learning_rate': 6.671767451653752e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b151f621504a9b8d9a211be802708c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer.py:1912\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1910\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer.py:2342\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2342\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2346\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer.py:2752\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2750\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2752\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2755\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer_seq2seq.py:180\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer.py:3608\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3605\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3607\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3608\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3609\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3611\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3612\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3618\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer.py:3793\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 3793\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3794\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3795\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\trainer_seq2seq.py:310\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m generation_inputs\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m generation_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    306\u001b[0m ):\n\u001b[0;32m    307\u001b[0m     generation_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    308\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    309\u001b[0m     }\n\u001b[1;32m--> 310\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\generation\\utils.py:1861\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1854\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1855\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   1856\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1857\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1858\u001b[0m     )\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 1861\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   1862\u001b[0m         input_ids,\n\u001b[0;32m   1863\u001b[0m         beam_scorer,\n\u001b[0;32m   1864\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1865\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   1866\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1867\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   1868\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1869\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1870\u001b[0m     )\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   1873\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1874\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1875\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1876\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1882\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1883\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\generation\\utils.py:2720\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   2717\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m stack_model_outputs(outputs_per_sub_batch)\n\u001b[0;32m   2719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Unchanged original behavior\u001b[39;00m\n\u001b[1;32m-> 2720\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2721\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2722\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2723\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2724\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2725\u001b[0m     )\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2728\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\accelerate\\utils\\operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\accelerate\\utils\\operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1399\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1395\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1396\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1397\u001b[0m         )\n\u001b[1;32m-> 1399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1416\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1418\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1194\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1188\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1189\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1190\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1191\u001b[0m     )\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1194\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:994\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    981\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    982\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    983\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    991\u001b[0m         use_cache,\n\u001b[0;32m    992\u001b[0m     )\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 994\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:404\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    402\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 404\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    412\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:179\u001b[0m, in \u001b[0;36mMarianAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    176\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(key_value_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    180\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    181\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m0\u001b[39m], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./english-tamil-final-model\\\\tokenizer_config.json',\n",
       " './english-tamil-final-model\\\\special_tokens_map.json',\n",
       " './english-tamil-final-model\\\\vocab.json',\n",
       " './english-tamil-final-model\\\\source.spm',\n",
       " './english-tamil-final-model\\\\target.spm',\n",
       " './english-tamil-final-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"./english-tamil-final-model\"\n",
    "\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, model, tokenizer, max_length=128):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        length_penalty=0.6,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hello, how are you?\"\n",
    "translated = translate_text(sample_text, model, tokenizer)\n",
    "print(f\"English: {sample_text}\")\n",
    "print(f\"Tamil: {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
